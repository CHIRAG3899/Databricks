{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6965e0c7-9b6c-4eaa-b445-094bc9c5ccaf"}}},{"cell_type":"markdown","source":["# Optimizing Data Storage with Delta Lake\n\nDatabricks supports a number of optimization for clustering data and improving directory and file skipping while scanning and loading data files. While some of these optimizations will use the word \"index\" in describing the process used, these indices differ from the algorithms many users will be familiar with from traditional SQL database systems.\n\nIn this notebook we'll explore how optional data storage and optimization settings on Delta Lake interact with file size and data skipping.\n\n## Learning Objectives\nBy the end of this lessons, students will be able to:\n- Describe default behavior for statistics collection and file skipping on Delta Lake\n- Identify columns well-suited to partitioning\n- Use `OPTIMIZE` to compact small files\n- Apply Z-order to optimize file skipping on high cardinality fields\n- Use bloom filters to speed up queries on fields with arbitrary text"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e61391ba-1b08-4161-a50b-29c783b910df"}}},{"cell_type":"code","source":["%run ../Includes/data-storage-setup $mode=\"reset\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8af31ed4-db54-47bf-90ed-4910a9d6f4b0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Notebook not found: Users/chiraggoel@kpmg.com/Applied-Data-Engineering-Solutions/Includes/_user. Notebooks can be specified via a relative path (./Notebook or ../folder/Notebook) or via an absolute path (/Abs/Path/to/Notebook). Make sure you are specifying the path correctly.\n\nStacktrace:\n  /Users/chiraggoel@kpmg.com/Applied-Data-Engineering-Solutions/01 - Architecting For The Lakehouse/ADES 1.02 - Optimizing Data Storage: python\n  /Users/chiraggoel@kpmg.com/Applied-Data-Engineering-Solutions/Includes/data-storage-setup: python","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Create a Delta Table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"57277a78-f87c-41bf-9352-3ca22c763365"}}},{"cell_type":"code","source":["spark.sql(f\"\"\"\n    CREATE OR REPLACE TABLE no_part_table\n    LOCATION '{userhome}/no_part_table'\n    AS SELECT * FROM raw_data\n\"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2baedf6a-222b-4375-bd39-deb7f46b0d2c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Delta Lake File Statistics\n\nBy default, Delta Lake will capture statistics on the first 32 columns that appear in a table. These statistics indicate the total number of records per file, as well as the minimum, maximum, and null value counts for each of the columns.\n\nStatistics are recorded in the Delta Lake transaction log files. Files are initially committed in the JSON format, but are compacted to Parquet format automatically to accelerate metadata retrieval.\n\nTransaction logs can be viewed in the `_delta_log` directory within the table location."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8c5e79b9-fa41-44a6-ad7e-f42b2d83508d"}}},{"cell_type":"code","source":["dbutils.fs.ls(f\"{userhome}/no_part_table/_delta_log\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fc36cc84-f475-4cb4-9692-adcec30502ca"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["JSON log files can be easily parsed with Spark. Statistics for each file are accessible in the `add` column.\n\nWhen a query with a selective filter (`WHERE` clause) is executed against a Delta Lake table, the query optimizer uses the information stored in the transaction logs to identify files that **may** contain records matching the conditional filter."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d5f8e864-39c2-44be-b6e3-12f9a8033ed0"}}},{"cell_type":"code","source":["display(spark.read.json(f\"{userhome}/no_part_table/_delta_log/00000000000000000000.json\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fc1d262f-6239-45c3-b48a-b3a5fca4b78d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note that columns used when creating Z-order or Bloom Filter indexes need to have statistics collected. Even without additional optimization metrics, statistics will always be leveraged for file skipping.\n\n**NOTE**: Calculating statistics on free-form text fields (product reviews, user messages, etc.) can be time consuming. For best performance, set these fields later in the schema and [change the number of columns that statistics are collected on](https://docs.databricks.com/delta/optimizations/file-mgmt.html#data-skipping)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f967b38a-0c57-4a39-ac11-820a53ef45c2"}}},{"cell_type":"markdown","source":["## Partitioning Delta Lake Tables\n\nThe partitioning method used in Delta Lake is similar to that used by Hive or Spark with Parquet (recall that Delta Lake data files are stored as Parquet).\n\nWhen a column is used to partition a table, each unique value found in that column will create a separate directory for data. When choosing partition columns, it's good to consider the following:\n1. How many total values will be present in a column?\n1. How many total records will share a given value for a column?\n1. Will records with a given value continue to arrive indefinitely?\n\n**NOTE**: When in doubt, do not partition data at all. Other data skipping features in Delta Lake can achieve similar speeds as partitioning, but data that is over-partitioned or incorrectly partitioned will suffer greatly (and require a full rewrite of all data files to remedy).\n\nColumns representing measures of time and low-cardinality fields used frequently in queries are good candidates for partitioning. The code below creates a table partitioned by date using [generated columns](https://docs.databricks.com/delta/delta-batch.html#deltausegeneratedcolumns)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"46e6290b-a21b-4ec0-80be-4d0c3acc96c0"}}},{"cell_type":"code","source":["spark.sql(f\"\"\"\n    CREATE OR REPLACE TABLE date_part_table (\n      key STRING,\n      value BINARY,\n      topic STRING,\n      partition LONG,\n      offset LONG,\n      timestamp LONG,\n      p_date DATE GENERATED ALWAYS AS (CAST(CAST(timestamp/1000 AS timestamp) AS DATE))\n    )\n    PARTITIONED BY (p_date)\n    LOCATION '{userhome}/date_part_table'\n\"\"\")\n\nspark.table(\"raw_data\").write.mode(\"append\").saveAsTable(\"date_part_table\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c8a8cdd3-ed32-40fc-adc0-e11dc4179e92"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Listing the location used for the table reveals that the unique values in the partition column are used to generate data directories. Note that the Parquet format used to store the data for Delta Lake leverages these partitions directly when determining column value (the column values for `p_date` are not stored redundantly within the data files)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d646c47b-e847-4c3c-a7f8-face66f6075e"}}},{"cell_type":"code","source":["dbutils.fs.ls(f\"{userhome}/date_part_table\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9185f2c7-86dc-4d40-ae67-d9bc72f04f4c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The data in this table look largely the same, except that more files were written because of the separation of data into separate directories based on the date."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"beafc9d7-b527-4bdb-aa18-458ae082b3e4"}}},{"cell_type":"code","source":["display(spark.read.json(f\"{userhome}/date_part_table/_delta_log/00000000000000000001.json\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"853a1f57-24b9-4160-9b55-ef55c14990ee"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["When running a query that filters data on a column used for partitioning, partitions not matching a conditional statement will be skipped entirely. Delta Lake also have several operations (including `OPTIMIZE` commands) that can be applied at the partition level.\n\nNote that because data files will be separated into different directories based on partition values, files cannot be combined or compacted across these partition boundaries. Depending on the size of data in a given table, the \"right size\" for a partition will vary, but if most partitions in a table will not contain at least 1GB of data, the table is likely over-partitioned, which will lead to slowdowns for most general queries."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"16801c5d-5836-4ac8-874c-1e0d14c121eb"}}},{"cell_type":"code","source":["%sql\nSELECT p_date, COUNT(*) FROM date_part_table GROUP BY p_date"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eea67f56-b4e7-49a5-90c3-5be83217e9f2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## File Compaction\nDelta Lake supports the `OPTIMIZE` operation, which performs file compaction. The [target file size can be auto-tuned](https://docs.databricks.com/delta/optimizations/file-mgmt.html#autotune-based-on-table-size) by Databricks, and is typically between 256 MB and 1 GB depending on overall table size.\n\nNote that data files cannot be combined across partitions. As such, some tables will benefits from not using partitions to minimize storage costs and total number of files to scan."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"44ab3a90-83ce-4d31-b351-fe7542e1062b"}}},{"cell_type":"markdown","source":["## Z-Order Indexing\n\nZ-order indexing is a technique to colocate related information in the same set of files. This co-locality is automatically used by Delta Lake on Databricks data-skipping algorithms to dramatically reduce the amount of data that needs to be read.\n\nDon't worry about <a href=\"https://en.wikipedia.org/wiki/Z-order_curve\" target=\"_blank\">the math</a> (tl;dr: Z-order maps multidimensional data to one dimension while preserving locality of the data points).\n\nMultiple columns can be used for Z-ordering, but the algorithm does lose some efficiency with each additional column. The best columns for Z-order are high cardinality columns that will be used commonly in queries.\n\nZ-order must be executed at the same time as `OPTIMIZE`, as it requires rewriting data files.\n\nBelow is the code to Z-order and optimize the `date_part_table` by `timestamp` (this might be useful for regular queries within granular time ranges)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"321454bc-b447-4f28-a320-f1a99d5bed17"}}},{"cell_type":"code","source":["%sql\nOPTIMIZE date_part_table\nZORDER BY (timestamp)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"42448e3e-f34a-4561-8535-27cd7e899527"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note that the metrics will provide an overview of what happened during the operation; reviewing the table history will also provide this information."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e03b40d-3938-45cd-963f-35a77fa7c3b0"}}},{"cell_type":"code","source":["%sql\nDESCRIBE HISTORY date_part_table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e22ac4cd-84be-49cd-8b9a-d191c78fd8cd"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Bloom Filter Indexes\n\nWhile Z-order provides useful data clustering for high cardinality data, it's often most effective when working with queries that filter against continuous numeric variables.\n\nBloom filters provide an efficient algorithm for probabilistically identifying files that may contain data using fields containing arbitrary text. Appropriate fields would include hashed values, alphanumeric codes, or free-form text fields.\n\nBloom filters calculate indexes that indicate the likelihood a given value **could** be in a file; the size of the calculated index will vary based on the number of unique values present in the field being indexed and the configured tolerance for false positives.\n\n**NOTE**: A false positive would be a file that the index thinks could have a matching record but does not. Files containing data matching a selective filter will never be skipped; false positives just mean that extra time was spent scanning files without matching records.\n\nLooking at the distribution for the `key` field, this is an ideal candidate for this technique."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a7b51e38-f24a-4d49-bc47-50f6a6b86b81"}}},{"cell_type":"code","source":["%sql\nSELECT key, count(*) FROM no_part_table GROUP BY key ORDER BY count(*) ASC"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"361b07d4-cc4f-4796-b46b-2a75f0ac84f9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The code below sets a bloom filter index on the `key` field with a false positivity allowance of 0.1%."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a43a450-2d58-479e-a8e9-1b6e376b5845"}}},{"cell_type":"code","source":["%sql\nCREATE BLOOMFILTER INDEX\nON TABLE date_part_table\nFOR COLUMNS(key OPTIONS (fpp=0.1, numItems=200))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"89221a63-9420-4869-b4ac-51b91ad7d837"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Hands-On\n\nGo through the process of adding a Z-order index and bloom filter index to the `no_part_table`. Review the history for the table to confirm the operations were successful.\n\nIf you have extra time, also create a 3rd table partitioned by `topic` and `date`, and then apply the same Z-order and bloom filter indices."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"563e8d64-1dc6-465e-92e5-e24499aff506"}}},{"cell_type":"code","source":["# TODO\n<FILL-IN>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7669649f-b433-4234-8436-f9ec53743e2c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2021 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a76eb72-b109-4f1b-8f43-79b8ec729b57"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ADES 1.02 - Optimizing Data Storage","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2598055170583271}},"nbformat":4,"nbformat_minor":0}
