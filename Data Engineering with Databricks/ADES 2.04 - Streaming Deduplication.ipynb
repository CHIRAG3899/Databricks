{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1ad313b4-6229-4f68-85aa-e2ea34274790"}}},{"cell_type":"markdown","source":["# Streaming Deduplication\n\nIn this notebook, you'll learn how to eliminate duplicate records while working with Structured Streaming and Delta Lake.\n\n## Learning Objectives\nBy the end of this lesson, you should be able to:\n- Apply `dropDuplicates` to streaming data\n- Use watermarking to manage state information\n- Write an insert-only merge to prevent inserting duplicate records into a Delta table\n- Use `foreachBatch` to perform a streaming upsert"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0f66ddb0-0690-44de-913b-a8e7a05ccce2"}}},{"cell_type":"markdown","source":["Declare database and set all path variables."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f85cc65-2c9a-4ac8-87e9-6f6af3eb1e7f"}}},{"cell_type":"code","source":["%run ../Includes/silver-setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1c8baf9f-5b20-4485-a363-8ff4dbef7311"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The following cell deletes the target silver table and checkpoint for idempotent demo execution."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"94339301-cb1b-4411-9ef5-27104539b55c"}}},{"cell_type":"code","source":["spark.sql(\"DROP TABLE IF EXISTS heart_rate_silver\")\ndbutils.fs.rm(Paths.silverRecordingsTable, True)\ndbutils.fs.rm(Paths.silverRecordingsCheckpoint, True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"76bb1eec-5174-4626-8111-ab80e75dad9a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Identify Duplicate Records\n\nBecause Kafka provides at-least-once guarantees on data delivery, all Kafka consumers should be prepared to handle duplicate records. The de-duplication methods shown here can also be applied when necessary in other parts of your Delta Lake applications.\n\nLet's start by identifying the number of duplicate records in our `bpm` topic of the bronze table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"866a7191-ae97-4671-b1d2-5a76df5cef92"}}},{"cell_type":"code","source":["(spark.read\n  .table(\"bronze\")\n  .filter(\"topic = 'bpm'\")\n  .count()\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b28fb789-96fb-4d74-813f-2462c945e5b9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["(spark.read\n  .table(\"bronze\")\n  .filter(\"topic = 'bpm'\")\n  .select(F.from_json(F.col(\"value\").cast(\"string\"), \"device_id LONG, time TIMESTAMP, heartrate DOUBLE\").alias(\"v\"))\n  .select(\"v.*\")\n  .dropDuplicates([\"device_id\", \"time\"])\n  .count()\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"16b7c446-8afe-4b52-b465-753a16395e4c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["It appears that around 10-20% of our records are duplicates. Note that here we're choosing to apply deduplication at the silver rather than the bronze level. While we are storing some duplicate records, our bronze table retains a history of the true state of our streaming source, presenting all records as they arrived (with some additional metadata recorded). This allows us to recreate any state of our downstream system, if necessary, and prevents potential data loss due to overly aggressive quality enforcement at the initial ingestion as well as minimizing latencies for data ingestion."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"50bc573d-13a4-4745-809d-1f9df3cc329f"}}},{"cell_type":"markdown","source":["## Define a Streaming Read on the Bronze BPM Records\n\nHere we'll bring back in our final logic from our last notebook."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b85b7295-93c9-4223-ac56-0462a7f6c78c"}}},{"cell_type":"code","source":["bpmDF = (spark.readStream\n  .table(\"bronze\")\n  .filter(\"topic = 'bpm'\")\n  .select(F.from_json(F.col(\"value\").cast(\"string\"), \"device_id LONG, time TIMESTAMP, heartrate DOUBLE\").alias(\"v\"))\n  .select(\"v.*\")\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"da9ca338-db62-49dc-ab8b-3f15d96ba9d5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["When dealing with streaming deduplication, there is a level of complexity compared to static data.\n\nAs each micro-batch is processed, we need to ensure:\n- No duplicate records exist in the microbatch\n- Records to be inserted are not already in the target table\n\nSpark Structured Streaming can track state information for the unique keys to ensure that duplicate records do not exist within or between microbatches. Over time, this state information will scale to represent all history. Applying a watermark of appropriate duration allows us to only track state information for a window of time in which we reasonably expect records could be delayed. Here, we'll define that watermark as 30 seconds.\n\nThe cell below updates our previous query."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bda7a0e6-fc68-4d3c-9342-a6ce9c4379dd"}}},{"cell_type":"code","source":["dedupedDF = bpmDF = (spark.readStream\n  .table(\"bronze\")\n  .filter(\"topic = 'bpm'\")\n  .select(F.from_json(F.col(\"value\").cast(\"string\"), \"device_id LONG, time TIMESTAMP, heartrate DOUBLE\").alias(\"v\"))\n  .select(\"v.*\")\n  .withWatermark(\"time\", \"30 seconds\")\n  .dropDuplicates([\"device_id\", \"time\"])\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a785a536-dc5b-49f7-8d97-3dce8d01e7a0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Insert Only Merge\nDelta Lake has optimized functionality for insert-only merges. This operation is ideal for de-duplication: define logic to match on unique keys, and only insert those records for keys that don't already exist.\n\nNote that in this application, we proceed in this fashion because we know two records with the same matching keys represent the same information. If the later arriving records indicated a necessary change to an existing record, we would need to change our logic to include a `WHEN MATCHED` clause.\n\nA merge into query is defined in SQL below against a view titled `stream_updates`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eb371453-5766-47a7-bb81-8e2c79412140"}}},{"cell_type":"code","source":["query = \"\"\"\n  MERGE INTO heart_rate_silver a\n  USING stream_updates b\n  ON a.device_id=b.device_id AND a.time=b.time\n  WHEN NOT MATCHED THEN INSERT *\n\"\"\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c6935bb-60bb-4ee4-a05d-42ac2464d06e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Defining a Microbatch Function for `foreachBatch`\n\nThe Spark Structured Streaming `foreachBatch` method allows users to define custom logic when writing.\n\nThe logic applied during `foreachBatch` addresses the present microbatch as if it were a batch (rather than streaming) data.\n\nThe class defined in the following cell defines simple logic that will allow us to register any SQL `MERGE INTO` query for use in a Structured Streaming write."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d25a633f-4610-4c04-9711-6feb20f53e14"}}},{"cell_type":"code","source":["class Upsert:\n    def __init__(self, query, update_temp=\"stream_updates\"):\n        self.query = query\n        self.update_temp = update_temp \n        \n    def upsertToDelta(self, microBatchDF, batch):\n        microBatchDF.createOrReplaceTempView(self.update_temp)\n        microBatchDF._jdf.sparkSession().sql(self.query)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1d039f0a-9e7b-4366-96cd-40323d3105f7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Because we're using SQL to write to our Delta table, we'll need to make sure this table exists before we begin."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d669919d-2ac6-4d6f-8123-e37a18cae905"}}},{"cell_type":"code","source":["spark.sql(\"DROP TABLE IF EXISTS heart_rate_silver\")\n\nspark.sql(f\"\"\"\n  CREATE TABLE IF NOT EXISTS heart_rate_silver\n  (device_id LONG, time TIMESTAMP, heartrate DOUBLE)\n  USING DELTA\n  LOCATION '{Paths.silverRecordingsTable}'\n\"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"85baf1a6-37d2-4b74-a740-b5475430486b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now pass the previously define SQL query to the `Upsert` class."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7634de3c-d974-4130-80fa-1419e2f28f12"}}},{"cell_type":"code","source":["streamingMerge=Upsert(query)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"686055e1-1fa2-48aa-9b76-861d66c5e84a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["And then use this class in our `foreachBatch` logic."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f5e6cf6e-e5e3-4edb-b067-835e8dff6ac5"}}},{"cell_type":"code","source":["(dedupedDF.writeStream\n   .foreachBatch(streamingMerge.upsertToDelta)\n   .outputMode(\"update\")\n   .option(\"checkpointLocation\", Paths.silverRecordingsCheckpoint)\n   .trigger(once=True)\n   .start()\n   .awaitTermination(300))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb3409df-270b-4276-b828-bbce3e0c0023"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We can see that our number of unique entries that have been processed to the `heart_rate_silver` table matches our batch de-duplication query from above."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a14a4a81-152d-489d-9dc1-5b487138eaf4"}}},{"cell_type":"code","source":["%sql\nSELECT COUNT(*)\nFROM heart_rate_silver"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e9ead0f2-fd24-4b02-a188-3acf71824bf2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2021 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"364b025d-6508-4fc6-b378-9d4526ddc279"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ADES 2.04 - Streaming Deduplication","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2598055170583578}},"nbformat":4,"nbformat_minor":0}
