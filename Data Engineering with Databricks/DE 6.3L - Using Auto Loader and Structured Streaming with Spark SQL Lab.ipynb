{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fc5d2b5d-366d-4fc0-a3d5-ad5b57a5d743"}}},{"cell_type":"markdown","source":["# Using Auto Loader and Structured Streaming with Spark SQL\n\n## Learning Objectives\nBy the end of this lab, you should be able to:\n* Ingest data using Auto Loader\n* Aggregate streaming data\n* Stream data to a Delta table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"754348fb-191e-4ed6-9da8-f1afe3af3a4a"}}},{"cell_type":"markdown","source":["## Setup\nRun the following script to setup necessary variables and clear out past runs of this notebook. Note that re-executing this cell will allow you to start the lab over."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"81bbd4d2-eaef-40a7-abad-c9305452d165"}}},{"cell_type":"code","source":["%run ../Includes/Classroom-Setup-6.3L"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"15a16dfe-1c9f-45d7-948a-d11ae16ae5be"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Python interpreter will be restarted.\nPython interpreter will be restarted.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Python interpreter will be restarted.\nPython interpreter will be restarted.\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\nCreating the database \"dbacademy_chiraggoel_kpmg_com_dewd_6_3l\"\n\nPredefined Paths:\n  DA.paths.working_dir: dbfs:/user/chiraggoel@kpmg.com/dbacademy/dewd/6.3l\n  DA.paths.user_db:     dbfs:/user/chiraggoel@kpmg.com/dbacademy/dewd/6.3l/6_3l.db\n  DA.paths.checkpoints: dbfs:/user/chiraggoel@kpmg.com/dbacademy/dewd/6.3l/_checkpoints\n\nPredefined tables in dbacademy_chiraggoel_kpmg_com_dewd_6_3l:\n  -none-\n\nSetup completed in 2 seconds\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\nCreating the database \"dbacademy_chiraggoel_kpmg_com_dewd_6_3l\"\n\nPredefined Paths:\n  DA.paths.working_dir: dbfs:/user/chiraggoel@kpmg.com/dbacademy/dewd/6.3l\n  DA.paths.user_db:     dbfs:/user/chiraggoel@kpmg.com/dbacademy/dewd/6.3l/6_3l.db\n  DA.paths.checkpoints: dbfs:/user/chiraggoel@kpmg.com/dbacademy/dewd/6.3l/_checkpoints\n\nPredefined tables in dbacademy_chiraggoel_kpmg_com_dewd_6_3l:\n  -none-\n\nSetup completed in 2 seconds\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Configure Streaming Read\n\nThis lab uses a collection of customer-related CSV data from DBFS found in */databricks-datasets/retail-org/customers/*.\n\nRead this data using <a href=\"https://docs.databricks.com/spark/latest/structured-streaming/auto-loader.html\" target=\"_blank\">Auto Loader</a> using its schema inference (use **`customers_checkpoint_path`** to store the schema info). Create a streaming temporary view called **`customers_raw_temp`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"efb57f2d-ac69-4125-9569-d1f142cd254b"}}},{"cell_type":"code","source":["customers_checkpoint_path = f\"{DA.paths.checkpoints}/customers\"\n\n(spark.readStream\n      .format(\"cloudFiles\")\n      .option(\"cloudFiles.format\", \"csv\")\n      .option(\"cloudFiles.schemaLocation\", customers_checkpoint_path)\n      .load(\"/databricks-datasets/retail-org/customers/\")\n      .createOrReplaceTempView(\"customers_raw_temp\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"401202a7-1381-47e5-a014-a3aa059b01c4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n\u001B[0;32m<command-2841292000074646>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mcustomers_checkpoint_path\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34mf\"{DA.paths.checkpoints}/customers\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m (spark.readStream\n\u001B[0m\u001B[1;32m      4\u001B[0m       \u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"cloudFiles\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m       \u001B[0;34m.\u001B[0m\u001B[0moption\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"cloudFiles.format\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"csv\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/streaming.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(self, path, format, schema, **options)\u001B[0m\n\u001B[1;32m    450\u001B[0m                 raise ValueError(\"If the path is provided for stream, it needs to be a \" +\n\u001B[1;32m    451\u001B[0m                                  \"non-empty string. List of paths are not supported.\")\n\u001B[0;32m--> 452\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    453\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    454\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    115\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 117\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    118\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o523.load.\n: java.lang.UnsupportedOperationException: com.databricks.backend.daemon.data.client.DBFSV1.resolvePathOnPhysicalStorage(path: Path)\n\tat com.databricks.tahoe.store.EnhancedDatabricksFileSystemV1.resolvePathOnPhysicalStorage(EnhancedFileSystem.scala:330)\n\tat com.databricks.tahoe.store.S3LogStoreBase.resolvePathOnPhysicalStorage(S3LogStore.scala:298)\n\tat com.databricks.tahoe.store.DelegatingLogStore.resolvePathOnPhysicalStorage(DelegatingLogStore.scala:156)\n\tat com.databricks.sql.fileNotification.autoIngest.CloudFilesSetupOptions.rawResolvedUri$lzycompute(CloudFilesSetupOptions.scala:55)\n\tat com.databricks.sql.fileNotification.autoIngest.CloudFilesSetupOptions.rawResolvedUri(CloudFilesSetupOptions.scala:52)\n\tat com.databricks.sql.fileNotification.autoIngest.CloudFilesSourceOptions.globPath$lzycompute(CloudFilesSourceOptions.scala:574)\n\tat com.databricks.sql.fileNotification.autoIngest.CloudFilesSourceOptions.globPath(CloudFilesSourceOptions.scala:573)\n\tat com.databricks.sql.fileNotification.autoIngest.CloudFilesSourceOptions.globber$lzycompute(CloudFilesSourceOptions.scala:582)\n\tat com.databricks.sql.fileNotification.autoIngest.CloudFilesSourceOptions.globber(CloudFilesSourceOptions.scala:581)\n\tat com.databricks.sql.fileNotification.autoIngest.SchemaInferenceUtils$.sampleData(SchemaInferenceUtils.scala:62)\n\tat com.databricks.sql.fileNotification.autoIngest.SchemaInferenceUtils$.getOrUpdatePersistedSchema(SchemaInferenceUtils.scala:173)\n\tat com.databricks.sql.fileNotification.autoIngest.SchemaInferenceUtils$.inferSchema(SchemaInferenceUtils.scala:559)\n\tat com.databricks.sql.fileNotification.autoIngest.CloudFilesSourceProvider.determineSchema(CloudFilesSourceProvider.scala:77)\n\tat com.databricks.sql.fileNotification.autoIngest.CloudFilesSourceProvider.sourceSchema(CloudFilesSourceProvider.scala:114)\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceSchema(DataSource.scala:259)\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceInfo$lzycompute(DataSource.scala:140)\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceInfo(DataSource.scala:140)\n\tat org.apache.spark.sql.execution.streaming.StreamingRelation$.apply(StreamingRelation.scala:34)\n\tat org.apache.spark.sql.streaming.DataStreamReader.loadInternal(DataStreamReader.scala:196)\n\tat org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:238)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n","errorSummary":"java.lang.UnsupportedOperationException: com.databricks.backend.daemon.data.client.DBFSV1.resolvePathOnPhysicalStorage(path: Path)","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n\u001B[0;32m<command-2841292000074646>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mcustomers_checkpoint_path\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34mf\"{DA.paths.checkpoints}/customers\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m (spark.readStream\n\u001B[0m\u001B[1;32m      4\u001B[0m       \u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"cloudFiles\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m       \u001B[0;34m.\u001B[0m\u001B[0moption\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"cloudFiles.format\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"csv\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/streaming.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(self, path, format, schema, **options)\u001B[0m\n\u001B[1;32m    450\u001B[0m                 raise ValueError(\"If the path is provided for stream, it needs to be a \" +\n\u001B[1;32m    451\u001B[0m                                  \"non-empty string. List of paths are not supported.\")\n\u001B[0;32m--> 452\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    453\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    454\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    115\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 117\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    118\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o523.load.\n: java.lang.UnsupportedOperationException: com.databricks.backend.daemon.data.client.DBFSV1.resolvePathOnPhysicalStorage(path: Path)\n\tat com.databricks.tahoe.store.EnhancedDatabricksFileSystemV1.resolvePathOnPhysicalStorage(EnhancedFileSystem.scala:330)\n\tat com.databricks.tahoe.store.S3LogStoreBase.resolvePathOnPhysicalStorage(S3LogStore.scala:298)\n\tat com.databricks.tahoe.store.DelegatingLogStore.resolvePathOnPhysicalStorage(DelegatingLogStore.scala:156)\n\tat com.databricks.sql.fileNotification.autoIngest.CloudFilesSetupOptions.rawResolvedUri$lzycompute(CloudFilesSetupOptions.scala:55)\n\tat com.databricks.sql.fileNotification.autoIngest.CloudFilesSetupOptions.rawResolvedUri(CloudFilesSetupOptions.scala:52)\n\tat com.databricks.sql.fileNotification.autoIngest.CloudFilesSourceOptions.globPath$lzycompute(CloudFilesSourceOptions.scala:574)\n\tat com.databricks.sql.fileNotification.autoIngest.CloudFilesSourceOptions.globPath(CloudFilesSourceOptions.scala:573)\n\tat com.databricks.sql.fileNotification.autoIngest.CloudFilesSourceOptions.globber$lzycompute(CloudFilesSourceOptions.scala:582)\n\tat com.databricks.sql.fileNotification.autoIngest.CloudFilesSourceOptions.globber(CloudFilesSourceOptions.scala:581)\n\tat com.databricks.sql.fileNotification.autoIngest.SchemaInferenceUtils$.sampleData(SchemaInferenceUtils.scala:62)\n\tat com.databricks.sql.fileNotification.autoIngest.SchemaInferenceUtils$.getOrUpdatePersistedSchema(SchemaInferenceUtils.scala:173)\n\tat com.databricks.sql.fileNotification.autoIngest.SchemaInferenceUtils$.inferSchema(SchemaInferenceUtils.scala:559)\n\tat com.databricks.sql.fileNotification.autoIngest.CloudFilesSourceProvider.determineSchema(CloudFilesSourceProvider.scala:77)\n\tat com.databricks.sql.fileNotification.autoIngest.CloudFilesSourceProvider.sourceSchema(CloudFilesSourceProvider.scala:114)\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceSchema(DataSource.scala:259)\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceInfo$lzycompute(DataSource.scala:140)\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceInfo(DataSource.scala:140)\n\tat org.apache.spark.sql.execution.streaming.StreamingRelation$.apply(StreamingRelation.scala:34)\n\tat org.apache.spark.sql.streaming.DataStreamReader.loadInternal(DataStreamReader.scala:196)\n\tat org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:238)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import Row\nassert Row(tableName=\"customers_raw_temp\", isTemporary=True) in spark.sql(\"show tables\").select(\"tableName\", \"isTemporary\").collect(), \"Table not present or not temporary\"\nassert spark.table(\"customers_raw_temp\").dtypes ==  [('customer_id', 'string'),\n ('tax_id', 'string'),\n ('tax_code', 'string'),\n ('customer_name', 'string'),\n ('state', 'string'),\n ('city', 'string'),\n ('postcode', 'string'),\n ('street', 'string'),\n ('number', 'string'),\n ('unit', 'string'),\n ('region', 'string'),\n ('district', 'string'),\n ('lon', 'string'),\n ('lat', 'string'),\n ('ship_to_address', 'string'),\n ('valid_from', 'string'),\n ('valid_to', 'string'),\n ('units_purchased', 'string'),\n ('loyalty_segment', 'string'),\n ('_rescued_data', 'string')], \"Incorrect Schema\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8768b9ec-f49d-4ac9-9ec6-f9e310bbd4ef"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Define a streaming aggregation\n\nUsing CTAS syntax, define a new streaming view called **`customer_count_by_state_temp`** that counts the number of customers per **`state`**, in a field called **`customer_count`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e8191ef-75d2-437e-8879-266d4b67186f"}}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TEMPORARY VIEW customer_count_by_state_temp AS\nSELECT\n  state,\n  count(customer_id) AS customer_count\n  FROM customers_raw_temp\n  GROUP BY\n  state"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f198df08-bbc0-4a4b-bfef-aff7e287caef"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["assert Row(tableName=\"customer_count_by_state_temp\", isTemporary=True) in spark.sql(\"show tables\").select(\"tableName\", \"isTemporary\").collect(), \"Table not present or not temporary\"\nassert spark.table(\"customer_count_by_state_temp\").dtypes == [('state', 'string'), ('customer_count', 'bigint')], \"Incorrect Schema\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a3d62aeb-1847-4bce-80bb-cfd62f686b13"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Write aggregated data to a Delta table\n\nStream data from the **`customer_count_by_state_temp`** view to a Delta table called **`customer_count_by_state`**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"de3b554b-63ab-4b86-930a-4c5379004ea2"}}},{"cell_type":"code","source":["customers_count_checkpoint_path = f\"{DA.paths.checkpoints}/customers_count\"\n\nquery = (spark.table(\"customer_count_by_state_temp\")\n              .writeStream\n              .format(\"delta\")\n              .option(\"checkpointLocation\", customers_count_checkpoint_path)\n              .outputMode(\"complete\")\n              .table(\"customer_count_by_state\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"17be4fd7-79e7-4629-b7ba-be49080cae06"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["DA.block_until_stream_is_ready(query)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4d4559d4-1b07-43d2-bd99-d845b3d7b098"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["assert Row(tableName=\"customer_count_by_state\", isTemporary=False) in spark.sql(\"show tables\").select(\"tableName\", \"isTemporary\").collect(), \"Table not present or not temporary\"\nassert spark.table(\"customer_count_by_state\").dtypes == [('state', 'string'), ('customer_count', 'bigint')], \"Incorrect Schema\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"adf132be-8f72-4c05-8d19-d2b28b43b77f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Query the results\n\nQuery the **`customer_count_by_state`** table (this will not be a streaming query). Plot the results as a bar graph and also using the map plot."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"21d14de3-2975-4ee1-a463-c779f75030ff"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM customer_count_by_state"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"507bbbae-ef6d-4393-b080-4b20f7831718"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Wrapping Up\n\nRun the following cell to remove the database and all data associated with this lab."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e58df71-7f54-489c-9a6c-fa1d81e44f5b"}}},{"cell_type":"code","source":["DA.cleanup()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a4e2d7c6-a278-4d66-be06-47b33d0b19cf"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["By completing this lab, you should now feel comfortable:\n* Using PySpark to configure Auto Loader for incremental data ingestion\n* Using Spark SQL to aggregate streaming data\n* Streaming data to a Delta table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b5097e3d-6bad-4b84-935a-3abbe06d0f93"}}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"147efe36-3d6f-4fea-b679-0f767d1421b1"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"DE 6.3L - Using Auto Loader and Structured Streaming with Spark SQL Lab","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2841292000074640}},"nbformat":4,"nbformat_minor":0}
