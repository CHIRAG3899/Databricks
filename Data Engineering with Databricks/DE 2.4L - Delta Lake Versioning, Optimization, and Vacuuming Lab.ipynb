{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7022c06e-9cec-44f2-b9b9-fbabff5bee5c"}}},{"cell_type":"markdown","source":["# Delta Lake Versioning, Optimization, and Vacuuming\n\nThis notebook provides a hands-on review of some of the more esoteric features Delta Lake brings to the data lakehouse.\n\n## Learning Objectives\nBy the end of this lab, you should be able to:\n- Review table history\n- Query previous table versions and rollback a table to a specific version\n- Perform file compaction and Z-order indexing\n- Preview files marked for permanent deletion and commit these deletes"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c3a696d-4d0d-4594-86ce-a1fc7ffd1a49"}}},{"cell_type":"markdown","source":["## Setup\nRun the following script to setup necessary variables and clear out past runs of this notebook. Note that re-executing this cell will allow you to start the lab over."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e4bc7ff-2a47-4a89-b09a-fe52c081d555"}}},{"cell_type":"code","source":["%run ../Includes/Classroom-Setup-2.4L"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ca6825dc-281f-47cf-aabb-e1a418913c23"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Python interpreter will be restarted.\nPython interpreter will be restarted.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Python interpreter will be restarted.\nPython interpreter will be restarted.\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\nCreating the database \"dbacademy_chiraggoel_kpmg_com_dewd_2_4l\"\n\nPredefined Paths:\n  DA.paths.working_dir: dbfs:/user/chiraggoel@kpmg.com/dbacademy/dewd/2.4l\n  DA.paths.user_db:     dbfs:/user/chiraggoel@kpmg.com/dbacademy/dewd/2.4l/2_4l.db\n\nPredefined tables in dbacademy_chiraggoel_kpmg_com_dewd_2_4l:\n  -none-\n\nSetup completed in 2 seconds\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\nCreating the database \"dbacademy_chiraggoel_kpmg_com_dewd_2_4l\"\n\nPredefined Paths:\n  DA.paths.working_dir: dbfs:/user/chiraggoel@kpmg.com/dbacademy/dewd/2.4l\n  DA.paths.user_db:     dbfs:/user/chiraggoel@kpmg.com/dbacademy/dewd/2.4l/2_4l.db\n\nPredefined tables in dbacademy_chiraggoel_kpmg_com_dewd_2_4l:\n  -none-\n\nSetup completed in 2 seconds\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Recreate the History of your Bean Collection\n\nThis lab picks up where the last lab left off. The cell below condenses all the operations from the last lab into a single cell (other than the final **`DROP TABLE`** statement).\n\nFor quick reference, the schema of the **`beans`** table created is:\n\n| Field Name | Field type |\n| --- | --- |\n| name | STRING |\n| color | STRING |\n| grams | FLOAT |\n| delicious | BOOLEAN |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3f9de610-d470-493d-9e6a-1c10a2e43b89"}}},{"cell_type":"code","source":["%sql\nCREATE TABLE beans \n(name STRING, color STRING, grams FLOAT, delicious BOOLEAN);\n\nINSERT INTO beans VALUES\n(\"black\", \"black\", 500, true),\n(\"lentils\", \"brown\", 1000, true),\n(\"jelly\", \"rainbow\", 42.5, false);\n\nINSERT INTO beans VALUES\n('pinto', 'brown', 1.5, true),\n('green', 'green', 178.3, true),\n('beanbag chair', 'white', 40000, false);\n\nUPDATE beans\nSET delicious = true\nWHERE name = \"jelly\";\n\nUPDATE beans\nSET grams = 1500\nWHERE name = 'pinto';\n\nDELETE FROM beans\nWHERE delicious = false;\n\nCREATE OR REPLACE TEMP VIEW new_beans(name, color, grams, delicious) AS VALUES\n('black', 'black', 60.5, true),\n('lentils', 'green', 500, true),\n('kidney', 'red', 387.2, true),\n('castor', 'brown', 25, false);\n\nMERGE INTO beans a\nUSING new_beans b\nON a.name=b.name AND a.color = b.color\nWHEN MATCHED THEN\n  UPDATE SET grams = a.grams + b.grams\nWHEN NOT MATCHED AND b.delicious = true THEN\n  INSERT *;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e8beab84-abf4-46f7-b6e5-0c2e1be669f3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[3,1,0,2]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"num_affected_rows","type":"\"long\"","metadata":"{}"},{"name":"num_updated_rows","type":"\"long\"","metadata":"{}"},{"name":"num_deleted_rows","type":"\"long\"","metadata":"{}"},{"name":"num_inserted_rows","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_updated_rows</th><th>num_deleted_rows</th><th>num_inserted_rows</th></tr></thead><tbody><tr><td>3</td><td>1</td><td>0</td><td>2</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Review the Table History\n\nDelta Lake's transaction log stores information about each transaction that modifies a table's contents or settings.\n\nReview the history of the **`beans`** table below."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d311bac8-0ede-42bc-b113-2fc81eda3f4a"}}},{"cell_type":"code","source":["%sql\nDESCRIBE HISTORY beans"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0cc1de4c-432f-4ab2-9296-489e57e5d2c1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[6,"2022-05-22T11:39:09.000+0000","6138111496777303","chiraggoel@kpmg.com","MERGE",{"predicate":"((a.name = b.name) AND (a.color = b.color))","matchedPredicates":"[{\"actionType\":\"update\"}]","notMatchedPredicates":"[{\"predicate\":\"b.delicious\",\"actionType\":\"insert\"}]"},null,["2841292000074433"],"0522-095621-z1upwij9",5,"WriteSerializable",false,{"numTargetRowsCopied":"0","numTargetRowsDeleted":"0","numTargetFilesAdded":"1","executionTimeMs":"4079","numTargetRowsInserted":"2","scanTimeMs":"1846","numTargetRowsUpdated":"1","numOutputRows":"3","numTargetChangeFilesAdded":"0","numSourceRows":"4","numTargetFilesRemoved":"1","rewriteTimeMs":"2138"},null,"Databricks-Runtime/10.4.x-scala2.12"],[5,"2022-05-22T11:39:02.000+0000","6138111496777303","chiraggoel@kpmg.com","DELETE",{"predicate":"[\"(NOT spark_catalog.dbacademy_chiraggoel_kpmg_com_dewd_2_4l.beans.delicious)\"]"},null,["2841292000074433"],"0522-095621-z1upwij9",4,"WriteSerializable",false,{"numRemovedFiles":"1","numCopiedRows":"0","numAddedChangeFiles":"0","executionTimeMs":"1722","numDeletedRows":"1","scanTimeMs":"1141","numAddedFiles":"0","rewriteTimeMs":"581"},null,"Databricks-Runtime/10.4.x-scala2.12"],[4,"2022-05-22T11:38:58.000+0000","6138111496777303","chiraggoel@kpmg.com","UPDATE",{"predicate":"(name#14611 = pinto)"},null,["2841292000074433"],"0522-095621-z1upwij9",3,"WriteSerializable",false,{"numRemovedFiles":"1","numCopiedRows":"0","numAddedChangeFiles":"0","executionTimeMs":"1927","scanTimeMs":"139","numAddedFiles":"1","numUpdatedRows":"1","rewriteTimeMs":"1787"},null,"Databricks-Runtime/10.4.x-scala2.12"],[3,"2022-05-22T11:38:53.000+0000","6138111496777303","chiraggoel@kpmg.com","UPDATE",{"predicate":"(name#14060 = jelly)"},null,["2841292000074433"],"0522-095621-z1upwij9",2,"WriteSerializable",false,{"numRemovedFiles":"1","numCopiedRows":"0","numAddedChangeFiles":"0","executionTimeMs":"2193","scanTimeMs":"192","numAddedFiles":"1","numUpdatedRows":"1","rewriteTimeMs":"2001"},null,"Databricks-Runtime/10.4.x-scala2.12"],[2,"2022-05-22T11:38:48.000+0000","6138111496777303","chiraggoel@kpmg.com","WRITE",{"mode":"Append","partitionBy":"[]"},null,["2841292000074433"],"0522-095621-z1upwij9",1,"WriteSerializable",true,{"numFiles":"3","numOutputRows":"3","numOutputBytes":"3908"},null,"Databricks-Runtime/10.4.x-scala2.12"],[1,"2022-05-22T11:38:44.000+0000","6138111496777303","chiraggoel@kpmg.com","WRITE",{"mode":"Append","partitionBy":"[]"},null,["2841292000074433"],"0522-095621-z1upwij9",0,"WriteSerializable",true,{"numFiles":"3","numOutputRows":"3","numOutputBytes":"3880"},null,"Databricks-Runtime/10.4.x-scala2.12"],[0,"2022-05-22T11:38:38.000+0000","6138111496777303","chiraggoel@kpmg.com","CREATE TABLE",{"isManaged":"true","description":null,"partitionBy":"[]","properties":"{}"},null,["2841292000074433"],"0522-095621-z1upwij9",null,"WriteSerializable",true,{},null,"Databricks-Runtime/10.4.x-scala2.12"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"version","type":"\"long\"","metadata":"{}"},{"name":"timestamp","type":"\"timestamp\"","metadata":"{}"},{"name":"userId","type":"\"string\"","metadata":"{}"},{"name":"userName","type":"\"string\"","metadata":"{}"},{"name":"operation","type":"\"string\"","metadata":"{}"},{"name":"operationParameters","type":"{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}","metadata":"{}"},{"name":"job","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"jobId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"runId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobOwnerId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"triggerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}","metadata":"{}"},{"name":"notebook","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"notebookId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}","metadata":"{}"},{"name":"clusterId","type":"\"string\"","metadata":"{}"},{"name":"readVersion","type":"\"long\"","metadata":"{}"},{"name":"isolationLevel","type":"\"string\"","metadata":"{}"},{"name":"isBlindAppend","type":"\"boolean\"","metadata":"{}"},{"name":"operationMetrics","type":"{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}","metadata":"{}"},{"name":"userMetadata","type":"\"string\"","metadata":"{}"},{"name":"engineInfo","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr></thead><tbody><tr><td>6</td><td>2022-05-22T11:39:09.000+0000</td><td>6138111496777303</td><td>chiraggoel@kpmg.com</td><td>MERGE</td><td>Map(predicate -> ((a.name = b.name) AND (a.color = b.color)), matchedPredicates -> [{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"predicate\":\"b.delicious\",\"actionType\":\"insert\"}])</td><td>null</td><td>List(2841292000074433)</td><td>0522-095621-z1upwij9</td><td>5</td><td>WriteSerializable</td><td>false</td><td>Map(numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, executionTimeMs -> 4079, numTargetRowsInserted -> 2, scanTimeMs -> 1846, numTargetRowsUpdated -> 1, numOutputRows -> 3, numTargetChangeFilesAdded -> 0, numSourceRows -> 4, numTargetFilesRemoved -> 1, rewriteTimeMs -> 2138)</td><td>null</td><td>Databricks-Runtime/10.4.x-scala2.12</td></tr><tr><td>5</td><td>2022-05-22T11:39:02.000+0000</td><td>6138111496777303</td><td>chiraggoel@kpmg.com</td><td>DELETE</td><td>Map(predicate -> [\"(NOT spark_catalog.dbacademy_chiraggoel_kpmg_com_dewd_2_4l.beans.delicious)\"])</td><td>null</td><td>List(2841292000074433)</td><td>0522-095621-z1upwij9</td><td>4</td><td>WriteSerializable</td><td>false</td><td>Map(numRemovedFiles -> 1, numCopiedRows -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 1722, numDeletedRows -> 1, scanTimeMs -> 1141, numAddedFiles -> 0, rewriteTimeMs -> 581)</td><td>null</td><td>Databricks-Runtime/10.4.x-scala2.12</td></tr><tr><td>4</td><td>2022-05-22T11:38:58.000+0000</td><td>6138111496777303</td><td>chiraggoel@kpmg.com</td><td>UPDATE</td><td>Map(predicate -> (name#14611 = pinto))</td><td>null</td><td>List(2841292000074433)</td><td>0522-095621-z1upwij9</td><td>3</td><td>WriteSerializable</td><td>false</td><td>Map(numRemovedFiles -> 1, numCopiedRows -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 1927, scanTimeMs -> 139, numAddedFiles -> 1, numUpdatedRows -> 1, rewriteTimeMs -> 1787)</td><td>null</td><td>Databricks-Runtime/10.4.x-scala2.12</td></tr><tr><td>3</td><td>2022-05-22T11:38:53.000+0000</td><td>6138111496777303</td><td>chiraggoel@kpmg.com</td><td>UPDATE</td><td>Map(predicate -> (name#14060 = jelly))</td><td>null</td><td>List(2841292000074433)</td><td>0522-095621-z1upwij9</td><td>2</td><td>WriteSerializable</td><td>false</td><td>Map(numRemovedFiles -> 1, numCopiedRows -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 2193, scanTimeMs -> 192, numAddedFiles -> 1, numUpdatedRows -> 1, rewriteTimeMs -> 2001)</td><td>null</td><td>Databricks-Runtime/10.4.x-scala2.12</td></tr><tr><td>2</td><td>2022-05-22T11:38:48.000+0000</td><td>6138111496777303</td><td>chiraggoel@kpmg.com</td><td>WRITE</td><td>Map(mode -> Append, partitionBy -> [])</td><td>null</td><td>List(2841292000074433)</td><td>0522-095621-z1upwij9</td><td>1</td><td>WriteSerializable</td><td>true</td><td>Map(numFiles -> 3, numOutputRows -> 3, numOutputBytes -> 3908)</td><td>null</td><td>Databricks-Runtime/10.4.x-scala2.12</td></tr><tr><td>1</td><td>2022-05-22T11:38:44.000+0000</td><td>6138111496777303</td><td>chiraggoel@kpmg.com</td><td>WRITE</td><td>Map(mode -> Append, partitionBy -> [])</td><td>null</td><td>List(2841292000074433)</td><td>0522-095621-z1upwij9</td><td>0</td><td>WriteSerializable</td><td>true</td><td>Map(numFiles -> 3, numOutputRows -> 3, numOutputBytes -> 3880)</td><td>null</td><td>Databricks-Runtime/10.4.x-scala2.12</td></tr><tr><td>0</td><td>2022-05-22T11:38:38.000+0000</td><td>6138111496777303</td><td>chiraggoel@kpmg.com</td><td>CREATE TABLE</td><td>Map(isManaged -> true, description -> null, partitionBy -> [], properties -> {})</td><td>null</td><td>List(2841292000074433)</td><td>0522-095621-z1upwij9</td><td>null</td><td>WriteSerializable</td><td>true</td><td>Map()</td><td>null</td><td>Databricks-Runtime/10.4.x-scala2.12</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["If all the previous operations were completed as described you should see 7 versions of the table (**NOTE**: Delta Lake versioning starts with 0, so the max version number will be 6).\n\nThe operations should be as follows:\n\n| version | operation |\n| --- | --- |\n| 0 | CREATE TABLE |\n| 1 | WRITE |\n| 2 | WRITE |\n| 3 | UPDATE |\n| 4 | UPDATE |\n| 5 | DELETE |\n| 6 | MERGE |\n\nThe **`operationsParameters`** column will let you review predicates used for updates, deletes, and merges. The **`operationMetrics`** column indicates how many rows and files are added in each operation.\n\nSpend some time reviewing the Delta Lake history to understand which table version matches with a given transaction.\n\n**NOTE**: The **`version`** column designates the state of a table once a given transaction completes. The **`readVersion`** column indicates the version of the table an operation executed against. In this simple demo (with no concurrent transactions), this relationship should always increment by 1."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eee0db7b-20e3-4d9e-842b-39120f1f4f84"}}},{"cell_type":"markdown","source":["## Query a Specific Version\n\nAfter reviewing the table history, you decide you want to view the state of your table after your very first data was inserted.\n\nRun the query below to see this."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ebc8511-375d-4550-9cab-f47b5fea3ba0"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM beans VERSION AS OF 1"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"39ce4b6e-fd49-4e8c-9bf6-b8c747595819"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["lentils","brown",1000.0,true],["jelly","rainbow",42.5,false],["black","black",500.0,true]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"color","type":"\"string\"","metadata":"{}"},{"name":"grams","type":"\"float\"","metadata":"{}"},{"name":"delicious","type":"\"boolean\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>color</th><th>grams</th><th>delicious</th></tr></thead><tbody><tr><td>lentils</td><td>brown</td><td>1000.0</td><td>true</td></tr><tr><td>jelly</td><td>rainbow</td><td>42.5</td><td>false</td></tr><tr><td>black</td><td>black</td><td>500.0</td><td>true</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["And now review the current state of your data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a484d05b-30f3-4018-a0cf-d1220845ff97"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM beans"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"02cc1c22-e4c3-445a-ac6c-a655ca55eb57"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["black","black",560.5,true],["kidney","red",387.2,true],["lentils","green",500.0,true],["lentils","brown",1000.0,true],["jelly","rainbow",42.5,true],["green","green",178.3,true],["pinto","brown",1500.0,true]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"color","type":"\"string\"","metadata":"{}"},{"name":"grams","type":"\"float\"","metadata":"{}"},{"name":"delicious","type":"\"boolean\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>color</th><th>grams</th><th>delicious</th></tr></thead><tbody><tr><td>black</td><td>black</td><td>560.5</td><td>true</td></tr><tr><td>kidney</td><td>red</td><td>387.2</td><td>true</td></tr><tr><td>lentils</td><td>green</td><td>500.0</td><td>true</td></tr><tr><td>lentils</td><td>brown</td><td>1000.0</td><td>true</td></tr><tr><td>jelly</td><td>rainbow</td><td>42.5</td><td>true</td></tr><tr><td>green</td><td>green</td><td>178.3</td><td>true</td></tr><tr><td>pinto</td><td>brown</td><td>1500.0</td><td>true</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["You want to review the weights of your beans before you deleted any records.\n\nFill in the statement below to register a temporary view of the version just before data was deleted, then run the following cell to query the view."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6daa39ee-528d-48ac-9a23-d2070479e054"}}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TEMP VIEW pre_delete_vw AS\n  SELECT * FROM beans VERSION AS OF 4;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5dc09866-3c8c-44aa-9741-7efff521ba1b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT * FROM pre_delete_vw"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7ae2c393-0e85-4b8d-a17f-0a72f5487f54"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["beanbag chair","white",40000.0,false],["lentils","brown",1000.0,true],["jelly","rainbow",42.5,true],["black","black",500.0,true],["green","green",178.3,true],["pinto","brown",1500.0,true]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"color","type":"\"string\"","metadata":"{}"},{"name":"grams","type":"\"float\"","metadata":"{}"},{"name":"delicious","type":"\"boolean\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>color</th><th>grams</th><th>delicious</th></tr></thead><tbody><tr><td>beanbag chair</td><td>white</td><td>40000.0</td><td>false</td></tr><tr><td>lentils</td><td>brown</td><td>1000.0</td><td>true</td></tr><tr><td>jelly</td><td>rainbow</td><td>42.5</td><td>true</td></tr><tr><td>black</td><td>black</td><td>500.0</td><td>true</td></tr><tr><td>green</td><td>green</td><td>178.3</td><td>true</td></tr><tr><td>pinto</td><td>brown</td><td>1500.0</td><td>true</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Run the cell below to check that you have captured the correct version."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d8cf4030-86ff-4dcb-a392-52e11eca1856"}}},{"cell_type":"code","source":["%python\nassert spark.table(\"pre_delete_vw\"), \"Make sure you have registered the temporary view with the provided name `pre_delete_vw`\"\nassert spark.table(\"pre_delete_vw\").count() == 6, \"Make sure you're querying a version of the table with 6 records\"\nassert spark.table(\"pre_delete_vw\").selectExpr(\"int(sum(grams))\").first()[0] == 43220, \"Make sure you query the version of the table after updates were applied\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d4e97ad3-a85d-42bd-944e-c4dd71bfff49"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Restore a Previous Version\n\nApparently there was a misunderstanding; the beans your friend gave you that you merged into your collection were not intended for you to keep.\n\nRevert your table to the version before this **`MERGE`** statement completed."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb746bbe-3470-486a-9aa4-842c1fd053ff"}}},{"cell_type":"code","source":["%sql\nRESTORE TABLE beans TO VERSION AS OF 5"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b176d4bf-b788-4fc9-82ab-de4a1e9d241c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[6448,5,1,1,1304,1284]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"table_size_after_restore","type":"\"long\"","metadata":"{}"},{"name":"num_of_files_after_restore","type":"\"long\"","metadata":"{}"},{"name":"num_removed_files","type":"\"long\"","metadata":"{}"},{"name":"num_restored_files","type":"\"long\"","metadata":"{}"},{"name":"removed_files_size","type":"\"long\"","metadata":"{}"},{"name":"restored_files_size","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>table_size_after_restore</th><th>num_of_files_after_restore</th><th>num_removed_files</th><th>num_restored_files</th><th>removed_files_size</th><th>restored_files_size</th></tr></thead><tbody><tr><td>6448</td><td>5</td><td>1</td><td>1</td><td>1304</td><td>1284</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Review the history of your table. Make note of the fact that restoring to a previous version adds another table version."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2204ab8a-3439-4b46-bc53-e1e167652bfd"}}},{"cell_type":"code","source":["%sql\nDESCRIBE HISTORY beans"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"053aa067-1e6d-4e54-bc67-a0a73ec45ae0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[7,"2022-05-22T11:40:47.000+0000","6138111496777303","chiraggoel@kpmg.com","RESTORE",{"version":"5","timestamp":null},null,["2841292000074433"],"0522-095621-z1upwij9",6,"Serializable",false,{"numRestoredFiles":"1","removedFilesSize":"1304","numRemovedFiles":"1","restoredFilesSize":"1284","numOfFilesAfterRestore":"5","tableSizeAfterRestore":"6448"},null,"Databricks-Runtime/10.4.x-scala2.12"],[6,"2022-05-22T11:39:09.000+0000","6138111496777303","chiraggoel@kpmg.com","MERGE",{"predicate":"((a.name = b.name) AND (a.color = b.color))","matchedPredicates":"[{\"actionType\":\"update\"}]","notMatchedPredicates":"[{\"predicate\":\"b.delicious\",\"actionType\":\"insert\"}]"},null,["2841292000074433"],"0522-095621-z1upwij9",5,"WriteSerializable",false,{"numTargetRowsCopied":"0","numTargetRowsDeleted":"0","numTargetFilesAdded":"1","executionTimeMs":"4079","numTargetRowsInserted":"2","scanTimeMs":"1846","numTargetRowsUpdated":"1","numOutputRows":"3","numTargetChangeFilesAdded":"0","numSourceRows":"4","numTargetFilesRemoved":"1","rewriteTimeMs":"2138"},null,"Databricks-Runtime/10.4.x-scala2.12"],[5,"2022-05-22T11:39:02.000+0000","6138111496777303","chiraggoel@kpmg.com","DELETE",{"predicate":"[\"(NOT spark_catalog.dbacademy_chiraggoel_kpmg_com_dewd_2_4l.beans.delicious)\"]"},null,["2841292000074433"],"0522-095621-z1upwij9",4,"WriteSerializable",false,{"numRemovedFiles":"1","numCopiedRows":"0","numAddedChangeFiles":"0","executionTimeMs":"1722","numDeletedRows":"1","scanTimeMs":"1141","numAddedFiles":"0","rewriteTimeMs":"581"},null,"Databricks-Runtime/10.4.x-scala2.12"],[4,"2022-05-22T11:38:58.000+0000","6138111496777303","chiraggoel@kpmg.com","UPDATE",{"predicate":"(name#14611 = pinto)"},null,["2841292000074433"],"0522-095621-z1upwij9",3,"WriteSerializable",false,{"numRemovedFiles":"1","numCopiedRows":"0","numAddedChangeFiles":"0","executionTimeMs":"1927","scanTimeMs":"139","numAddedFiles":"1","numUpdatedRows":"1","rewriteTimeMs":"1787"},null,"Databricks-Runtime/10.4.x-scala2.12"],[3,"2022-05-22T11:38:53.000+0000","6138111496777303","chiraggoel@kpmg.com","UPDATE",{"predicate":"(name#14060 = jelly)"},null,["2841292000074433"],"0522-095621-z1upwij9",2,"WriteSerializable",false,{"numRemovedFiles":"1","numCopiedRows":"0","numAddedChangeFiles":"0","executionTimeMs":"2193","scanTimeMs":"192","numAddedFiles":"1","numUpdatedRows":"1","rewriteTimeMs":"2001"},null,"Databricks-Runtime/10.4.x-scala2.12"],[2,"2022-05-22T11:38:48.000+0000","6138111496777303","chiraggoel@kpmg.com","WRITE",{"mode":"Append","partitionBy":"[]"},null,["2841292000074433"],"0522-095621-z1upwij9",1,"WriteSerializable",true,{"numFiles":"3","numOutputRows":"3","numOutputBytes":"3908"},null,"Databricks-Runtime/10.4.x-scala2.12"],[1,"2022-05-22T11:38:44.000+0000","6138111496777303","chiraggoel@kpmg.com","WRITE",{"mode":"Append","partitionBy":"[]"},null,["2841292000074433"],"0522-095621-z1upwij9",0,"WriteSerializable",true,{"numFiles":"3","numOutputRows":"3","numOutputBytes":"3880"},null,"Databricks-Runtime/10.4.x-scala2.12"],[0,"2022-05-22T11:38:38.000+0000","6138111496777303","chiraggoel@kpmg.com","CREATE TABLE",{"isManaged":"true","description":null,"partitionBy":"[]","properties":"{}"},null,["2841292000074433"],"0522-095621-z1upwij9",null,"WriteSerializable",true,{},null,"Databricks-Runtime/10.4.x-scala2.12"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"version","type":"\"long\"","metadata":"{}"},{"name":"timestamp","type":"\"timestamp\"","metadata":"{}"},{"name":"userId","type":"\"string\"","metadata":"{}"},{"name":"userName","type":"\"string\"","metadata":"{}"},{"name":"operation","type":"\"string\"","metadata":"{}"},{"name":"operationParameters","type":"{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}","metadata":"{}"},{"name":"job","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"jobId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"runId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobOwnerId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"triggerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}","metadata":"{}"},{"name":"notebook","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"notebookId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}","metadata":"{}"},{"name":"clusterId","type":"\"string\"","metadata":"{}"},{"name":"readVersion","type":"\"long\"","metadata":"{}"},{"name":"isolationLevel","type":"\"string\"","metadata":"{}"},{"name":"isBlindAppend","type":"\"boolean\"","metadata":"{}"},{"name":"operationMetrics","type":"{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}","metadata":"{}"},{"name":"userMetadata","type":"\"string\"","metadata":"{}"},{"name":"engineInfo","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr></thead><tbody><tr><td>7</td><td>2022-05-22T11:40:47.000+0000</td><td>6138111496777303</td><td>chiraggoel@kpmg.com</td><td>RESTORE</td><td>Map(version -> 5, timestamp -> null)</td><td>null</td><td>List(2841292000074433)</td><td>0522-095621-z1upwij9</td><td>6</td><td>Serializable</td><td>false</td><td>Map(numRestoredFiles -> 1, removedFilesSize -> 1304, numRemovedFiles -> 1, restoredFilesSize -> 1284, numOfFilesAfterRestore -> 5, tableSizeAfterRestore -> 6448)</td><td>null</td><td>Databricks-Runtime/10.4.x-scala2.12</td></tr><tr><td>6</td><td>2022-05-22T11:39:09.000+0000</td><td>6138111496777303</td><td>chiraggoel@kpmg.com</td><td>MERGE</td><td>Map(predicate -> ((a.name = b.name) AND (a.color = b.color)), matchedPredicates -> [{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"predicate\":\"b.delicious\",\"actionType\":\"insert\"}])</td><td>null</td><td>List(2841292000074433)</td><td>0522-095621-z1upwij9</td><td>5</td><td>WriteSerializable</td><td>false</td><td>Map(numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, executionTimeMs -> 4079, numTargetRowsInserted -> 2, scanTimeMs -> 1846, numTargetRowsUpdated -> 1, numOutputRows -> 3, numTargetChangeFilesAdded -> 0, numSourceRows -> 4, numTargetFilesRemoved -> 1, rewriteTimeMs -> 2138)</td><td>null</td><td>Databricks-Runtime/10.4.x-scala2.12</td></tr><tr><td>5</td><td>2022-05-22T11:39:02.000+0000</td><td>6138111496777303</td><td>chiraggoel@kpmg.com</td><td>DELETE</td><td>Map(predicate -> [\"(NOT spark_catalog.dbacademy_chiraggoel_kpmg_com_dewd_2_4l.beans.delicious)\"])</td><td>null</td><td>List(2841292000074433)</td><td>0522-095621-z1upwij9</td><td>4</td><td>WriteSerializable</td><td>false</td><td>Map(numRemovedFiles -> 1, numCopiedRows -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 1722, numDeletedRows -> 1, scanTimeMs -> 1141, numAddedFiles -> 0, rewriteTimeMs -> 581)</td><td>null</td><td>Databricks-Runtime/10.4.x-scala2.12</td></tr><tr><td>4</td><td>2022-05-22T11:38:58.000+0000</td><td>6138111496777303</td><td>chiraggoel@kpmg.com</td><td>UPDATE</td><td>Map(predicate -> (name#14611 = pinto))</td><td>null</td><td>List(2841292000074433)</td><td>0522-095621-z1upwij9</td><td>3</td><td>WriteSerializable</td><td>false</td><td>Map(numRemovedFiles -> 1, numCopiedRows -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 1927, scanTimeMs -> 139, numAddedFiles -> 1, numUpdatedRows -> 1, rewriteTimeMs -> 1787)</td><td>null</td><td>Databricks-Runtime/10.4.x-scala2.12</td></tr><tr><td>3</td><td>2022-05-22T11:38:53.000+0000</td><td>6138111496777303</td><td>chiraggoel@kpmg.com</td><td>UPDATE</td><td>Map(predicate -> (name#14060 = jelly))</td><td>null</td><td>List(2841292000074433)</td><td>0522-095621-z1upwij9</td><td>2</td><td>WriteSerializable</td><td>false</td><td>Map(numRemovedFiles -> 1, numCopiedRows -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 2193, scanTimeMs -> 192, numAddedFiles -> 1, numUpdatedRows -> 1, rewriteTimeMs -> 2001)</td><td>null</td><td>Databricks-Runtime/10.4.x-scala2.12</td></tr><tr><td>2</td><td>2022-05-22T11:38:48.000+0000</td><td>6138111496777303</td><td>chiraggoel@kpmg.com</td><td>WRITE</td><td>Map(mode -> Append, partitionBy -> [])</td><td>null</td><td>List(2841292000074433)</td><td>0522-095621-z1upwij9</td><td>1</td><td>WriteSerializable</td><td>true</td><td>Map(numFiles -> 3, numOutputRows -> 3, numOutputBytes -> 3908)</td><td>null</td><td>Databricks-Runtime/10.4.x-scala2.12</td></tr><tr><td>1</td><td>2022-05-22T11:38:44.000+0000</td><td>6138111496777303</td><td>chiraggoel@kpmg.com</td><td>WRITE</td><td>Map(mode -> Append, partitionBy -> [])</td><td>null</td><td>List(2841292000074433)</td><td>0522-095621-z1upwij9</td><td>0</td><td>WriteSerializable</td><td>true</td><td>Map(numFiles -> 3, numOutputRows -> 3, numOutputBytes -> 3880)</td><td>null</td><td>Databricks-Runtime/10.4.x-scala2.12</td></tr><tr><td>0</td><td>2022-05-22T11:38:38.000+0000</td><td>6138111496777303</td><td>chiraggoel@kpmg.com</td><td>CREATE TABLE</td><td>Map(isManaged -> true, description -> null, partitionBy -> [], properties -> {})</td><td>null</td><td>List(2841292000074433)</td><td>0522-095621-z1upwij9</td><td>null</td><td>WriteSerializable</td><td>true</td><td>Map()</td><td>null</td><td>Databricks-Runtime/10.4.x-scala2.12</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%python\nlast_tx = spark.conf.get(\"spark.databricks.delta.lastCommitVersionInSession\")\nassert spark.sql(f\"DESCRIBE HISTORY beans\").select(\"operation\").first()[0] == \"RESTORE\", \"Make sure you reverted your table with the `RESTORE` keyword\"\nassert spark.table(\"beans\").count() == 5, \"Make sure you reverted to the version after deleting records but before merging\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5e9c4e3-489d-4269-8836-7e98e99a4d08"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## File Compaction\nLooking at the transaction metrics during your reversion, you are surprised you have some many files for such a small collection of data.\n\nWhile indexing on a table of this size is unlikely to improve performance, you decide to add a Z-order index on the **`name`** field in anticipation of your bean collection growing exponentially over time.\n\nUse the cell below to perform file compaction and Z-order indexing."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"00ebdc38-18c2-485f-ad5a-e9077c62f833"}}},{"cell_type":"code","source":["%sql\nOPTIMIZE beans\nZORDER BY name"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"01124e6d-b782-4c57-b362-ffd7571a0514"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/user/chiraggoel@kpmg.com/dbacademy/dewd/2.4l/2_4l.db/beans",[1,5,[1381,1381,1381.0,1,1381],[1284,1298,1289.6,5,6448],0,["minCubeSize(107374182400)",[0,0],[5,6448],0,[5,6448],1,null],1,5,0,false]]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"metrics","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"numFilesAdded\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numFilesRemoved\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"filesAdded\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"min\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"max\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"avg\",\"type\":\"double\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalFiles\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalSize\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"filesRemoved\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"min\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"max\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"avg\",\"type\":\"double\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalFiles\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalSize\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"partitionsOptimized\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"zOrderStats\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"strategyName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"inputCubeFiles\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"num\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"inputOtherFiles\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"num\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"inputNumCubes\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"mergedFiles\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"num\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"numOutputCubes\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"mergedNumCubes\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"numBatches\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalConsideredFiles\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalFilesSkipped\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"preserveInsertionOrder\",\"type\":\"boolean\",\"nullable\":false,\"metadata\":{}}]}","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>metrics</th></tr></thead><tbody><tr><td>dbfs:/user/chiraggoel@kpmg.com/dbacademy/dewd/2.4l/2_4l.db/beans</td><td>List(1, 5, List(1381, 1381, 1381.0, 1, 1381), List(1284, 1298, 1289.6, 5, 6448), 0, List(minCubeSize(107374182400), List(0, 0), List(5, 6448), 0, List(5, 6448), 1, null), 1, 5, 0, false)</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Your data should have been compacted to a single file; confirm this manually by running the following cell."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc189ebf-4ea5-4d04-a69e-dadb5f6cefaa"}}},{"cell_type":"code","source":["%sql\nDESCRIBE DETAIL beans"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9d2376d7-446a-4851-b3e7-2de9013a7cfb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["delta","ce471376-3d03-4049-9078-5513ac1542f3","dbacademy_chiraggoel_kpmg_com_dewd_2_4l.beans",null,"dbfs:/user/chiraggoel@kpmg.com/dbacademy/dewd/2.4l/2_4l.db/beans","2022-05-22T11:38:36.609+0000","2022-05-22T11:41:26.000+0000",[],1,1381,{},1,2]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"format","type":"\"string\"","metadata":"{}"},{"name":"id","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"description","type":"\"string\"","metadata":"{}"},{"name":"location","type":"\"string\"","metadata":"{}"},{"name":"createdAt","type":"\"timestamp\"","metadata":"{}"},{"name":"lastModified","type":"\"timestamp\"","metadata":"{}"},{"name":"partitionColumns","type":"{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}","metadata":"{}"},{"name":"numFiles","type":"\"long\"","metadata":"{}"},{"name":"sizeInBytes","type":"\"long\"","metadata":"{}"},{"name":"properties","type":"{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}","metadata":"{}"},{"name":"minReaderVersion","type":"\"integer\"","metadata":"{}"},{"name":"minWriterVersion","type":"\"integer\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>format</th><th>id</th><th>name</th><th>description</th><th>location</th><th>createdAt</th><th>lastModified</th><th>partitionColumns</th><th>numFiles</th><th>sizeInBytes</th><th>properties</th><th>minReaderVersion</th><th>minWriterVersion</th></tr></thead><tbody><tr><td>delta</td><td>ce471376-3d03-4049-9078-5513ac1542f3</td><td>dbacademy_chiraggoel_kpmg_com_dewd_2_4l.beans</td><td>null</td><td>dbfs:/user/chiraggoel@kpmg.com/dbacademy/dewd/2.4l/2_4l.db/beans</td><td>2022-05-22T11:38:36.609+0000</td><td>2022-05-22T11:41:26.000+0000</td><td>List()</td><td>1</td><td>1381</td><td>Map()</td><td>1</td><td>2</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Run the cell below to check that you've successfully optimized and indexed your table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4002903e-d751-4d0b-903b-ce8e0eb21528"}}},{"cell_type":"code","source":["%python\nlast_tx = spark.sql(\"DESCRIBE HISTORY beans\").first()\nassert last_tx[\"operation\"] == \"OPTIMIZE\", \"Make sure you used the `OPTIMIZE` command to perform file compaction\"\nassert last_tx[\"operationParameters\"][\"zOrderBy\"] == '[\"name\"]', \"Use `ZORDER BY name` with your optimize command to index your table\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7bea4c5f-f411-4bfa-a0c9-d00547221b45"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Cleaning Up Stale Data Files\n\nYou know that while all your data now resides in 1 data file, the data files from previous versions of your table are still being stored alongside this. You wish to remove these files and remove access to previous versions of the table by running **`VACUUM`** on the table.\n\nExecuting **`VACUUM`** performs garbage cleanup on the table directory. By default, a retention threshold of 7 days will be enforced.\n\nThe cell below modifies some Spark configurations. The first command overrides the retention threshold check to allow us to demonstrate permanent removal of data. \n\n**NOTE**: Vacuuming a production table with a short retention can lead to data corruption and/or failure of long-running queries. This is for demonstration purposes only and extreme caution should be used when disabling this setting.\n\nThe second command sets **`spark.databricks.delta.vacuum.logging.enabled`** to **`true`** to ensure that the **`VACUUM`** operation is recorded in the transaction log.\n\n**NOTE**: Because of slight differences in storage protocols on various clouds, logging **`VACUUM`** commands is not on by default for some clouds as of DBR 9.1."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"554ee9d9-53a0-46ec-ac3e-9defd25f4fbf"}}},{"cell_type":"code","source":["%sql\nSET spark.databricks.delta.retentionDurationCheck.enabled = false;\nSET spark.databricks.delta.vacuum.logging.enabled = true;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"99199133-46ea-4f6d-a72f-b47800534431"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["spark.databricks.delta.vacuum.logging.enabled","true"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"key","type":"\"string\"","metadata":"{}"},{"name":"value","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>key</th><th>value</th></tr></thead><tbody><tr><td>spark.databricks.delta.vacuum.logging.enabled</td><td>true</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Before permanently deleting data files, review them manually using the **`DRY RUN`** option."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0bba5a4f-cc6a-4dd2-9d95-87a899c7a3ca"}}},{"cell_type":"code","source":["%sql\nVACUUM beans RETAIN 0 HOURS DRY RUN"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"edc2b63f-de3b-471d-922c-d46bf49ad6a2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/user/chiraggoel@kpmg.com/dbacademy/dewd/2.4l/2_4l.db/beans/part-00000-02553a1a-d8b3-4e0a-9daf-06ecf4b103b4-c000.snappy.parquet"],["dbfs:/user/chiraggoel@kpmg.com/dbacademy/dewd/2.4l/2_4l.db/beans/part-00000-04e0d37a-7cee-488d-bc0e-54cace751fc9-c000.snappy.parquet"],["dbfs:/user/chiraggoel@kpmg.com/dbacademy/dewd/2.4l/2_4l.db/beans/part-00000-36a12a65-5987-4c51-a406-79f346abf35a-c000.snappy.parquet"],["dbfs:/user/chiraggoel@kpmg.com/dbacademy/dewd/2.4l/2_4l.db/beans/part-00000-6990a644-8ee1-44e3-be5e-37809b940b1a-c000.snappy.parquet"],["dbfs:/user/chiraggoel@kpmg.com/dbacademy/dewd/2.4l/2_4l.db/beans/part-00000-94640dc4-739d-4a13-b914-698ceffcff8d-c000.snappy.parquet"],["dbfs:/user/chiraggoel@kpmg.com/dbacademy/dewd/2.4l/2_4l.db/beans/part-00001-2a93677e-d227-4b96-91b1-c0f411fa3423-c000.snappy.parquet"],["dbfs:/user/chiraggoel@kpmg.com/dbacademy/dewd/2.4l/2_4l.db/beans/part-00001-84963c11-0f1a-414f-8e37-96f7f1bbb897-c000.snappy.parquet"],["dbfs:/user/chiraggoel@kpmg.com/dbacademy/dewd/2.4l/2_4l.db/beans/part-00002-765a00f0-0cc7-4aff-b14b-854e72fe8081-c000.snappy.parquet"],["dbfs:/user/chiraggoel@kpmg.com/dbacademy/dewd/2.4l/2_4l.db/beans/part-00002-fdaf5531-cd09-4c55-80b0-574fbc2552bc-c000.snappy.parquet"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th></tr></thead><tbody><tr><td>dbfs:/user/chiraggoel@kpmg.com/dbacademy/dewd/2.4l/2_4l.db/beans/part-00000-02553a1a-d8b3-4e0a-9daf-06ecf4b103b4-c000.snappy.parquet</td></tr><tr><td>dbfs:/user/chiraggoel@kpmg.com/dbacademy/dewd/2.4l/2_4l.db/beans/part-00000-04e0d37a-7cee-488d-bc0e-54cace751fc9-c000.snappy.parquet</td></tr><tr><td>dbfs:/user/chiraggoel@kpmg.com/dbacademy/dewd/2.4l/2_4l.db/beans/part-00000-36a12a65-5987-4c51-a406-79f346abf35a-c000.snappy.parquet</td></tr><tr><td>dbfs:/user/chiraggoel@kpmg.com/dbacademy/dewd/2.4l/2_4l.db/beans/part-00000-6990a644-8ee1-44e3-be5e-37809b940b1a-c000.snappy.parquet</td></tr><tr><td>dbfs:/user/chiraggoel@kpmg.com/dbacademy/dewd/2.4l/2_4l.db/beans/part-00000-94640dc4-739d-4a13-b914-698ceffcff8d-c000.snappy.parquet</td></tr><tr><td>dbfs:/user/chiraggoel@kpmg.com/dbacademy/dewd/2.4l/2_4l.db/beans/part-00001-2a93677e-d227-4b96-91b1-c0f411fa3423-c000.snappy.parquet</td></tr><tr><td>dbfs:/user/chiraggoel@kpmg.com/dbacademy/dewd/2.4l/2_4l.db/beans/part-00001-84963c11-0f1a-414f-8e37-96f7f1bbb897-c000.snappy.parquet</td></tr><tr><td>dbfs:/user/chiraggoel@kpmg.com/dbacademy/dewd/2.4l/2_4l.db/beans/part-00002-765a00f0-0cc7-4aff-b14b-854e72fe8081-c000.snappy.parquet</td></tr><tr><td>dbfs:/user/chiraggoel@kpmg.com/dbacademy/dewd/2.4l/2_4l.db/beans/part-00002-fdaf5531-cd09-4c55-80b0-574fbc2552bc-c000.snappy.parquet</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["All data files not in the current version of the table will be shown in the preview above.\n\nRun the command again without **`DRY RUN`** to permanently delete these files.\n\n**NOTE**: All previous versions of the table will no longer be accessible."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"683965e4-0bbd-446b-9e5f-246b6e77326a"}}},{"cell_type":"code","source":["%sql\nVACUUM beans RETAIN 0 HOURS"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e371a23-c530-45d0-bbe3-fcbe1a217e03"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/user/chiraggoel@kpmg.com/dbacademy/dewd/2.4l/2_4l.db/beans"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th></tr></thead><tbody><tr><td>dbfs:/user/chiraggoel@kpmg.com/dbacademy/dewd/2.4l/2_4l.db/beans</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Because **`VACUUM`** can be such a destructive act for important datasets, it's always a good idea to turn the retention duration check back on. Run the cell below to reactive this setting."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"65c19d2a-78c8-4ea7-bf50-8a49eae3dca7"}}},{"cell_type":"code","source":["%sql\nSET spark.databricks.delta.retentionDurationCheck.enabled = true"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b7b85ff1-2510-41b4-9ba2-550869e85654"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["spark.databricks.delta.retentionDurationCheck.enabled","true"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"key","type":"\"string\"","metadata":"{}"},{"name":"value","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>key</th><th>value</th></tr></thead><tbody><tr><td>spark.databricks.delta.retentionDurationCheck.enabled</td><td>true</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Note that the table history will indicate the user that completed the **`VACUUM`** operation, the number of files deleted, and log that the retention check was disabled during this operation."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ef732fbe-7c81-4d15-955d-fecb413cacce"}}},{"cell_type":"code","source":["%sql\nDESCRIBE HISTORY beans"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92085e4f-db84-4878-9d54-c56a36dde3c9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[10,"2022-05-22T11:42:37.000+0000","6138111496777303","chiraggoel@kpmg.com","VACUUM END",{"status":"COMPLETED"},null,["2841292000074433"],"0522-095621-z1upwij9",9,"SnapshotIsolation",true,{"numDeletedFiles":"9","numVacuumedDirectories":"1"},null,"Databricks-Runtime/10.4.x-scala2.12"],[9,"2022-05-22T11:42:24.000+0000","6138111496777303","chiraggoel@kpmg.com","VACUUM START",{"retentionCheckEnabled":"false","specifiedRetentionMillis":"0","defaultRetentionMillis":"604800000"},null,["2841292000074433"],"0522-095621-z1upwij9",8,"SnapshotIsolation",true,{"numFilesToDelete":"9"},null,"Databricks-Runtime/10.4.x-scala2.12"],[8,"2022-05-22T11:41:26.000+0000","6138111496777303","chiraggoel@kpmg.com","OPTIMIZE",{"predicate":"[]","zOrderBy":"[\"name\"]","batchId":"0","auto":"false"},null,["2841292000074433"],"0522-095621-z1upwij9",7,"SnapshotIsolation",false,{"numRemovedFiles":"5","numRemovedBytes":"6448","p25FileSize":"1381","minFileSize":"1381","numAddedFiles":"1","maxFileSize":"1381","p75FileSize":"1381","p50FileSize":"1381","numAddedBytes":"1381"},null,"Databricks-Runtime/10.4.x-scala2.12"],[7,"2022-05-22T11:40:47.000+0000","6138111496777303","chiraggoel@kpmg.com","RESTORE",{"version":"5","timestamp":null},null,["2841292000074433"],"0522-095621-z1upwij9",6,"Serializable",false,{"numRestoredFiles":"1","removedFilesSize":"1304","numRemovedFiles":"1","restoredFilesSize":"1284","numOfFilesAfterRestore":"5","tableSizeAfterRestore":"6448"},null,"Databricks-Runtime/10.4.x-scala2.12"],[6,"2022-05-22T11:39:09.000+0000","6138111496777303","chiraggoel@kpmg.com","MERGE",{"predicate":"((a.name = b.name) AND (a.color = b.color))","matchedPredicates":"[{\"actionType\":\"update\"}]","notMatchedPredicates":"[{\"predicate\":\"b.delicious\",\"actionType\":\"insert\"}]"},null,["2841292000074433"],"0522-095621-z1upwij9",5,"WriteSerializable",false,{"numTargetRowsCopied":"0","numTargetRowsDeleted":"0","numTargetFilesAdded":"1","executionTimeMs":"4079","numTargetRowsInserted":"2","scanTimeMs":"1846","numTargetRowsUpdated":"1","numOutputRows":"3","numTargetChangeFilesAdded":"0","numSourceRows":"4","numTargetFilesRemoved":"1","rewriteTimeMs":"2138"},null,"Databricks-Runtime/10.4.x-scala2.12"],[5,"2022-05-22T11:39:02.000+0000","6138111496777303","chiraggoel@kpmg.com","DELETE",{"predicate":"[\"(NOT spark_catalog.dbacademy_chiraggoel_kpmg_com_dewd_2_4l.beans.delicious)\"]"},null,["2841292000074433"],"0522-095621-z1upwij9",4,"WriteSerializable",false,{"numRemovedFiles":"1","numCopiedRows":"0","numAddedChangeFiles":"0","executionTimeMs":"1722","numDeletedRows":"1","scanTimeMs":"1141","numAddedFiles":"0","rewriteTimeMs":"581"},null,"Databricks-Runtime/10.4.x-scala2.12"],[4,"2022-05-22T11:38:58.000+0000","6138111496777303","chiraggoel@kpmg.com","UPDATE",{"predicate":"(name#14611 = pinto)"},null,["2841292000074433"],"0522-095621-z1upwij9",3,"WriteSerializable",false,{"numRemovedFiles":"1","numCopiedRows":"0","numAddedChangeFiles":"0","executionTimeMs":"1927","scanTimeMs":"139","numAddedFiles":"1","numUpdatedRows":"1","rewriteTimeMs":"1787"},null,"Databricks-Runtime/10.4.x-scala2.12"],[3,"2022-05-22T11:38:53.000+0000","6138111496777303","chiraggoel@kpmg.com","UPDATE",{"predicate":"(name#14060 = jelly)"},null,["2841292000074433"],"0522-095621-z1upwij9",2,"WriteSerializable",false,{"numRemovedFiles":"1","numCopiedRows":"0","numAddedChangeFiles":"0","executionTimeMs":"2193","scanTimeMs":"192","numAddedFiles":"1","numUpdatedRows":"1","rewriteTimeMs":"2001"},null,"Databricks-Runtime/10.4.x-scala2.12"],[2,"2022-05-22T11:38:48.000+0000","6138111496777303","chiraggoel@kpmg.com","WRITE",{"mode":"Append","partitionBy":"[]"},null,["2841292000074433"],"0522-095621-z1upwij9",1,"WriteSerializable",true,{"numFiles":"3","numOutputRows":"3","numOutputBytes":"3908"},null,"Databricks-Runtime/10.4.x-scala2.12"],[1,"2022-05-22T11:38:44.000+0000","6138111496777303","chiraggoel@kpmg.com","WRITE",{"mode":"Append","partitionBy":"[]"},null,["2841292000074433"],"0522-095621-z1upwij9",0,"WriteSerializable",true,{"numFiles":"3","numOutputRows":"3","numOutputBytes":"3880"},null,"Databricks-Runtime/10.4.x-scala2.12"],[0,"2022-05-22T11:38:38.000+0000","6138111496777303","chiraggoel@kpmg.com","CREATE TABLE",{"isManaged":"true","description":null,"partitionBy":"[]","properties":"{}"},null,["2841292000074433"],"0522-095621-z1upwij9",null,"WriteSerializable",true,{},null,"Databricks-Runtime/10.4.x-scala2.12"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"version","type":"\"long\"","metadata":"{}"},{"name":"timestamp","type":"\"timestamp\"","metadata":"{}"},{"name":"userId","type":"\"string\"","metadata":"{}"},{"name":"userName","type":"\"string\"","metadata":"{}"},{"name":"operation","type":"\"string\"","metadata":"{}"},{"name":"operationParameters","type":"{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}","metadata":"{}"},{"name":"job","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"jobId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"runId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobOwnerId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"triggerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}","metadata":"{}"},{"name":"notebook","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"notebookId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}","metadata":"{}"},{"name":"clusterId","type":"\"string\"","metadata":"{}"},{"name":"readVersion","type":"\"long\"","metadata":"{}"},{"name":"isolationLevel","type":"\"string\"","metadata":"{}"},{"name":"isBlindAppend","type":"\"boolean\"","metadata":"{}"},{"name":"operationMetrics","type":"{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}","metadata":"{}"},{"name":"userMetadata","type":"\"string\"","metadata":"{}"},{"name":"engineInfo","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr></thead><tbody><tr><td>10</td><td>2022-05-22T11:42:37.000+0000</td><td>6138111496777303</td><td>chiraggoel@kpmg.com</td><td>VACUUM END</td><td>Map(status -> COMPLETED)</td><td>null</td><td>List(2841292000074433)</td><td>0522-095621-z1upwij9</td><td>9</td><td>SnapshotIsolation</td><td>true</td><td>Map(numDeletedFiles -> 9, numVacuumedDirectories -> 1)</td><td>null</td><td>Databricks-Runtime/10.4.x-scala2.12</td></tr><tr><td>9</td><td>2022-05-22T11:42:24.000+0000</td><td>6138111496777303</td><td>chiraggoel@kpmg.com</td><td>VACUUM START</td><td>Map(retentionCheckEnabled -> false, specifiedRetentionMillis -> 0, defaultRetentionMillis -> 604800000)</td><td>null</td><td>List(2841292000074433)</td><td>0522-095621-z1upwij9</td><td>8</td><td>SnapshotIsolation</td><td>true</td><td>Map(numFilesToDelete -> 9)</td><td>null</td><td>Databricks-Runtime/10.4.x-scala2.12</td></tr><tr><td>8</td><td>2022-05-22T11:41:26.000+0000</td><td>6138111496777303</td><td>chiraggoel@kpmg.com</td><td>OPTIMIZE</td><td>Map(predicate -> [], zOrderBy -> [\"name\"], batchId -> 0, auto -> false)</td><td>null</td><td>List(2841292000074433)</td><td>0522-095621-z1upwij9</td><td>7</td><td>SnapshotIsolation</td><td>false</td><td>Map(numRemovedFiles -> 5, numRemovedBytes -> 6448, p25FileSize -> 1381, minFileSize -> 1381, numAddedFiles -> 1, maxFileSize -> 1381, p75FileSize -> 1381, p50FileSize -> 1381, numAddedBytes -> 1381)</td><td>null</td><td>Databricks-Runtime/10.4.x-scala2.12</td></tr><tr><td>7</td><td>2022-05-22T11:40:47.000+0000</td><td>6138111496777303</td><td>chiraggoel@kpmg.com</td><td>RESTORE</td><td>Map(version -> 5, timestamp -> null)</td><td>null</td><td>List(2841292000074433)</td><td>0522-095621-z1upwij9</td><td>6</td><td>Serializable</td><td>false</td><td>Map(numRestoredFiles -> 1, removedFilesSize -> 1304, numRemovedFiles -> 1, restoredFilesSize -> 1284, numOfFilesAfterRestore -> 5, tableSizeAfterRestore -> 6448)</td><td>null</td><td>Databricks-Runtime/10.4.x-scala2.12</td></tr><tr><td>6</td><td>2022-05-22T11:39:09.000+0000</td><td>6138111496777303</td><td>chiraggoel@kpmg.com</td><td>MERGE</td><td>Map(predicate -> ((a.name = b.name) AND (a.color = b.color)), matchedPredicates -> [{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"predicate\":\"b.delicious\",\"actionType\":\"insert\"}])</td><td>null</td><td>List(2841292000074433)</td><td>0522-095621-z1upwij9</td><td>5</td><td>WriteSerializable</td><td>false</td><td>Map(numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, executionTimeMs -> 4079, numTargetRowsInserted -> 2, scanTimeMs -> 1846, numTargetRowsUpdated -> 1, numOutputRows -> 3, numTargetChangeFilesAdded -> 0, numSourceRows -> 4, numTargetFilesRemoved -> 1, rewriteTimeMs -> 2138)</td><td>null</td><td>Databricks-Runtime/10.4.x-scala2.12</td></tr><tr><td>5</td><td>2022-05-22T11:39:02.000+0000</td><td>6138111496777303</td><td>chiraggoel@kpmg.com</td><td>DELETE</td><td>Map(predicate -> [\"(NOT spark_catalog.dbacademy_chiraggoel_kpmg_com_dewd_2_4l.beans.delicious)\"])</td><td>null</td><td>List(2841292000074433)</td><td>0522-095621-z1upwij9</td><td>4</td><td>WriteSerializable</td><td>false</td><td>Map(numRemovedFiles -> 1, numCopiedRows -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 1722, numDeletedRows -> 1, scanTimeMs -> 1141, numAddedFiles -> 0, rewriteTimeMs -> 581)</td><td>null</td><td>Databricks-Runtime/10.4.x-scala2.12</td></tr><tr><td>4</td><td>2022-05-22T11:38:58.000+0000</td><td>6138111496777303</td><td>chiraggoel@kpmg.com</td><td>UPDATE</td><td>Map(predicate -> (name#14611 = pinto))</td><td>null</td><td>List(2841292000074433)</td><td>0522-095621-z1upwij9</td><td>3</td><td>WriteSerializable</td><td>false</td><td>Map(numRemovedFiles -> 1, numCopiedRows -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 1927, scanTimeMs -> 139, numAddedFiles -> 1, numUpdatedRows -> 1, rewriteTimeMs -> 1787)</td><td>null</td><td>Databricks-Runtime/10.4.x-scala2.12</td></tr><tr><td>3</td><td>2022-05-22T11:38:53.000+0000</td><td>6138111496777303</td><td>chiraggoel@kpmg.com</td><td>UPDATE</td><td>Map(predicate -> (name#14060 = jelly))</td><td>null</td><td>List(2841292000074433)</td><td>0522-095621-z1upwij9</td><td>2</td><td>WriteSerializable</td><td>false</td><td>Map(numRemovedFiles -> 1, numCopiedRows -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 2193, scanTimeMs -> 192, numAddedFiles -> 1, numUpdatedRows -> 1, rewriteTimeMs -> 2001)</td><td>null</td><td>Databricks-Runtime/10.4.x-scala2.12</td></tr><tr><td>2</td><td>2022-05-22T11:38:48.000+0000</td><td>6138111496777303</td><td>chiraggoel@kpmg.com</td><td>WRITE</td><td>Map(mode -> Append, partitionBy -> [])</td><td>null</td><td>List(2841292000074433)</td><td>0522-095621-z1upwij9</td><td>1</td><td>WriteSerializable</td><td>true</td><td>Map(numFiles -> 3, numOutputRows -> 3, numOutputBytes -> 3908)</td><td>null</td><td>Databricks-Runtime/10.4.x-scala2.12</td></tr><tr><td>1</td><td>2022-05-22T11:38:44.000+0000</td><td>6138111496777303</td><td>chiraggoel@kpmg.com</td><td>WRITE</td><td>Map(mode -> Append, partitionBy -> [])</td><td>null</td><td>List(2841292000074433)</td><td>0522-095621-z1upwij9</td><td>0</td><td>WriteSerializable</td><td>true</td><td>Map(numFiles -> 3, numOutputRows -> 3, numOutputBytes -> 3880)</td><td>null</td><td>Databricks-Runtime/10.4.x-scala2.12</td></tr><tr><td>0</td><td>2022-05-22T11:38:38.000+0000</td><td>6138111496777303</td><td>chiraggoel@kpmg.com</td><td>CREATE TABLE</td><td>Map(isManaged -> true, description -> null, partitionBy -> [], properties -> {})</td><td>null</td><td>List(2841292000074433)</td><td>0522-095621-z1upwij9</td><td>null</td><td>WriteSerializable</td><td>true</td><td>Map()</td><td>null</td><td>Databricks-Runtime/10.4.x-scala2.12</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Query your table again to confirm you still have access to the current version."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d91b9cbc-a35b-4c98-ba7f-aa2444021dcc"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM beans"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fa857d74-aa02-43cb-b3d6-674349a037af"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["jelly","rainbow",42.5,true],["lentils","brown",1000.0,true],["black","black",500.0,true],["pinto","brown",1500.0,true],["green","green",178.3,true]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"color","type":"\"string\"","metadata":"{}"},{"name":"grams","type":"\"float\"","metadata":"{}"},{"name":"delicious","type":"\"boolean\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>color</th><th>grams</th><th>delicious</th></tr></thead><tbody><tr><td>jelly</td><td>rainbow</td><td>42.5</td><td>true</td></tr><tr><td>lentils</td><td>brown</td><td>1000.0</td><td>true</td></tr><tr><td>black</td><td>black</td><td>500.0</td><td>true</td></tr><tr><td>pinto</td><td>brown</td><td>1500.0</td><td>true</td></tr><tr><td>green</td><td>green</td><td>178.3</td><td>true</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["<img src=\"https://files.training.databricks.com/images/icon_warn_32.png\"> Because Delta Cache stores copies of files queried in the current session on storage volumes deployed to your currently active cluster, you may still be able to temporarily access previous table versions (though systems should **not** be designed to expect this behavior). \n\nRestarting the cluster will ensure that these cached data files are permanently purged.\n\nYou can see an example of this by uncommenting and running the following cell that may, or may not, fail\n(depending on the state of the cache)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c5728019-068a-436a-a07e-c17262805788"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM beans@v1"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a8f3df1c-325b-4d7a-a414-76cc12e3a2c7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 609.0 failed 1 times, most recent failure: Lost task 0.0 in stage 609.0 (TID 4289) (ip-10-172-177-11.us-west-2.compute.internal executor driver): com.databricks.sql.io.FileReadException: Error while reading file dbfs:REDACTED_LOCAL_PART@kpmg.com/dbacademy/dewd/2.4l/2_4l.db/beans/part-00001-2a93677e-d227-4b96-91b1-c0f411fa3423-c000.snappy.parquet. A file referenced in the transaction log cannot be found. This occurs when data has been manually deleted from the file system rather than using the table `DELETE` statement. For more information, see https://docs.databricks.com/delta/delta-intro.html#frequently-asked-questions\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:521)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:473)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:614)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:356)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:351)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:1017)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:155)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:826)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1670)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:829)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:684)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.FileNotFoundException: /user/chiraggoel@kpmg.com/dbacademy/dewd/2.4l/2_4l.db/beans/part-00001-2a93677e-d227-4b96-91b1-c0f411fa3423-c000.snappy.parquet\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:120)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:68)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.getFileStatus(DatabricksFileSystemV1.scala:272)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:215)\n\tat com.databricks.spark.metrics.FileSystemWithMetrics.getFileStatus(FileSystemWithMetrics.scala:375)\n\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.readFooter(CachingParquetFooterReader.java:203)\n\tat com.databricks.sql.io.parquet.CachingParquetFileReader.readFooter(CachingParquetFileReader.java:350)\n\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.prepare(SpecificParquetRecordReaderBase.java:123)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$2.apply(ParquetFileFormat.scala:535)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$2.apply(ParquetFileFormat.scala:420)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:457)\n\t... 31 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2931)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2925)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2925)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1345)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1345)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1345)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3193)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3134)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1107)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2628)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:266)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:276)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:81)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:87)\n\tat org.apache.spark.sql.execution.collect.InternalRowFormat$.collect(cachedSparkResults.scala:75)\n\tat org.apache.spark.sql.execution.collect.InternalRowFormat$.collect(cachedSparkResults.scala:62)\n\tat org.apache.spark.sql.execution.ResultCacheManager.collectResult$1(ResultCacheManager.scala:587)\n\tat org.apache.spark.sql.execution.ResultCacheManager.computeResult(ResultCacheManager.scala:596)\n\tat org.apache.spark.sql.execution.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:542)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:541)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:438)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:417)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:422)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3132)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectResult$1(Dataset.scala:3123)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3930)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:213)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:360)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:160)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:958)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:115)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:310)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3928)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3122)\n\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation0(OutputAggregator.scala:268)\n\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation(OutputAggregator.scala:102)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:115)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$11(DriverLocal.scala:605)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:28)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:26)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:60)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:60)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:582)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:526)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:561)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:431)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:374)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:225)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.sql.io.FileReadException: Error while reading file dbfs:REDACTED_LOCAL_PART@kpmg.com/dbacademy/dewd/2.4l/2_4l.db/beans/part-00001-2a93677e-d227-4b96-91b1-c0f411fa3423-c000.snappy.parquet. A file referenced in the transaction log cannot be found. This occurs when data has been manually deleted from the file system rather than using the table `DELETE` statement. For more information, see https://docs.databricks.com/delta/delta-intro.html#frequently-asked-questions\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:521)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:473)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:614)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:356)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:351)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:1017)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:155)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:826)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1670)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:829)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:684)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.io.FileNotFoundException: /user/chiraggoel@kpmg.com/dbacademy/dewd/2.4l/2_4l.db/beans/part-00001-2a93677e-d227-4b96-91b1-c0f411fa3423-c000.snappy.parquet\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:120)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:68)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.getFileStatus(DatabricksFileSystemV1.scala:272)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:215)\n\tat com.databricks.spark.metrics.FileSystemWithMetrics.getFileStatus(FileSystemWithMetrics.scala:375)\n\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.readFooter(CachingParquetFooterReader.java:203)\n\tat com.databricks.sql.io.parquet.CachingParquetFileReader.readFooter(CachingParquetFileReader.java:350)\n\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.prepare(SpecificParquetRecordReaderBase.java:123)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$2.apply(ParquetFileFormat.scala:535)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$2.apply(ParquetFileFormat.scala:420)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:457)\n\t... 31 more\n","errorSummary":"FileReadException: Error while reading file dbfs:REDACTED_LOCAL_PART@kpmg.com/dbacademy/dewd/2.4l/2_4l.db/beans/part-00001-2a93677e-d227-4b96-91b1-c0f411fa3423-c000.snappy.parquet. A file referenced in the transaction log cannot be found. This occurs when data has been manually deleted from the file system rather than using the table `DELETE` statement. For more information, see https://docs.databricks.com/delta/delta-intro.html#frequently-asked-questions\nCaused by: FileNotFoundException: /user/chiraggoel@kpmg.com/dbacademy/dewd/2.4l/2_4l.db/beans/part-00001-2a93677e-d227-4b96-91b1-c0f411fa3423-c000.snappy.parquet","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 609.0 failed 1 times, most recent failure: Lost task 0.0 in stage 609.0 (TID 4289) (ip-10-172-177-11.us-west-2.compute.internal executor driver): com.databricks.sql.io.FileReadException: Error while reading file dbfs:REDACTED_LOCAL_PART@kpmg.com/dbacademy/dewd/2.4l/2_4l.db/beans/part-00001-2a93677e-d227-4b96-91b1-c0f411fa3423-c000.snappy.parquet. A file referenced in the transaction log cannot be found. This occurs when data has been manually deleted from the file system rather than using the table `DELETE` statement. For more information, see https://docs.databricks.com/delta/delta-intro.html#frequently-asked-questions\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:521)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:473)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:614)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:356)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:351)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:1017)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:155)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:826)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1670)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:829)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:684)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.FileNotFoundException: /user/chiraggoel@kpmg.com/dbacademy/dewd/2.4l/2_4l.db/beans/part-00001-2a93677e-d227-4b96-91b1-c0f411fa3423-c000.snappy.parquet\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:120)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:68)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.getFileStatus(DatabricksFileSystemV1.scala:272)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:215)\n\tat com.databricks.spark.metrics.FileSystemWithMetrics.getFileStatus(FileSystemWithMetrics.scala:375)\n\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.readFooter(CachingParquetFooterReader.java:203)\n\tat com.databricks.sql.io.parquet.CachingParquetFileReader.readFooter(CachingParquetFileReader.java:350)\n\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.prepare(SpecificParquetRecordReaderBase.java:123)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$2.apply(ParquetFileFormat.scala:535)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$2.apply(ParquetFileFormat.scala:420)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:457)\n\t... 31 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2931)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2925)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2925)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1345)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1345)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1345)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3193)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3134)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1107)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2628)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:266)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:276)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:81)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:87)\n\tat org.apache.spark.sql.execution.collect.InternalRowFormat$.collect(cachedSparkResults.scala:75)\n\tat org.apache.spark.sql.execution.collect.InternalRowFormat$.collect(cachedSparkResults.scala:62)\n\tat org.apache.spark.sql.execution.ResultCacheManager.collectResult$1(ResultCacheManager.scala:587)\n\tat org.apache.spark.sql.execution.ResultCacheManager.computeResult(ResultCacheManager.scala:596)\n\tat org.apache.spark.sql.execution.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:542)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:541)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:438)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:417)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:422)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3132)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectResult$1(Dataset.scala:3123)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3930)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:213)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:360)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:160)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:958)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:115)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:310)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3928)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3122)\n\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation0(OutputAggregator.scala:268)\n\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation(OutputAggregator.scala:102)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:115)\n\tat com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:145)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$11(DriverLocal.scala:605)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:28)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:26)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:60)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:60)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:582)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:615)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:607)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:526)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:561)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:431)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:374)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:225)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.sql.io.FileReadException: Error while reading file dbfs:REDACTED_LOCAL_PART@kpmg.com/dbacademy/dewd/2.4l/2_4l.db/beans/part-00001-2a93677e-d227-4b96-91b1-c0f411fa3423-c000.snappy.parquet. A file referenced in the transaction log cannot be found. This occurs when data has been manually deleted from the file system rather than using the table `DELETE` statement. For more information, see https://docs.databricks.com/delta/delta-intro.html#frequently-asked-questions\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:521)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:473)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:614)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:356)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:351)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:1017)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:155)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:156)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:826)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1670)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:829)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:684)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.io.FileNotFoundException: /user/chiraggoel@kpmg.com/dbacademy/dewd/2.4l/2_4l.db/beans/part-00001-2a93677e-d227-4b96-91b1-c0f411fa3423-c000.snappy.parquet\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:120)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:68)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.getFileStatus(DatabricksFileSystemV1.scala:272)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:215)\n\tat com.databricks.spark.metrics.FileSystemWithMetrics.getFileStatus(FileSystemWithMetrics.scala:375)\n\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.readFooter(CachingParquetFooterReader.java:203)\n\tat com.databricks.sql.io.parquet.CachingParquetFileReader.readFooter(CachingParquetFileReader.java:350)\n\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.prepare(SpecificParquetRecordReaderBase.java:123)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$2.apply(ParquetFileFormat.scala:535)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$2.apply(ParquetFileFormat.scala:420)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:457)\n\t... 31 more"]}}],"execution_count":0},{"cell_type":"markdown","source":["By completing this lab, you should now feel comfortable:\n* Completing standard Delta Lake table creation and data manipulation commands\n* Reviewing table metadata including table history\n* Leverage Delta Lake versioning for snapshot queries and rollbacks\n* Compacting small files and indexing tables\n* Using **`VACUUM`** to review files marked for deletion and committing these deletes"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fbc660e3-9b45-45a9-a09c-d97a4ffc0887"}}},{"cell_type":"markdown","source":["Run the following cell to delete the tables and files associated with this lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bce2686d-a01f-4a71-aa85-36f862cabef2"}}},{"cell_type":"code","source":["%python\nDA.cleanup()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a20f38c4-6c72-449a-9f51-afcd6101c2e4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Dropping the database \"dbacademy_chiraggoel_kpmg_com_dewd_2_4l\"\nRemoving the working directory \"dbfs:/user/chiraggoel@kpmg.com/dbacademy/dewd/2.4l\"\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Dropping the database \"dbacademy_chiraggoel_kpmg_com_dewd_2_4l\"\nRemoving the working directory \"dbfs:/user/chiraggoel@kpmg.com/dbacademy/dewd/2.4l\"\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce3279fd-8f23-49d2-954e-b664b4e970c3"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"DE 2.4L - Delta Lake Versioning, Optimization, and Vacuuming Lab","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2841292000074433}},"nbformat":4,"nbformat_minor":0}
