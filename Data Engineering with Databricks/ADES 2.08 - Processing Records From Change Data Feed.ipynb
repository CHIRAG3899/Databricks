{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"64df14fe-5f96-409f-bbfd-5831e964a722"}}},{"cell_type":"markdown","source":["# Processing Records from Delta Change Data Feed\n\nIn this notebook, we'll demonstrate an end-to-end of how you can easily propagate changes through a Lakehouse with Delta Lake Change Data Feed (CDF).\n\nFor this demo, we'll work with a slightly different dataset representing patient information for medical records. Descriptions of the data at various stages follow.\n\n### Bronze Table\nHere we store all records as consumed. A row represents:\n1. A new patient providing data for the first time\n1. An existing patient confirming that their information is still correct\n1. An existing patient updating some of their information\n\nThe type of action a row represents is not captured.\n\n### Silver Table\nThis is the validated view of our data. Each patient will appear only once in this table. An upsert statement will be used to identify rows that have changed.\n\n### Gold Table\nFor this example, we'll create a simple gold table that captures patients that have a new address.\n\n## Learning Objectives\nBy the end of this lesson, students will be able to:\n- Enable Change Data Feed on a cluster or for a particular table\n- Describe how changes are recorded\n- Read CDF output with Spark SQL or PySpark\n- Refactor ELT code to process CDF output"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"df065388-ed54-443d-9784-cc40de872438"}}},{"cell_type":"markdown","source":["### Setup\n\nThe following code defines some paths, a demo database, and clears out previous runs of the demo.\n\nIt also defines a variable `Raw` that we'll use to land raw data in our source directory, allowing us to process new records as if they were arriving in production."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"38624bce-b23f-4e92-ba64-44fb30194bd8"}}},{"cell_type":"code","source":["%run ../Includes/cdc-setup $mode=\"reset\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"52f290db-909e-42f2-84de-5317561713a6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Enable CDF using Spark conf setting in a notebook or on a cluster will ensure it's used on all newly created Delta tables in that scope."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4656df57-3795-4b76-a1c6-8553463292a6"}}},{"cell_type":"code","source":["spark.conf.set('spark.databricks.delta.properties.defaults.enableChangeDataFeed',True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"98b1904e-8f6d-4c1c-aabe-6447a9ea3243"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Confirm source directory is empty."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3d28c149-32ac-44e6-b3df-c002830953ab"}}},{"cell_type":"code","source":["try:\n    dbutils.fs.ls(Raw.userdir)\nexcept Exception as e:\n    print(e)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be4e4ce0-ec66-4a5e-a77d-048820975199"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Ingest Data with Auto Loader\n\nHere we'll use Auto Loader to ingest data as it arrives.\n\nThe logic below is set up to either use trigger once to process all records loaded so far, or to continuously process records as they arrive.\n\nWe'll turn on continuous processing."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e5d2750-0a0d-43f5-a298-c17653588ede"}}},{"cell_type":"code","source":["schema = \"mrn BIGINT, dob DATE, sex STRING, gender STRING, first_name STRING, last_name STRING, street_address STRING, zip BIGINT, city STRING, state STRING, updated timestamp\"\n\nbronzePath = f\"{userhome}/bronze\"\n\nspark.sql(f\"\"\"\n    CREATE TABLE IF NOT EXISTS bronze\n    (mrn BIGINT, dob DATE, sex STRING, gender STRING, first_name STRING, last_name STRING, street_address STRING, zip BIGINT, city STRING, state STRING, updated timestamp) \n    LOCATION '{bronzePath}'\n\"\"\")\n\n(spark.readStream\n    .format(\"cloudFiles\")\n    .option(\"cloudFiles.format\", \"json\")\n    .schema(schema)\n    .load(Raw.userdir)\n    .writeStream\n    .format(\"delta\")\n    .outputMode(\"append\")\n#     .trigger(once=True)\n    .trigger(processingTime='5 seconds')\n    .option(\"checkpointLocation\", userhome + \"/_bronze_checkpoint\")\n    .table(\"bronze\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8c2a1bc9-87e5-4f25-9a13-371fdc7620ca"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Expand the stream monitor above to see the progress of your stream. No files should have been ingestedl.\n\nUse the cell below to land a batch of data and list files in the source; you should see these records processed as a batch."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fbc2acaa-a8c0-4d96-a895-db2bea64e507"}}},{"cell_type":"code","source":["Raw.arrival()\ndbutils.fs.ls(Raw.userdir)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a202dd2e-e9f1-4224-9a19-c34292da6359"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Create a Target Table\n\nHere we use `DEEP CLONE` to move read-only data from PROD to our DEV environment (where we have full write/delete access)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c440a88-df10-456e-9e9c-bab05b8186d1"}}},{"cell_type":"code","source":["silverPath = userhome + \"/silver\"\n\nspark.sql(f\"\"\"\n    CREATE TABLE silver\n    DEEP CLONE delta.`{URI}/pii/silver`\n    LOCATION '{silverPath}'\n\"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c56ea6f-f194-4089-acde-e8cb8badac6a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Tables that were not created with CDF enabled will not have it turned on by default, but can be altered to capture changes with the following syntax.\n\nNote that editing properties will version a table. Also note that no CDC is captured during the CLONE operation above."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b17c1150-0694-4152-9dae-121cf8954875"}}},{"cell_type":"code","source":["%sql\nALTER TABLE silver SET TBLPROPERTIES (delta.enableChangeDataFeed = true);"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9545c37d-12b3-4d09-b1c0-6ee7af351930"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Upsert Data with Delta Lake\n\nHere we define upsert logic into the silver table using a streaming read against the bronze table, matching on our unique identifier `mrn`.\n\nWe specify an additional conditional check to ensure that a field in the data has changed before inserting the new record."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"51ca006c-8798-4a45-a262-f837464204bb"}}},{"cell_type":"code","source":["def upsertToDelta(microBatchDF, batchId):\n    microBatchDF.createOrReplaceTempView(\"updates\")\n    microBatchDF._jdf.sparkSession().sql(\"\"\"\n        MERGE INTO silver s\n        USING updates u\n        ON s.mrn = u.mrn\n        WHEN MATCHED AND s.dob <> u.dob OR\n                         s.sex <> u.sex OR\n                         s.gender <> u.gender OR\n                         s.first_name <> u.first_name OR\n                         s.last_name <> u.last_name OR\n                         s.street_address <> u.street_address OR\n                         s.zip <> u.zip OR\n                         s.city <> u.city OR\n                         s.state <> u.state OR\n                         s.updated <> u.updated\n            THEN UPDATE SET *\n        WHEN NOT MATCHED\n            THEN INSERT *\n    \"\"\")\n    \n(spark.readStream\n    .table(\"bronze\")\n    .writeStream\n    .foreachBatch(upsertToDelta)\n    .outputMode(\"update\")\n#     .trigger(once=True)\n    .trigger(processingTime='5 seconds')\n    .start())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"727165d4-5a57-42ba-9f16-971a06a12887"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note that we have an additional metadata directory nested in our table directory, `_change_data`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"af3c925e-6411-46fa-8d91-079c2db8e31e"}}},{"cell_type":"code","source":["dbutils.fs.ls(silverPath)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"51c4e630-457f-4474-a183-6eb40ad7ba0e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We can see this directory also contains parquet files."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"34d705e4-a65c-4b6d-9817-bd75e828dbd8"}}},{"cell_type":"code","source":["dbutils.fs.ls(silverPath + \"/_change_data\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b8fa7276-7ba1-425f-ac16-d78ca9d5236e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Read the Change Data Feed\n\nTo pick up the recorded CDC data, we add two options:\n- `readChangeData`\n- `startingVersion` (can use `startingTimestamp` instead)\n\nHere we'll do a streaming display of just those patients in LA. Note that users with changes have two records present."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"baaaf948-3206-4d46-802d-c50e568395a5"}}},{"cell_type":"code","source":["cdcDF = spark.readStream.format(\"delta\").option(\"readChangeData\", True).option(\"startingVersion\", 0).table(\"silver\")\ndisplay(cdcDF.filter(\"city = 'Los Angeles'\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bb70100d-696c-434e-a947-ce576e89ac4d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["If we land another file in our source directory and wait a few seconds, we'll see that we now have captured CDC changes for multiple `_commit_version` (change the sort order of the `_commit_version` column in the display above to see this)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"baa8bbef-6d90-41d9-bdf8-0aa024c5a1d5"}}},{"cell_type":"code","source":["Raw.arrival()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5d813bb0-70ff-4cb9-9fa5-84c2253b7caa"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Gold Table\nOur gold table will capture all of those patients that have a new address, and record this information alongside 2 timestamps: the time at which this change was made in our source system (currently labeled `updated`) and the time this was processed into our silver table (captured by the `_commit_timestamp` generated CDC field).\n\nWithin silver table CDC records:\n- check for max `_commit_version` for each record\n- if new version and address change, insert to gold table\n- record `updated_timestamp` and `processed_timestamp`\n\n#### Gold Table Schema\n| field | type |\n| --- | --- |\n| mrn | long |\n| new_street_address | string |\n| new_zip | long |\n| new_city | string |\n| new_state | string |\n| old_street_address | string |\n| old_zip | long |\n| old_city | string |\n| old_state | string |\n| updated_timestamp | timestamp |\n| processed_timestamp | timestamp |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9f4e79b2-63f3-4f49-85f2-ee9e3afaf58d"}}},{"cell_type":"code","source":["goldPath = userhome + \"/gold\"\n\nspark.sql(f\"\"\"\n    CREATE TABLE gold\n        (mrn BIGINT,\n        new_street_address STRING,\n        new_zip BIGINT,\n        new_city STRING,\n        new_state STRING,\n        old_street_address STRING,\n        old_zip BIGINT,\n        old_city STRING,\n        old_state STRING,\n        updated_timestamp TIMESTAMP,\n        processed_timestamp TIMESTAMP)\n    USING DELTA\n    LOCATION '{goldPath}'\n\"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3d4cfbdc-c902-4b16-8dcb-a69775fdfe93"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note that we are using a table that has updates written to it as a streaming source! This is a **huge** value add, and something that historically has required exensively workarounds to process correctly."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be076116-a6f4-4352-9e54-12aead1d869e"}}},{"cell_type":"code","source":["silverStreamDF = spark.readStream.format(\"delta\").option(\"readChangeData\", True).option(\"startingVersion\", 0).table(\"silver\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8e51d260-dae2-4d55-9274-5791f7c44c8e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Our `_change_type` field lets us easily distinguish valid and invalid records.\n\nNew valid rows will have the `update_postimage` or `insert` label.\nNew invalid rows will have the `update_preimage` or `delete` label. \n\n(**NOTE**: We'll demonstrate logic for propagating deletes a little later)\n\nIn the cell below, we'll define two queries against our streaming source to perform a stream-stream merge on our data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1462c112-f3b0-4ee9-8b17-e4386f13d7f6"}}},{"cell_type":"code","source":["newDF = (silverStreamDF.filter(F.col(\"_change_type\").isin([\"update_postimage\", \"insert\"]))\n             .selectExpr(\"mrn\",\n                 \"street_address new_street_address\",\n                 \"zip new_zip\",\n                 \"city new_city\",\n                 \"state new_state\",\n                 \"updated updated_timestamp\",\n                 \"_commit_timestamp processed_timestamp\"))\n\n                                                                                         \noldDF = (silverStreamDF.filter(F.col(\"_change_type\").isin([\"update_preimage\"]))\n             .selectExpr(\"mrn\",\n                 \"street_address old_street_address\",\n                 \"zip old_zip\",\n                 \"city old_city\",\n                 \"state old_state\",\n                 \"_commit_timestamp processed_timestamp\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"18269aea-fb4e-4ece-b23b-10b9149bf95b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Assuming that we have properly deduplicated our data to ensure that only a single record for our `mrn` can be processed to our silver table, `mrn` and `_commit_timestamp` (aliased to `processed_timestamp` here) serve as a unique composite key.\n\nOur join will allow us to match up the current and previous states of our data to track all changes.\n\nThis table could drive further downstream processes, such as triggering confirmation emails or automatic mailings for patients with updated addresses.\n\nOur CDC data arrives as a stream, so only newly changed data at the silver level will be processed. Therefore, we can write to our gold table in append mode and maintain the grain of our data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"49f5af48-c7e8-43c0-9a88-a2c336d65cc5"}}},{"cell_type":"code","source":["(newDF.withWatermark(\"processed_timestamp\", \"3 minutes\")\n    .join(oldDF, [\"mrn\", \"processed_timestamp\"], \"left\")\n    .filter(\"new_street_address <> old_street_address OR old_street_address IS NULL\")\n    .writeStream\n    .outputMode(\"append\")\n#     .trigger(once=True)\n    .trigger(processingTime=\"5 seconds\")\n    .option(\"checkpointLocation\", userhome + \"/_gold_checkpoint\")\n    .table(\"gold\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f19deab6-45f0-451c-89fc-ab014559f68f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note the number of rows in our gold table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a48defb-aea7-4d9e-b41b-4d0c9dc35cc7"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM gold"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"808f413e-3a7b-494b-a5f6-b11bae5ba6f8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["If we land a new raw file and wait a few seconds, we can see that all of our changes have propagated through our pipeline.\n\n(This assumes you're using `processingTime` instead of trigger once processing. Scroll up to the gold table streaming write to wait for a new peak in the processing rate to know your data has arrived.)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fba6f53c-4007-4ddb-98af-d9aa53a098b4"}}},{"cell_type":"code","source":["Raw.arrival()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce7583fe-a5c8-44ef-87ec-5eabeb8ae075"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["You should be able to see a jump in the number of records in your gold table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"df8eadf8-4ccd-49d4-9441-f7849ed0a6d5"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM gold"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f7660934-d89d-471e-aa76-6da885373cea"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Make sure to run the following cell to stop all active streams."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96691e54-fad5-437f-ab29-19dc4cce74d1"}}},{"cell_type":"code","source":["for stream in spark.streams.active:\n    stream.stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fe05ac6d-33fb-4076-aaf6-06967df9bdf9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Propagating Deletes\n\nWhile some use cases may require processing deletes alongside updates and inserts, the most important delete requests are those that allow companies to maintain compliance with privacy regulations such as GDPR and CCPA. Most companies have stated SLAs around how long these requests will take to process, but for various reasons, these are often handled in pipelines separate from their core ETL.\n\nHere, we should a single user being deleted from our `silver` table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"27525638-10b5-4cf5-b77d-4343fa83e54f"}}},{"cell_type":"code","source":["%sql\nDELETE FROM silver WHERE mrn = 14125426"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c747ac47-d525-4a19-8c62-7b76cfd71a13"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["As expected, when we try to locate this user in our `silver` table, we'll get no result."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"63b38b02-e12c-4abf-8017-a15b14e54fb5"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM silver WHERE mrn = 14125426"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb27452d-50b8-46ef-9930-875a870dfa34"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["This change has been captured in our Change Data Feed."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0db08cf4-13f8-4d41-bf12-06233aa23807"}}},{"cell_type":"code","source":["%sql\nSELECT * \nFROM table_changes(\"silver\", 0)\nWHERE mrn = 14125426"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8293a2bb-e0d0-4082-9003-076c7b415bdb"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Because we have a record of this delete action, we can define logic that propagates deletes to our `gold` table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2a70cbcb-153c-4a37-87e9-79beab3ba71b"}}},{"cell_type":"code","source":["%sql\nWITH deletes AS (\n  SELECT mrn\n  FROM table_changes(\"silver\", 0)\n  WHERE _change_type='delete'\n)\n\nMERGE INTO gold g\nUSING deletes d\nON d.mrn=g.mrn\nWHEN MATCHED\n  THEN DELETE"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6d6fa5a4-74a3-4dbb-90a3-cd4f2f924784"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["This drastically simplifies deleting user data, and allows the keys and logic used in your ETL to also be used for propagating delete requests."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dbce8f1a-8d4b-4727-a23a-dcae0f06e0e6"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM gold WHERE mrn = 14125426"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"803aac8c-b46d-4b49-b5e1-c89eb82bde1e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2021 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b8274e0-add7-4177-b94e-161f24ca9ab3"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ADES 2.08 - Processing Records From Change Data Feed","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2598055170583427}},"nbformat":4,"nbformat_minor":0}
