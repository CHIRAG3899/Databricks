{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e8ad6e0c-968f-402a-bc82-c4eb26422959"}}},{"cell_type":"markdown","source":["# Quality Enforcement\n\nOne of the main motivations for using Delta Lake to store data is that you can provide guarantees on the quality of your data. While schema enforcement is automatic, additional quality checks can be helpful to ensure that only data that meets your expectations makes it into your Lakehouse.\n\n## Learning Objectives\nBy the end of this lesson, you should be able to:\n- Add check constraints to Delta tables\n- Describe and implement a quarantine table\n- Apply logic to add data quality tags to Delta tables"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f7b9ebb5-4482-49c4-b619-7aafa20472da"}}},{"cell_type":"code","source":["%run ../Includes/silver-setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f65d12d8-08cc-4327-a01c-4a2d6482e5f1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Table Constraints\n\nDatabricks allows <a href=\"https://docs.databricks.com/delta/delta-constraints.html\" target=\"_blank\">table constraints</a> to be set on Delta tables.\n\nTable constraints apply boolean filters to columns within a table and prevent data that does not fulfill these constraints from being written."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7660e203-078c-4194-822f-861a5f384011"}}},{"cell_type":"markdown","source":["Start by looking at our existing tables."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b534af8a-d394-4174-a164-7693848a670e"}}},{"cell_type":"code","source":["%sql\nSHOW TABLES"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8600e451-8ea3-436a-b7d4-8a97aa47034e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["If these exist, table constraints will be listed under the `properties` of the extended table description."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9f92becd-c1a9-4431-9771-3d2e2d3315da"}}},{"cell_type":"code","source":["%sql\nDESCRIBE EXTENDED heart_rate_silver"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a79288d-cfa3-4a01-8254-a21139c15a60"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["When defining a constraint, be sure to give it a human-readable name. (Note that names are not case sensitive.)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7e91c2b6-5599-4c83-9424-3247ad10ce16"}}},{"cell_type":"code","source":["%sql\nALTER TABLE heart_rate_silver ADD CONSTRAINT date_within_range CHECK (time > '2017-01-01');"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2bdb8d33-fa2c-4a53-80e2-d95f425ab516"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["None of the existing data in our table violated this constraint. Both the name and the actual check are displayed in the `properties` field."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9c8f97c1-5024-44a8-8bb8-6e04b7a1fee0"}}},{"cell_type":"code","source":["%sql\nDESCRIBE EXTENDED heart_rate_silver"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"21baf392-e8dd-43e3-959c-008ba548aee8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["But what happens if the conditions of the constraint aren't met?\n\nWe know that some of our devices occassionally send negative `bpm` recordings."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8164d3aa-0765-4a9e-adf0-f1784ecd55ff"}}},{"cell_type":"code","source":["%sql\nSELECT COUNT(*) FROM heart_rate_silver\nWHERE heartrate <= 0 "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb811e95-fd24-4aa5-9487-f7877025e765"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Delta Lake will prevent us from applying a constraint that existing records violate."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"524c0e99-41ed-4a94-8b51-e9fd953ae9bb"}}},{"cell_type":"code","source":["try:\n  spark.sql(\"ALTER TABLE heart_rate_silver ADD CONSTRAINT validbpm CHECK (heartrate > 0);\")\n  raise Exception(\"Expected failure\")\n  \nexcept pyspark.sql.utils.AnalysisException as e:\n  print(\"Failed as expected...\")\n  print(e)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f1fecd61-487b-4e7b-8e60-b0faf6b2f7b9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["How do we deal with this? \n\nWe could manually delete offending records and then set the check constraint, or set the check constraint before processing data from our bronze table.\n\nHowever, if we set a check constraint and a batch of data contains records that violate it, the job will fail and we'll throw an error. If our goal is to identify bad records but keep streaming jobs running, we'll need a different solution.\n\nOne idea would be to quarantine invalid records.\n\nNote that if you need to remove a constraint from a table, the following code would be executed."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bb841137-c235-4fb6-9ac6-113e1171ea1f"}}},{"cell_type":"code","source":["%sql\nALTER TABLE heart_rate_silver DROP CONSTRAINT validbpm;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"de2fbab9-e3ef-4778-8f1b-6f2ee7b6677f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Quarantining\n\nThe idea of quarantining is that bad records will be written to a separate location. This allows good data to processed efficiently, while additional logic and/or manual review of erroneous records can be defined and executed away from the main pipeline. Assuming that records can be successfully salvaged, they can be easily backfilled into the silver table they were deferred from."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"54d5adc5-2170-461b-9f6f-b96ddc75730f"}}},{"cell_type":"markdown","source":["Start by creating a table with the correct schema."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7ff493c6-32ba-45cb-a9dd-2df26021889c"}}},{"cell_type":"code","source":["spark.sql(\"DROP TABLE IF EXISTS bpm_quarantine\")\n\nspark.sql(f\"\"\"\nCREATE TABLE IF NOT EXISTS bpm_quarantine\n(device_id LONG, time TIMESTAMP, heartrate DOUBLE)\nUSING DELTA\nLOCATION '{Paths.silverPath}/bpm_quarantine'\n\"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"452e5e8b-ae9b-4ddb-aa16-78d917fa86d2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["With Structured Streaming operations, writing to an additional table can be accomplished within `foreachBatch` logic.\n\nBelow, we'll update the logic to add filters at the appropriate locations.\n\nFor simplicity, we won't check for duplicate records as we insert data into the quarantine table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"db4a4ef6-ea4f-4c43-a124-e0cf64665747"}}},{"cell_type":"code","source":["query = \"\"\"\nMERGE INTO heart_rate_silver a\nUSING stream_updates b\nON a.device_id=b.device_id AND a.time=b.time\nWHEN NOT MATCHED THEN INSERT *\n\"\"\"\n\nclass Upsert:\n    def __init__(self, query, update_temp=\"stream_updates\"):\n        self.query = query\n        self.update_temp = update_temp \n        \n    def upsertToDelta(self, microBatchDF, batch):\n        microBatchDF.filter(\"heartrate\" > 0).createOrReplaceTempView(self.update_temp)\n        microBatchDF._jdf.sparkSession().sql(self.query)\n        microBatchDF.filter(\"heartrate\" <= 0).write.format(\"delta\").mode(\"append\").saveAsTable(\"bpm_quarantine\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cda0468e-18be-46d1-b7d1-5be5f4f65bf9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note that within the `foreachBatch` logic, the DataFrame operations are treating the data in each batch as if it's static rather than streaming. As such, we use the `write` syntax instead of `writeStream`.\n\nThis also means that our exactly-once guarantees are relaxed. In our example above, we have two ACID transactions:\n1. Our SQL query executes to run an insert-only merge to avoid writing duplicate records to our silver table.\n2. We write a microbatch of records with negative heartrates to the `bpm_quarantine` table\n\nIf our job fails after our first transaction completes but before the second completes, we will re-execute the full microbatch logic on job restart.\n\nHowever, because our insert-only merge already prevents duplicate records from being saved to our table, this will not result in any data corruption."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"654aca95-ca3e-471e-85aa-c24564b291a0"}}},{"cell_type":"markdown","source":["## Flagging\nTo avoid multiple writes and managing multiple tables, you may choose to implement a flagging system to warn about violations while avoiding job failures. These flags can easily be used in downstream queries to isolate bad data.\n\n`case`/`when` logic makes this easy.\n\nRun the following cell to see the compiled Spark SQL from the PySpark code below."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e0762764-ef30-4869-813b-24fc30113e70"}}},{"cell_type":"code","source":["F.when(F.col(\"heartrate\") <= 0, \"Negative BPM\").otherwise(\"OK\").alias(\"bpm_check\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5e57095b-fe32-443d-9884-e99bf57e667d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Here, we'll just insert this logic as an additional transformation on a batch read of our bronze data to preview the output."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f352371a-a07d-424c-be75-45bce08d6fad"}}},{"cell_type":"code","source":["display(spark.read\n  .table(\"bronze\")\n  .filter(\"topic = 'bpm'\")\n  .select(F.from_json(F.col(\"value\").cast(\"string\"), \"device_id LONG, time TIMESTAMP, heartrate DOUBLE\").alias(\"v\"))\n  .select(\"v.*\", F.when(F.col(\"v.heartrate\") <= 0, \"Negative BPM\").otherwise(\"OK\").alias(\"bpm_check\"))\n  .dropDuplicates([\"device_id\", \"time\"])\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"38080017-7b28-46ca-a15d-7c8625a28e0f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2021 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"71886d24-dc52-4928-a328-a3319856dd53"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ADES 2.05 - Quality Enforcement","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2598055170583548}},"nbformat":4,"nbformat_minor":0}
