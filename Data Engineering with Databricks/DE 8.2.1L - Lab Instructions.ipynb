{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fdae68c6-3fdc-4984-bc23-f9b05c301cea"}}},{"cell_type":"markdown","source":["# Lab: Migrating SQL Notebooks to Delta Live Tables\n\nThis notebook describes the overall structure for the lab exercise, configures the environment for the lab, provides simulated data streaming, and performs cleanup once you are done. A notebook like this is not typically needed in a production pipeline scenario.\n\n## Learning Objectives\nBy the end of this lab, you should be able to:\n* Convert existing data pipelines to Delta Live Tables"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c40703ad-97a8-438c-936b-65d548b6b222"}}},{"cell_type":"markdown","source":["## Datasets Used\n\nThis demo uses simplified artificially generated medical data. The schema of our two datasets is represented below. Note that we will be manipulating these schema during various steps.\n\n#### Recordings\nThe main dataset uses heart rate recordings from medical devices delivered in the JSON format. \n\n| Field | Type |\n| --- | --- |\n| device_id | int |\n| mrn | long |\n| time | double |\n| heartrate | double |\n\n#### PII\nThese data will later be joined with a static table of patient information stored in an external system to identify patients by name.\n\n| Field | Type |\n| --- | --- |\n| mrn | long |\n| name | string |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6dfadc4d-0c8b-4851-8a1b-e175ce56a4a1"}}},{"cell_type":"markdown","source":["## Getting Started\n\nBegin by running the following cell to configure the lab environment."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"230647a5-ace5-4c85-928c-30d016d7ebc1"}}},{"cell_type":"code","source":["%run ../../Includes/Classroom-Setup-8.2.1L"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dcee542b-c805-4275-aad6-0ddda37f8da0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Land Initial Data\nSeed the landing zone with some data before proceeding. You will re-run this command to land additional data later."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"85bfcc7c-2658-46f7-b80d-1584c25a1be7"}}},{"cell_type":"code","source":["DA.data_factory.load()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7638ef60-71c0-47ac-97ea-6ed98ba2239d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Execute the following cell to print out values that will be used during the following configuration steps."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f38bbd4b-9f33-44e3-bbf0-534782934b95"}}},{"cell_type":"code","source":["DA.print_pipeline_config()    "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1ed453af-ed6c-4420-a7f7-43857294c110"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Create and Configure a Pipeline\n\n1. Click the **Jobs** button on the sidebar, then select the **Delta Live Tables** tab.\n1. Click **Create Pipeline**.\n1. Leave **Product Edition** as **Advanced**.\n1. Fill in a **Pipeline Name** - because these names must be unique, we suggest using the **Pipline Name** provided in the cell above.\n1. For **Notebook Libraries**, use the navigator to locate and select the notebook **`DE 8.2.2L - Migrating a SQL Pipeline to DLT Lab`**.\n1. Configure the Source\n    * Click **`Add configuration`**\n    * Enter the word **`source`** in the **Key** field\n    * Enter the **Source** value specified above to the **`Value`** field\n1. Enter the database name printed next to **`Target`** below in the **Target** field.\n1. Enter the location printed next to **`Storage Location`** below in the **Storage Location** field.\n1. Set **Pipeline Mode** to **Triggered**.\n1. Disable autoscaling.\n1. Set the number of **`workers`** to **`1`** (one).\n1. Click **Create**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"49b33e1d-94dd-4b2d-8709-28439dffc7f1"}}},{"cell_type":"markdown","source":["## Open and Complete DLT Pipeline Notebook\n\nYou will perform your work in the companion notebook [DE 8.2.2L - Migrating a SQL Pipeline to DLT Lab]($./DE 8.2.2L - Migrating a SQL Pipeline to DLT Lab),<br/>\nwhich you will ultimately deploy as a pipeline.\n\nOpen the Notebook and, following the guidelines provided therein, fill in the cells where prompted to<br/>\nimplement a multi-hop architecture similar to the one we worked with in the previous section."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a2efce79-0100-4d3f-9aa3-2a4c2945eb20"}}},{"cell_type":"markdown","source":["## Run your Pipeline\n\nSelect **Development** mode, which accelerates the development lifecycle by reusing the same cluster across runs.<br/>\nIt will also turn off automatic retries when jobs fail.\n\nClick **Start** to begin the first update to your table.\n\nDelta Live Tables will automatically deploy all the necessary infrastructure and resolve the dependencies between all datasets.\n\n**NOTE**: The first table update may take several minutes as relationships are resolved and infrastructure deploys."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fe1b442a-9fe0-4391-a329-2cbc0f31df46"}}},{"cell_type":"markdown","source":["## Troubleshooting Code in Development Mode\n\nDon't despair if your pipeline fails the first time. Delta Live Tables is in active development, and error messages are improving all the time.\n\nBecause relationships between tables are mapped as a DAG, error messages will often indicate that a dataset isn't found.\n\nLet's consider our DAG below:\n\n<img src=\"https://files.training.databricks.com/images/dlt-dag.png\">\n\nIf the error message **`Dataset not found: 'recordings_parsed'`** is raised, there may be several culprits:\n1. The logic defining **`recordings_parsed`** is invalid\n1. There is an error reading from **`recordings_bronze`**\n1. A typo exists in either **`recordings_parsed`** or **`recordings_bronze`**\n\nThe safest way to identify the culprit is to iteratively add table/view definitions back into your DAG starting from your initial ingestion tables. You can simply comment out later table/view definitions and uncomment these between runs."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"042fd0a9-3466-4a72-8258-820dfb33188a"}}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24c2b85e-5e2a-4c52-9c60-015ae9dc0c3f"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"DE 8.2.1L - Lab Instructions","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2841292000076987}},"nbformat":4,"nbformat_minor":0}
