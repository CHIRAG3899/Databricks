{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"19369265-28c7-43c0-8144-2d9d2fb663fc"}}},{"cell_type":"markdown","source":["# Streaming Design Patterns\n\nThe Lakehouse has been designed from the beginning to work seamlessly with datasets that grow infinitely over time. While Spark Structured Streaming is often positioned as a near real-time data processing solution, it combines with Delta Lake to also provide easy batch processing of incremental data while drastically simplifying the overhead required to track data changes over time.\n\n## Learning Objectives\nBy the end of this lessons, student will be able to:\n- Use Structured Streaming to complete simple incremental ETL\n- Perform incremental writes to multiple tables\n- Incrementally update values in a key value store\n- Process Change Data Capture (CDC) data into Delta Tables using `MERGE`\n- Join two incremental tables\n- Join incremental and batch tables"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ffbaa4b0-0f9f-472d-93b2-4287e562966b"}}},{"cell_type":"markdown","source":["Run the following script to setup necessary variables and clear out past runs of this notebook."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"38063bd8-c46d-42d2-b8d2-dd253f609648"}}},{"cell_type":"code","source":["%run ../Includes/sql-setup $course=\"stream_design\" $mode=\"reset\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b4fcb810-b4ec-4f2a-bfe4-a69bab2cf036"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note that because Structured Streaming will be used throughout this lesson, checkpoint directories will need to be specified for each of our different streaming queries.\n\nThe code below declares the checkpoints used throughout the lesson, and does a recursive delete to remove any state information from previous runs."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"75cd63eb-6a72-4e66-93cc-ef90d2dca896"}}},{"cell_type":"code","source":["checkpointPath = userhome + \"/_checkpoints/\"\nsilverCheckpoint = checkpointPath + \"silver/\"\nsplitStreamCheckpoint = checkpointPath + \"split_stream/\"\nkeyValueCheckpoint = checkpointPath + \"key_value/\"\nsilverStatusCheckpoint = checkpointPath + \"silver_status/\"\njoinedCheckpoint = checkpointPath + \"joined/\"\njoinStatusCheckpoint = checkpointPath + \"join_status/\"\n\ndbutils.fs.rm(checkpointPath, True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"595c34f7-ec7d-43b8-b573-c97eb1048191"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Simple Incremental ETL\n\nLikely the highest volume of data being processed by most organizations could largely be describing as moving data from one location to another while applying light transformations and validations. As most source data continues to grow as time passes, it's appropriate to refer to this data as incremental (sometimes also referred to as streaming data). Structured Streaming and Delta Lake make incremental ETL easy. \n\nBelow we'll create a simple table and insert some values."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58dc4097-d9dd-4a10-9cb4-ad3f7bba77ad"}}},{"cell_type":"code","source":["%sql\n\nCREATE TABLE bronze \n(id INT, name STRING, value DOUBLE); \n\nINSERT INTO bronze\nVALUES (1, \"Yve\", 1.0),\n  (2, \"Omar\", 2.5),\n  (3, \"Elia\", 3.3)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c44cdc5b-d005-4664-bbf9-7481579c2857"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The following cell defines an incremental read on the table just created using Structured Streaming, adds a field to capture when the record was processed, and writes out to a new table as a single batch."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9d1592cb-1236-4dea-b1b3-3087dc68e348"}}},{"cell_type":"code","source":["def update_silver():\n    spark.readStream.table(\"bronze\").withColumn(\"processed_time\", F.current_timestamp()).writeStream.option(\"checkpointLocation\", silverCheckpoint).trigger(once=True).table(\"silver\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"07d78988-6e02-41b4-bf6e-458702496bc0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note that while this code uses Structured Streaming, it's appropriate to think of this as a triggered batch processing incremental changes."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7da10c40-937e-4213-afa7-149375bbd649"}}},{"cell_type":"code","source":["update_silver()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a1806587-8971-46e8-af63-921ba3edc4de"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["As expected, the stream runs for a very brief time, and the `silver` table written contains all the values previously written to `bronze`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"34d821ad-e846-469c-befb-40c52d751a32"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM silver"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11ea4439-7e8b-4f12-91c1-6a017677a126"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Processing new records is as easy as adding them to our source table `bronze`..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0dcd5299-33f8-4111-9235-f755b94b3449"}}},{"cell_type":"code","source":["%sql\nINSERT INTO bronze\nVALUES (4, \"Ted\", 4.7),\n  (5, \"Tiffany\", 5.5),\n  (6, \"Vini\", 6.3)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5829ba26-93aa-417a-a98e-dc154e6ab095"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["... and re-executing the incremental batch processing code."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ebc595fd-dad4-44cf-874f-334f53b40647"}}},{"cell_type":"code","source":["update_silver()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5a4af8ed-5f6f-4eac-bb0a-8972b6ec53df"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Delta Lake is ideally suited for easily tracking and propagating inserted data through a series of tables. This pattern has a number of names, including \"medallion\", \"multi-hop\", \"Delta\", and \"bronze/silver/gold\" architecture."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3220927f-40a7-412b-bcb5-d01e2c5baffa"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM silver"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c5d5d7f9-0c61-4b8c-875b-ab2e35103d2c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Writing to Multiple Tables\n\nThose familiar with Structured Streaming may be aware that the `foreachBatch` method provides the option to execute custom data writing logic on each microbatch of streaming data.\n\nNew DBR functionality provides guarantees that these writes will be idempotent, even when writing to multiple tables. This is especially useful when data for multiple tables might be contained within a single record.\n\nThe code below first defines the custom writer logic to append records to two new tables, and then demonstrates using this function within `foreachBatch`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8107f143-2e70-4a63-9eb2-d0c08ac5911b"}}},{"cell_type":"code","source":["def write_twice(microBatchDF, batchId):\n    appId = 'write_twice'\n    \n    microBatchDF.select(\"id\", \"name\", F.current_timestamp().alias(\"processed_time\")).write.option(\"txnVersion\", batchId).option(\"txnAppId\", appId).mode(\"append\").saveAsTable(\"silver_name\")\n    \n    microBatchDF.select(\"id\", \"value\", F.current_timestamp().alias(\"processed_time\")).write.option(\"txnVersion\", batchId).option(\"txnAppId\", appId).mode(\"append\").saveAsTable(\"silver_value\")\n\n\ndef split_stream():\n    (spark.readStream.table(\"bronze\")\n        .writeStream\n        .foreachBatch(write_twice)\n        .outputMode(\"update\")\n        .option(\"checkpointLocation\", splitStreamCheckpoint)\n        .trigger(once=True)\n        .start())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0855abed-eee9-480e-a695-889fed73b8cd"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note that while a stream will again be triggered, the two writes contained within the `write_twice` function are using Spark batch syntax. This will always be the case for writers called by `foreachBatch`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e139690-ae4a-464e-b6ac-52de98e6efc1"}}},{"cell_type":"code","source":["split_stream()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"beafca3b-6b16-4e2b-afa8-49c7f256248b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The cells below demonstrate the logic was applied properly to split the initial data into two tables."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"390e89af-e59b-4b78-be5c-f7820d100a58"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM silver_name"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58ff0f63-8fa1-4eb7-a5e5-c174a6e37d44"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note that the `processed_time` for each of these tables differs slightly. The logic defined above captures the current timestamp at the time each write executes, demonstrating that while both writes happen within the same streaming microbatch process, they are fully independent transactions (as such, downstream logic should be tolerant for slightly asynchronous updates)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e990df89-da5f-4a87-a927-58c5f91850e2"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM silver_value"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"305279b7-01de-4a4f-8bf1-cb1c57c4e617"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Insert more values into the `bronze` table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9c1e3b36-24b0-45bc-a39b-d1e27ae78726"}}},{"cell_type":"code","source":["%sql\nINSERT INTO bronze\nVALUES (7, \"Viktor\", 7.4),\n  (8, \"Hiro\", 8.2),\n  (9, \"Shana\", 9.9)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d6cdad3-d4d7-422d-9139-fc0030993827"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["And we can now pick up these new records and write to two tables."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"811b3320-f0df-4b89-a272-403edd906946"}}},{"cell_type":"code","source":["split_stream()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9584622d-032b-474e-9dbb-38a9e1fa085a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["As expected, only new values are inserted into the two tables, again a few moments apart."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"15e7657f-ab65-443f-b1d9-e66cb8499099"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM silver_name"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e76f6306-7dfa-490f-8e8a-40d1c219af8a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT * FROM silver_value"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"913ff160-f1b1-4201-b07d-7753bf7630aa"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Update Aggregates in a Key-Value Store\n\nIncremental aggregation can be useful for a number of purposes, including dashboarding and enriching reports with current summary data.\n\nThe logic below defines a handful of aggregations against the `silver` table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"487641b5-de72-46d2-807a-54b42543a576"}}},{"cell_type":"code","source":["def update_key_value():\n    (spark.readStream\n         .table(\"silver\")\n         .groupBy(\"id\")\n         .agg(F.sum(\"value\").alias(\"total_value\"), \n              F.mean(\"value\").alias(\"avg_value\"),\n              F.count(\"value\").alias(\"record_count\"))\n         .writeStream\n         .option(\"checkpointLocation\", keyValueCheckpoint)\n         .outputMode(\"complete\")\n         .trigger(once=True)\n         .table(\"key_value\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cd5cd065-f365-4ccc-8ef4-540d497a7d2d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**NOTE**: Because the transformations above require shuffling data, setting the number of partitions to map to the cores in our streaming cluster will provide more efficient performance. (If the cluster size will be scaled up for production, the maximum number of cores that will be present in the cluster should be used when configuring this setting.)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b33074e1-f5b2-4ee3-9cdb-033fe0754d4b"}}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", 4)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9c82969c-2f1d-41db-9027-7a5daa13129a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["update_key_value()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d3fd1958-8b77-44e0-8b0a-b1e785eabe6a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT * FROM key_value"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b4a9a62a-c252-44db-963c-e81740bb292d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Adding more values to the `silver` table will allow more interesting aggregation."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d06e219b-c145-4181-8fc6-4b89b4a47796"}}},{"cell_type":"code","source":["%sql\nINSERT INTO silver\nVALUES (1, \"Yve\", 1.0, current_timestamp()),\n  (2, \"Omar\", 2.5, current_timestamp()),\n  (3, \"Elia\", 3.3, current_timestamp()),\n  (7, \"Viktor\", 7.4, current_timestamp()),\n  (8, \"Hiro\", 8.2, current_timestamp()),\n  (9, \"Shana\", 9.9, current_timestamp())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c4d5651f-ed3c-477b-a041-4715eb25e06b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["One thing to note is that the logic being executed is currently overwriting the resulting table with each write. In the next section, `MERGE` will be used in combination with `foreachBatch` to update existing records. This pattern can also be applied with key-value stores."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cadb2051-315e-430b-b5d0-a33679d8c9fb"}}},{"cell_type":"code","source":["update_key_value()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bfbcf1c7-a0b2-48c2-9b6e-910a0ca4c250"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT * FROM key_value"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f5248c2a-2e4f-4b83-b731-14802f960543"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Processing Change Data Capture Data\nWhile the change data capture (CDC) data emitted by various systems will vary greatly, incrementally processing these data with Databricks is straightforward.\n\nHere the `bronze_status` table will represent the raw CDC information, rather than row-level data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b108491-daff-4a7a-ab07-35d81abc240c"}}},{"cell_type":"code","source":["%sql\nCREATE TABLE bronze_status \n(user_id INT, status STRING, update_type STRING, processed_timestamp TIMESTAMP);\n\nINSERT INTO bronze_status\nVALUES  (1, \"new\", \"insert\", current_timestamp()),\n        (2, \"repeat\", \"update\", current_timestamp()),\n        (3, \"at risk\", \"update\", current_timestamp()),\n        (4, \"churned\", \"update\", current_timestamp()),\n        (5, null, \"delete\", current_timestamp())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a51f2538-76c2-410c-90b4-b98f45a1fe1e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The `silver_status` table below has been created to track the current `status` for a given `user_id`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"636fdeda-d8a1-440f-847b-060bd645b020"}}},{"cell_type":"code","source":["%sql\nCREATE TABLE silver_status (user_id INT, status STRING, updated_timestamp TIMESTAMP)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5a5821d1-d9c9-413d-917d-a6067e5cc157"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The `MERGE` statement can easily be written with SQL to apply CDC changes appropriately, given the type of update received.\n\nThe rest of the `upsert_cdc` method contains the logic necessary to run SQL code against a micro-batch in a PySpark DataStreamWriter."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bff3a694-c242-4523-b0d6-c0427722e12f"}}},{"cell_type":"code","source":["def upsert_cdc(microBatchDF, batchID):\n    microBatchDF.createTempView(\"bronze_batch\")\n    \n    query = \"\"\"\n        MERGE INTO silver_status s\n        USING bronze_batch b\n        ON b.user_id = s.user_id\n        WHEN MATCHED AND b.update_type = \"update\"\n          THEN UPDATE SET user_id=b.user_id, status=b.status, updated_timestamp=b.processed_timestamp\n        WHEN MATCHED AND b.update_type = \"delete\"\n          THEN DELETE\n        WHEN NOT MATCHED AND b.update_type = \"update\" OR b.update_type = \"insert\"\n          THEN INSERT (user_id, status, updated_timestamp)\n          VALUES (b.user_id, b.status, b.processed_timestamp)\n    \"\"\"\n    \n    microBatchDF._jdf.sparkSession().sql(query)\n    \ndef streaming_merge():\n    spark.readStream.table(\"bronze_status\").writeStream.foreachBatch(upsert_cdc).option(\"checkpointLocation\", silverStatusCheckpoint).outputMode(\"update\").trigger(once=True).start()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d13e665b-f71b-498c-9424-568ab1aa41c3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["As always, we incrementally process newly arriving records."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c36088c0-0ad8-44e7-a9b8-1608e763252c"}}},{"cell_type":"code","source":["streaming_merge()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"59fef3ca-b66e-48df-80d8-a0ec8d0fa9a5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT * FROM silver_status"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f23603d-bfad-49df-854c-970b4e946f3d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Inserting new records will allow us to then apply these changes to our silver data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b6acbb07-d435-4f62-ad6a-fe8b1bae0643"}}},{"cell_type":"code","source":["%sql\nINSERT INTO bronze_status\nVALUES  (1, \"repeat\", \"update\", current_timestamp()),\n        (2, \"at risk\", \"update\", current_timestamp()),\n        (3, \"churned\", \"update\", current_timestamp()),\n        (4, null, \"delete\", current_timestamp()),\n        (6, \"new\", \"insert\", current_timestamp())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fc4370c2-b892-4790-adc6-d36a71eaa168"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["streaming_merge()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"adf07c9e-82c9-41e5-b9f6-5a3e8f6f27a5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note that at present, the logic would not be particularly robust to data arriving out-of-order or duplicate records (but these occurences can be handled)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cd05f12b-0834-4267-a34f-ef1c07f01004"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM silver_status"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"392bb928-5f9b-42fc-ba3d-539b0d4785a0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Joining Two Incremental Tables\n\nNote that there are many intricacies around watermarking and windows when dealing with incremental joins, and that not all join types are supported."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b77e7f82-d2da-41ba-8869-1ce400c4867e"}}},{"cell_type":"code","source":["def stream_stream_join():\n    nameDF = spark.readStream.table(\"silver_name\")\n    valueDF = spark.readStream.table(\"silver_value\")\n    \n    (nameDF.join(valueDF, nameDF.id == valueDF.id, \"inner\")\n        .select(nameDF.id, \n                nameDF.name, \n                valueDF.value, \n                F.current_timestamp().alias(\"joined_timestamp\"))\n        .writeStream\n        .option(\"checkpointLocation\", joinedCheckpoint)\n        .table(\"joined_streams\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ee4993ca-f1b5-4425-845f-a89d62a2ccd8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note that the logic defined above does not set a `trigger` option. This means that the stream will run in continuous execution mode, triggering every 500ms by default."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"59f96196-005a-4f1b-b81d-63c6eb65d211"}}},{"cell_type":"code","source":["stream_stream_join()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"41e37cba-2643-425b-ac06-daf5b6c95ea1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Running `display()` on a streaming table read is a way to monitor table updates in near-real-time while in interactive development. Note that a separate stream is started."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e3de9e42-aee2-49e7-928d-11dcbb74df5b"}}},{"cell_type":"code","source":["display(spark.readStream.table(\"joined_streams\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2c3691bd-9ac3-4a70-bb01-5085b955d7a9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Here we'll add new values to the `bronze` table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd7a4cb9-50c5-4709-83f5-8ba7ccb9cf5c"}}},{"cell_type":"code","source":["%sql\nINSERT INTO bronze\nVALUES (10, \"Pedro\", 10.5),\n  (11, \"Amelia\", 11.5),\n  (12, \"Diya\", 12.3),\n  (13, \"Li\", 13.4),\n  (14, \"Daiyu\", 14.2),\n  (15, \"Jacques\", 15.9)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0ceef4a9-0fac-4e90-b819-7845773b8b9e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The stream-stream join is configured against the tables resulting from the `split_stream` function; run this again and data should quickly process through the streaming join running above."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b3feddb9-86fc-4cb3-b21a-061f9a44c67a"}}},{"cell_type":"code","source":["split_stream()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"af405b4b-dad2-4a01-b1ab-8cb697ca9417"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Interactive streams should always be stopped before leaving a notebook session, as they can keep clusters from timing out and incur unnecessary cloud costs."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d0b30153-91d3-48ee-8d0f-d148df1ab27a"}}},{"cell_type":"code","source":["for stream in spark.streams.active:\n    stream.stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"df319f7b-881f-4182-862d-222f14b39eea"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Join Incremental and Static Data\n\nWhile incremental tables are ever-appending, static tables typically can be thought of as containing data that may be changed or overwritten.\n\nBecause of Delta Lake's transactional guarantees and caching, Databricks ensures that each microbatch of streaming data that's joined back to a static table will contain the current version of data from the static table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"26a29d3e-836a-4500-b76f-2fe4eedc0d60"}}},{"cell_type":"code","source":["statusDF = spark.read.table(\"silver_status\")\nbronzeDF = spark.readStream.table(\"bronze\")\n\nbronzeDF.alias(\"bronze\").join(statusDF.alias(\"status\"), bronzeDF.id==statusDF.user_id, \"inner\").select(\"bronze.*\", \"status.status\").writeStream.option(\"checkpointLocation\", joinStatusCheckpoint).table(\"joined_status\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ec1c0f77-71d1-4107-a308-d665c25c249e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT * FROM joined_status"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"159b5107-1feb-4dcd-bef2-a54c07aace15"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Only those records with a matching `id` in `joined_status` at the time the stream is processed will be represented in the resulting table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1d0c9e2a-d973-4685-b4f3-2787b4b30732"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM silver_status"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1bca50d2-ce96-4803-b997-8605021360e4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Processing new records into the `silver_status` table will not automatically trigger updates to the results of the stream-static join."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5219da78-9f70-49e1-9b09-9797aa4ec574"}}},{"cell_type":"code","source":["%sql\nINSERT INTO bronze_status\nVALUES  (11, \"repeat\", \"update\", current_timestamp()),\n        (12, \"at risk\", \"update\", current_timestamp()),\n        (16, \"new\", \"insert\", current_timestamp()),\n        (17, \"repeat\", \"update\", current_timestamp())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f41b3d71-9842-4cfd-b599-e3f12805a9b7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["streaming_merge()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9be61877-83d4-472e-9b6c-ca2c90f4332a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT * FROM joined_status"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"744524b1-aa35-4c8c-ad53-ccb5ef92d9b5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Only new data appearing on the streaming side of the query will trigger records to process using this pattern."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2b17966f-d699-4e9b-bd0c-a7d0e76e55a1"}}},{"cell_type":"code","source":["%sql\nINSERT INTO bronze\nVALUES (16, \"Marissa\", 1.9),\n  (17, \"Anne\", 2.7)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e751dde4-26fc-4e8b-b637-988aca9c1b26"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The incremental data in a stream-static join \"drives\" the stream, guaranteeing that each microbatch of data joins with the current values present in the valid version of the static table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"da2f1c6c-16af-4683-8f4b-b75669b5f7ac"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM joined_status"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2997a219-d2ed-45bf-bf0d-395e1e78adfb"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Stop Streaming Jobs"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f318c39d-7768-416d-a157-6941cbd30f4f"}}},{"cell_type":"code","source":["# Stop Streaming Job\nfor stream in spark.streams.active:\n        stopped = True\n        stream.stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"93d6da42-4393-42fe-8545-b438318f2f50"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2021 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1a9dbc6f-142e-4cc3-85d7-4c0efba284bf"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ADES 1.04 - Streaming Design Patterns","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2598055170583135}},"nbformat":4,"nbformat_minor":0}
