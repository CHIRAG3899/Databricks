{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7fe8d95d-3eba-4d8e-b682-af5f6dcbbfd1"}}},{"cell_type":"markdown","source":["# Reset Pipelines\n\nIn this notebook, code is provided to remove all existing databases, data, and tables. Code is then provided to redeclare each table used in the architecture."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"91f94f29-ee65-4f48-83e4-f2781d0905b6"}}},{"cell_type":"code","source":["%run ../Includes/reset-&-install-datasets"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5d7f1ead-3a4e-402a-bbcd-f9e06d1e5a0a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We'll be using the `bronze_dev` table, which is a clone that already contains all of our daily data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"883be505-66dc-49c5-a5c0-848dd6414481"}}},{"cell_type":"code","source":["display(spark.sql(f\"SHOW TABLES IN {database}\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"80f0c816-2875-4231-b9ee-f8da347b3b14"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Code to declare all the other tables in our pipelines are provided below."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"13aae910-15d3-402d-be22-fb6487a0eb1f"}}},{"cell_type":"code","source":["spark.sql(f\"\"\"\n  CREATE TABLE IF NOT EXISTS heart_rate_silver\n  (device_id LONG, time TIMESTAMP, heartrate DOUBLE, bpm_check STRING)\n  USING DELTA\n  LOCATION '{Paths.silverRecordingsTable}'\n\"\"\")\n\nspark.sql(f\"\"\"\n  CREATE TABLE IF NOT EXISTS workouts_silver\n  (user_id INT, workout_id INT, time TIMESTAMP, action STRING, session_id INT)\n  USING DELTA\n  LOCATION '{Paths.silverWorkoutsTable}'\n\"\"\")\n\nspark.sql(f\"\"\"\n  CREATE TABLE IF NOT EXISTS users\n  (alt_id STRING, dob DATE, sex STRING, gender STRING, first_name STRING, last_name STRING, street_address STRING, city STRING, state STRING, zip INT, updated TIMESTAMP)\n  USING DELTA\n  LOCATION '{Paths.users}'\n\"\"\")\n\nspark.sql(f\"\"\"\n    CREATE TABLE IF NOT EXISTS gym_mac_logs\n    (first_timestamp DOUBLE, gym BIGINT, last_timestamp DOUBLE, mac STRING)\n    USING delta\n    LOCATION '{Paths.gymMacLogs}'\n\"\"\")\n\nspark.sql(f\"\"\"\n    CREATE TABLE IF NOT EXISTS completed_workouts\n    (user_id INT, workout_id INT, session_id INT, start_time TIMESTAMP, end_time TIMESTAMP, in_progress BOOLEAN)\n    USING DELTA\n    LOCATION '{Paths.completedWorkouts}'\n\"\"\")\n\nspark.sql(f\"\"\"\n    CREATE TABLE IF NOT EXISTS workout_bpm\n    (user_id INT, workout_id INT, session_id INT, time TIMESTAMP, heartrate DOUBLE)\n    USING DELTA\n    LOCATION '{Paths.workoutBpm}'\n\"\"\")\n\nspark.sql(f\"\"\"\n    CREATE TABLE IF NOT EXISTS user_bins\n    (user_id BIGINT, age STRING, gender STRING, city STRING, state STRING)\n    USING DELTA\n    LOCATION '{Paths.userBins}'\n\"\"\")\n\nspark.sql(f\"\"\"\n    CREATE TABLE IF NOT EXISTS registered_users\n    (device_id long, mac_address string, registration_timestamp double, user_id long)\n    USING DELTA \n    LOCATION '{Paths.registeredUsers}'\n\"\"\")\n\nspark.sql(f\"\"\"\n    CREATE TABLE IF NOT EXISTS user_lookup\n    (alt_id string, device_id long, mac_address string, user_id long)\n    USING DELTA \n    LOCATION '{Paths.userLookup}'\n\"\"\")\n\nspark.sql(f\"\"\"\n    CREATE TABLE IF NOT EXISTS workout_bpm_summary\n    (workout_id INT, session_id INT, user_id BIGINT, age STRING, gender STRING, city STRING, state STRING, min_bpm DOUBLE, avg_bpm DOUBLE, max_bpm DOUBLE, num_recordings BIGINT)\n    USING DELTA \n    LOCATION '{Paths.workoutBpmSummary}'\n\"\"\")\n\nspark.sql(f\"\"\"\n    CREATE VIEW IF NOT EXISTS gym_user_stats AS (\n    SELECT gym, mac_address, date, workouts, (last_timestamp - first_timestamp)/60 minutes_in_gym, (to_unix_timestamp(end_workout) - to_unix_timestamp(start_workout))/60 minutes_exercising\n    FROM gym_mac_logs c\n    INNER JOIN (\n      SELECT b.mac_address, to_date(start_time) date, collect_set(workout_id) workouts, min(start_time) start_workout, max(end_time) end_workout\n          FROM completed_workouts a\n          INNER JOIN user_lookup b\n          ON a.user_id = b.user_id\n          GROUP BY mac_address, to_date(start_time)\n      ) d\n      ON c.mac = d.mac_address AND to_date(CAST(c.first_timestamp AS timestamp)) = d.date)\n\"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2915453a-d9d7-4ad4-b4d4-0d251111dd8f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["For this demo, we're only focused on processing those data coming through our multiplex bronze table, so we'll bypass the incremental loading for the `gym_mac_logs` and `user_lookup` tables and recreate the final results with a direct read of all files."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cc04dc19-ee49-4efb-9c1c-4a9f6cac71d4"}}},{"cell_type":"code","source":["spark.read.json(f\"{URI}/gym-logs/\").write.option(\"path\", Paths.gymMacLogs).mode(\"overwrite\").saveAsTable(\"gym_mac_logs\")\n\n(spark.read\n    .format(\"json\")\n    .schema(\"device_id long, mac_address string, registration_timestamp double, user_id long\")\n    .load(f\"{URI}/user-reg\")\n    .selectExpr(f\"sha2(concat(user_id,'BEANS'), 256) AS alt_id\", \"device_id\", \"mac_address\", \"user_id\")\n    .write\n    .option(\"path\", Paths.userLookup)\n    .mode(\"overwrite\")\n    .saveAsTable(\"user_lookup\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1091a752-394c-4bde-bfd1-b74636684b56"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2021 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ebf42def-e696-497c-bae1-20093979d9ef"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ADES 4.01 - Reset Pipelines","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2598055170582974}},"nbformat":4,"nbformat_minor":0}
