{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8efe4667-59c9-4e32-adcb-d4ced0b2f93a"}}},{"cell_type":"markdown","source":["# Scheduling a Batch Job\n\nThis notebook is designed to be scheduled as a batch job. \n\nAs a general rule, before scheduling notebooks, make sure you comment out:\n- Any file removal commands added during development\n- Any commands dropping or creating databases or tables (unless you wish these to be created fresh with each execution)\n- Any arbitrary actions/SQL queries that materialize results to the notebook (unless a human will regularly review this visual output)\n\n### Scheduling Against an Interactive Cluster\n\nBecause our data is small and the query we run here will complete fairly quickly, we'll take advantage of our already-on compute while scheduling this notebook.\n\nAfter defining a new job and selecting this notebook, click `Edit` on the far right of your cluster specs. On the screen that follows:\n\n![existing-cluster](https://files.training.databricks.com/images/enb/med_data/existing-cluster.png)\n\n1. Click the arrows under `Cluster Type` and choose \"Existing Interactive Cluster\"\n2. Select the cluster you've been using throughout class\n3. Click confirm\n\nOnce you're back to the jobs definition screen:\n\n![schedule-batch](https://files.training.databricks.com/images/enb/med_data/schedule-batch.png)\n\n1. Click `Edit` next to **Schedule**: None\n2. Change the scheduled frequency to every minute\n3. Click confirm\n\nYou can click `Run Now` if desired, or just wait until the top of the next minute for this to trigger automatically.\n\n### Best Practice: Warm Pools\n\nDuring this demo, we're making the conscious choice to take advantage of already-on compute to reduce friction and complexity for getting our code running. In production, jobs like this one (short duration and triggered frequently) should be scheduled against [warm pools](https://docs.microsoft.com/en-us/azure/databricks/clusters/instance-pools/).\n\nPools provide you the flexibility of having compute resources ready for scheduling jobs against while removing DBU charges for idle compute. DBUs billed are for jobs rather than all-purpose workloads (which is a lower cost). Additionally, using pools instead of interactive clusters eliminates the potential for resource contention between jobs sharing a single cluster or between scheduled jobs and interactive queries."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5fe6e831-8cbb-42c1-85d9-7175caf0c976"}}},{"cell_type":"code","source":["%run ../Includes/ade-setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6ae913b0-b652-4de6-8a73-7d58c6d7bf05"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# user_bins\ndef age_bins(dob_col):\n    age_col = F.floor(F.months_between(F.current_date(), dob_col)/12).alias(\"age\")\n    return (F.when((age_col < 18), \"under 18\")\n            .when((age_col >= 18) & (age_col < 25), \"18-25\")\n            .when((age_col >= 25) & (age_col < 35), \"25-35\")\n            .when((age_col >= 35) & (age_col < 45), \"35-45\")\n            .when((age_col >= 45) & (age_col < 55), \"45-55\")\n            .when((age_col >= 55) & (age_col < 65), \"55-65\")\n            .when((age_col >= 65) & (age_col < 75), \"65-75\")\n            .when((age_col >= 75) & (age_col < 85), \"75-85\")\n            .when((age_col >= 85) & (age_col < 95), \"85-95\")\n            .when((age_col >= 95), \"95+\")\n            .otherwise(\"invalid age\").alias(\"age\"))\n\nlookupDF = spark.table(\"user_lookup\").select(\"alt_id\", \"user_id\")\nbinsDF = spark.table(\"users\").join(lookupDF, [\"alt_id\"], \"left\").select(\"user_id\", age_bins(F.col(\"dob\")),\"gender\", \"city\", \"state\")\n\n(binsDF.write\n    .format(\"delta\")\n    .option(\"path\", Paths.userBins)\n    .mode(\"overwrite\")\n    .saveAsTable(\"user_bins\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0caa1985-ccd0-433b-8f5a-ec0aa2a9c1aa"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# completed_workouts\nspark.sql(\"\"\"\n    CREATE OR REPLACE TEMP VIEW TEMP_completed_workouts AS (\n      SELECT a.user_id, a.workout_id, a.session_id, a.start_time start_time, b.end_time end_time, a.in_progress AND (b.in_progress IS NULL) in_progress\n      FROM (\n        SELECT user_id, workout_id, session_id, time start_time, null end_time, true in_progress\n        FROM workouts_silver\n        WHERE action = \"start\") a\n      LEFT JOIN (\n        SELECT user_id, workout_id, session_id, null start_time, time end_time, false in_progress\n        FROM workouts_silver\n        WHERE action = \"stop\") b\n      ON a.user_id = b.user_id AND a.session_id = b.session_id\n    )\n\"\"\")\n\n(spark.table(\"TEMP_completed_workouts\").write\n    .mode(\"overwrite\")\n    .saveAsTable(\"completed_workouts\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b60ea570-9f9f-469d-b769-64d41cf82c79"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#workout_bpm\nspark.readStream.table(\"heart_rate_silver\").createOrReplaceTempView(\"TEMP_heart_rate_silver\")\n\nspark.sql(\"\"\"\n  SELECT d.user_id, d.workout_id, d.session_id, time, heartrate\n  FROM TEMP_heart_rate_silver c\n  INNER JOIN (\n    SELECT a.user_id, b.device_id, workout_id, session_id, start_time, end_time\n    FROM completed_workouts a\n    INNER JOIN user_lookup b\n    ON a.user_id = b.user_id) d\n  ON c.device_id = d.device_id AND time BETWEEN start_time AND end_time\n  WHERE c.bpm_check = 'OK'\"\"\").createOrReplaceTempView(\"TEMP_workout_bpm\")\n\n(spark.table(\"TEMP_workout_bpm\")\n    .writeStream\n    .outputMode(\"append\")\n    .option(\"checkpointLocation\", Paths.workoutBpmCheckpoint)\n    .trigger(once=True)\n    .table(\"workout_bpm\")\n    .awaitTermination())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d63d84f7-7148-4f2d-8182-678c8246f09f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# workout_bpm_summary\nspark.readStream.table(\"workout_bpm\").createOrReplaceTempView(\"TEMP_workout_bpm\")\n\n(spark.sql(\"\"\"\n    SELECT workout_id, session_id, a.user_id, age, gender, city, state, min_bpm, avg_bpm, max_bpm, num_recordings\n    FROM user_bins a\n    INNER JOIN\n      (SELECT user_id, workout_id, session_id, MIN(heartrate) min_bpm, MEAN(heartrate) avg_bpm, MAX(heartrate) max_bpm, COUNT(heartrate) num_recordings\n      FROM TEMP_workout_bpm\n      GROUP BY user_id, workout_id, session_id) b\n    ON a.user_id = b.user_id\"\"\"\n    ).writeStream\n        .option(\"path\", Paths.workoutBpmSummary)\n        .option(\"checkpointLocation\", Paths.workoutBpmSummaryCheckpoint)\n        .outputMode(\"complete\")\n        .trigger(once=True)\n        .table(\"workout_bpm_summary\")\n    ).awaitTermination()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"99c136ae-0e41-432d-bf95-b098dd0d8c48"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2021 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2ace1fa5-f127-4829-b660-61578a72621d"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ADES 4.03 - Schedule Batch Jobs","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2598055170582912}},"nbformat":4,"nbformat_minor":0}
