{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"66ac85a7-5412-4c34-8abd-ef25db5cbe07"}}},{"cell_type":"markdown","source":["# Streaming from Multiplex Bronze\n\nIn this notebook, you will configure a query to consume and parse raw data from a single topic as it lands in the multiplex bronze table configured in the last lesson. We'll continue refining this query in the following notebooks.\n\n<img src=\"https://files.training.databricks.com/images/ade/ADE_arch_heartrate_silver.png\" width=\"60%\" />\n\n## Learning Objectives\nBy the end of this lesson, you should be able to:\n- Describe how filters are applied to streaming jobs\n- Use built-in functions to flatten nested JSON data\n- Parse and save binary-encoded strings to native types"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7503e659-0697-4a7d-a9f2-fc87aaddc46d"}}},{"cell_type":"markdown","source":["Declare database and set all path variables."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0c45999f-94e7-48da-b18c-2ddd085ec138"}}},{"cell_type":"code","source":["%run ../Includes/silver-setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3ebc5f7f-b14a-4c83-8a17-893fa9d80e7a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Use the following cell to reset the target directories, if necessary."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b397ef27-9de0-4a78-899e-1bddd5b6ad08"}}},{"cell_type":"code","source":["spark.sql(\"DROP TABLE IF EXISTS heart_rate_silver\")\ndbutils.fs.rm(Paths.silverRecordingsTable, True)\ndbutils.fs.rm(Paths.silverRecordingsCheckpoint, True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f4adae40-86fe-432a-9337-a1a19021e688"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Define a Batch Read\n\nBefore building our streams, we'll start with a static view of our data. Working with static data can be easier during interactive development as no streams will be triggered. Because we're working with Delta Lake as our source, we'll still get the most up-to-date version of our table each time we execute a query.\n\nIf you're working with SQL, you can just directly query the table registered in the previous lesson `bronze`. Python and Scala users can easily create a Dataframe from a registered table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f233fce-8553-4bf4-a389-a39bf68c90f7"}}},{"cell_type":"code","source":["batchDF = spark.table(\"bronze\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"86209c26-e54f-43c7-a3a6-5c92bc31ba34"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Delta Lake stores our schema information. Let's print it out, just to make sure we remember."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"44cf3d21-7023-4c69-b0fe-b5742b317623"}}},{"cell_type":"code","source":["%sql\n\nDESCRIBE bronze"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b9c24a8f-8031-44de-b6e0-2a644cc361ee"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Preview your data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24b599b1-a786-4bc3-82ba-c93ce8c74fbc"}}},{"cell_type":"code","source":["%sql\n\nSELECT *\nFROM bronze\nLIMIT 20"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a5a211d4-8b74-4622-89fe-151747740d3c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["There are multiple topics being ingested. So, we'll need to define logic for each of these topics separately."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f19f1f1b-ef76-4632-a217-457947ce6044"}}},{"cell_type":"code","source":["%sql\n\nSELECT DISTINCT(topic)\nFROM bronze"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6393c0c6-c3ec-4804-b5f6-05b0cef0b851"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We'll cast our binary fields as strings, as this will allow us to manually review their contents."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3044e5ac-eea4-438f-87d1-db2afa65decd"}}},{"cell_type":"code","source":["%sql\n\nSELECT cast(key AS STRING), cast(value AS STRING)\nFROM bronze\nLIMIT 20"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"53911a73-ed9f-400a-8a03-305b5a26e7aa"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Parse Heart Rate Recordings\n\nLet's start by defining logic to parse our heart rate recordings. We'll write this logic against our static data. Note that there are some [unsupported operations](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#unsupported-operations) in Structured Streaming, so we may need to refactor some of our logic if we don't build our current queries with these limitations in mind.\n\nTogether, we'll iteratively develop a single query that parses our `bpm` topic to the following schema.\n\n| field | type |\n| --- | --- |\n| device_id | LONG | \n| time | TIMESTAMP | \n| heartrate | DOUBLE |\n\nWe'll be creating the table `heartrate_silver` in our architectural diagram.\n\n<img src=\"https://files.training.databricks.com/images/ade/ADE_arch_heartrate_silver.png\" width=\"60%\" />"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3fc135e1-5caf-4d52-8b68-c476aa07ba38"}}},{"cell_type":"code","source":["%sql\nMAGIC -- TODO\nMAGIC \nMAGIC -- Use this cell to explore and build your query"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1370ca68-d24b-4a1d-b5d3-c477708c9ba6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Convert Logic for Streaming Read\n\nWe can define a streaming read directly against our Delta table. Note that most configuration for streaming queries is done on write rather than read, so here we see little change to our above logic.\n\nThe cell below shows how to convert a static table into a streaming temp view (if you wish to write streaming queries with Spark SQL)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c7a545fb-ec2d-4e70-8f8f-0309a3b7699a"}}},{"cell_type":"code","source":["(spark.readStream\n  .table(\"bronze\")\n  .createOrReplaceTempView(\"TEMP_bronze\")\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"61e6fe11-eb69-4906-a2ac-e808ef9849a8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Updating our above query to refer to this temp view gives us a streaming result."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e9f90841-d0f9-45bf-ad90-60c830d00cc1"}}},{"cell_type":"code","source":["%sql\n\nSELECT\n v.*\nFROM\n (\n   SELECT\n     from_json(\n       cast(value AS STRING),\n       \"device_id LONG, time TIMESTAMP, heartrate DOUBLE\"\n     ) v\n   FROM\n     TEMP_bronze\n   WHERE\n     topic = \"bpm\"\n )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce9f7bfd-620a-46be-ba40-c1e354f1ec20"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The cell below has this logic refactored to Python."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7380bca1-a4b2-4653-8c64-19718185b15a"}}},{"cell_type":"code","source":["bpmDF = (spark.readStream\n  .table(\"bronze\")\n  .filter(\"topic = 'bpm'\")\n  .select(F.from_json(F.col(\"value\").cast(\"string\"), \"device_id LONG, time TIMESTAMP, heartrate DOUBLE\").alias(\"v\"))\n  .select(\"v.*\")\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"17a8cdb5-e080-4532-9897-5028f818570d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note that anytime a streaming read is displayed to a notebook, a streaming job will begin. To persist results to disk, a streaming write will need to be performed.\n\nUsing the `trigger(once=True)` option will process all records as a single batch."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"31fa2490-b70d-4f27-b84b-d9d20b8098e3"}}},{"cell_type":"code","source":["(bpmDF.writeStream\n    .option(\"checkpointLocation\", Paths.silverRecordingsCheckpoint)\n    .option(\"path\", Paths.silverRecordingsTable)\n    .trigger(once=True)\n    .table(\"heart_rate_silver\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a9193b72-712e-4730-ac59-92b0913dd635"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["<img src=\"https://files.training.databricks.com/images/icon_warn_24.png\"/> Before continuing, make sure you cancel any streams. The `Run All` button at the top of the screen will say `Stop Execution` if you have a stream still running."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e4778376-3446-4721-aa46-910c362010bb"}}},{"cell_type":"markdown","source":["## Silver Table Motivations\n\nIn addtion to parsing records and flattening and changing our schema, we should also check the quality of our data before writing to our silver tables.\n\nIn the following notebooks, we'll review various quality checks."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e01f647b-fa6a-4f46-8567-915b4922449b"}}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2021 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9de010b5-49b8-427d-a33e-71f157d2dc15"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ADES 2.03 - Streaming From Multiplex Bronze","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2598055170583397}},"nbformat":4,"nbformat_minor":0}
