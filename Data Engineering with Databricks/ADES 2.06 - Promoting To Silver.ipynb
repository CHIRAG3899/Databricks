{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"03320136-c4f6-4d7d-bfd6-6a72f1f14bb3"}}},{"cell_type":"markdown","source":["## Promoting to Silver\n\nHere we'll pull together the concepts of streaming from Delta Tables, deduplication, and quality enforcement to finalize our approach to our silver table.\n\n<img src=\"https://files.training.databricks.com/images/ade/ADE_arch_heartrate_silver.png\" width=\"60%\" />\n\n## Learning Objectives\nBy the end of this lesson, students will be able to:\n- Apply table constraints to Delta Lake tables\n- Use flagging to identify records failing to meet certain conditions\n- Apply de-duplication within an incremental microbatch\n- Use `MERGE` to avoid inserting duplicate records to a Delta Lake table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7bd4d32a-fe6a-435b-9b79-4532248d678a"}}},{"cell_type":"code","source":["%run ../Includes/silver-setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a582347-0b6e-4611-99d3-f1a3126dc884"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Begin by resetting your table and checkpoint to make sure there are no conflicts from previous writes."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"22d7d003-1f4b-4bda-a581-a7b646f10934"}}},{"cell_type":"code","source":["spark.sql(\"DROP TABLE IF EXISTS heart_rate_silver\")\n\ndbutils.fs.rm(Paths.silverRecordingsTable, True)\ndbutils.fs.rm(Paths.silverRecordingsCheckpoint, True)\n\nspark.sql(f\"\"\"\nCREATE TABLE IF NOT EXISTS heart_rate_silver\n(device_id LONG, time TIMESTAMP, heartrate DOUBLE, bpm_check STRING)\nUSING DELTA\nLOCATION '{Paths.silverRecordingsTable}'\n\"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c584f9b5-e68b-45ba-93ac-8623f0edd461"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Add a table constraint before inserting data. Name this constraint `dateWithinRange` and make sure that the time is greater than January 1, 2017."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7a8e07be-2758-4892-a80e-59cb87abeef1"}}},{"cell_type":"code","source":["%sql\nMAGIC -- TODO\nMAGIC -- ALTER TABLE -- <FILL-IN>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1bf10216-e616-4cce-b881-3cd6642c6048"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note that adding and removing constraints is recorded in the transaction log."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"499bef1e-8b9b-492b-bc9d-fa704272797d"}}},{"cell_type":"code","source":["%sql\nDESCRIBE HISTORY heart_rate_silver"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"938fce0d-73d9-4d4d-a27e-e89399ad4683"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Use the cell below to create a streaming read that includes:\n1. A filter for the topic `bpm`\n2. Logic to flatten the JSON payload and cast data to the appropriate schema\n3. A `bpm_check` column to flag negative records\n4. A duplicate check on `device_id` and `time` with a 30 second watermark on `time`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"23f9ba37-0176-441c-a3ee-028a0c09f5b5"}}},{"cell_type":"code","source":["# TODO\nstreamingDF = (\n# <FILL_IN>\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"559b5dda-2a24-4828-a65b-c6fa638608a1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Below, the upsert class used in the previous notebooks is provided."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9e7a199b-1c56-482f-ae75-b40c4af59f28"}}},{"cell_type":"code","source":["class Upsert:\n    def __init__(self, query, update_temp=\"stream_updates\"):\n        self.query = query\n        self.update_temp = update_temp \n        \n    def upsertToDelta(self, microBatchDF, batch):\n        microBatchDF.createOrReplaceTempView(self.update_temp)\n        microBatchDF._jdf.sparkSession().sql(self.query)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7032f7be-9d64-4ad4-8e10-99bc48846f4a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Use the cell below to define the upsert query to instantiate our class. Alternately, [consult the documentation](https://docs.databricks.com/delta/delta-update.html#upsert-into-a-table-using-merge&language-python) and try implementing this using the `DeltaTable` Python class."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"682b7275-2349-41db-857f-db4e37b93a1e"}}},{"cell_type":"code","source":["# TODO\nquery = \"\"\"<FILL-IN>\"\"\"\n \nstreamingMerge=Upsert(query)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d14ea3d0-c975-4737-b39f-5b3e6a17baa0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now execute a write with trigger once logic to process all existing data from the bronze table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"246204bb-2fa1-4a50-b52d-1a1168df5528"}}},{"cell_type":"code","source":["def process_silver_heartrate():\n    (streamingDF.writeStream\n       .foreachBatch(streamingMerge.upsertToDelta)\n       .outputMode(\"update\")\n       .option(\"checkpointLocation\", Paths.silverRecordingsCheckpoint)\n       .trigger(once=True)\n       .start()\n       .awaitTermination())\n\nprocess_silver_heartrate()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e786cf52-7839-4ccd-9b5e-b3d766a77222"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We should see the same number of total records in our silver table as previously, but a small percentage of these will correctly be flagged with \"Negative BPM\"."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c7dce3a9-6548-42a1-b9e7-2b37262a6bd7"}}},{"cell_type":"code","source":["%sql\nSELECT COUNT(*)\nFROM heart_rate_silver"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c976c374-31ce-4f86-a162-7ffe357b9293"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT COUNT(*)\nFROM heart_rate_silver\nWHERE bpm_check = \"Negative BPM\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"90cb4f01-422f-452a-ba15-3a021e743fd5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now land a new batch of data and propagate changes through bronze into the silver table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"81ef211b-6818-495c-9e77-3f0ecdf3d4f6"}}},{"cell_type":"code","source":["new_batch()\nprocess_silver_heartrate()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6d25ba49-cbfa-4ea3-94b6-6f1520685afe"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT COUNT(*)\nFROM heart_rate_silver"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d1ff4043-2513-45c9-bffa-0b067b5d715b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2021 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb8f0acd-f281-47d4-a6aa-5f0a8d5165f9"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ADES 2.06 - Promoting To Silver","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2598055170583327}},"nbformat":4,"nbformat_minor":0}
