{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a0855e09-1ffa-429a-ab75-e4dab25aaa50"}}},{"cell_type":"markdown","source":["# Scheduling Efficient Structured Streaming Jobs\n\nWe'll use this notebook as a framework to launch multiple streams on shared resources.\n\nThis notebook contains partially refactored code with all the updates and additions that will allow us to schedule our pipelines and run them as new data arrives, including logic for dealing with partition deletes from our `bronze` table.\n\nAlso included is logic to assign each stream to a scheduler pool. Review the code below and then follow the instructions in the following cell to schedule a streaming job.\n\n<img src=\"https://files.training.databricks.com/images/ade/ADE_arch_bronze.png\" width=\"60%\" />"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"54b8feea-fa74-4625-bf06-0b261ebc8e29"}}},{"cell_type":"markdown","source":["## Scheduling this Notebook\n\nThis notebook is designed to be scheduled against a jobs cluster.\n\n### Create a New Job\n0. Click the Jobs button on the left sidebar\n0. Click the blue `+ Create Job` button\n0. Name the job something unique but parseable, such as `ade-stream-<your_initials>`\n0. Next to **Task**, click \"Select Notebook\" and use the file picker to select this notebook; click OK.\n0. Next to **Cluster**, click \"Edit\".\n0. Change the following settings **only** (and then click \"Confirm\"):\n  - **Workers**: 2\n  - Under **Advanced Options** in the **Spark Config**, set: `sql.shuffle.partitions 8`\n0. Click \"Run Now\" to start your job.\n\n![sql-shuffle](https://files.training.databricks.com/images/enb/med_data/sql-shuffle.png)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2fec9885-67ba-484e-aab4-02ae8fc20663"}}},{"cell_type":"markdown","source":["## Widgets\n\nThe `widgets` submodule includes a number of methods to allow interactive variables to be set while working with notebooks in the workspace with an interactive cluster. To learn more about this functionality, refer to the [Databricks documentation](https://docs.databricks.com/notebooks/widgets.html#widgets).\n\nThis notebook will focus on only two of these methods, emphasizing their utility when running a notebook as a job:\n1. `dbutils.widgets.text` accepts a parameter name and a default value. This is the method through which external values can be passed into scheduled notebooks.\n1. `dbutils.widgets.get` accepts a parameter name and retrieves the associated value from the widget with that parameter name.\n\nTaken together, `dbutils.widgets.text` allows the passing of external values and `dbutils.widgets.get` allows those values to be referenced.\n\n**NOTE**: To run this notebook in streaming mode, pass the value `False` to the `once` widget, or add this as a parameter to your scheduled job."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"68617483-078b-4f15-8286-477c6d0d3f99"}}},{"cell_type":"code","source":["dbutils.widgets.text(\"once\", \"True\")\nonce = eval(dbutils.widgets.get(\"once\"))\nprint(f\"Once: {once}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2722de54-8df4-4fe4-aba0-43bc8174fc92"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Configure Apache Spark Scheduler Pools for Efficiency\n\nBy default, all queries started in a notebook run in the same <a href=\"https://spark.apache.org/docs/latest/job-scheduling.html#scheduling-within-an-application\" target=\"_blank\">fair scheduling pool</a>. Therefore, jobs generated by triggers from all of the streaming queries in a notebook run one after another in first in, first out (FIFO) order. This can cause unnecessary delays in the queries, because they are not efficiently sharing the cluster resources.\n\nIn particular, resource-intensive streams can hog the available compute in a cluster, preventing smaller streams from achieving low latency. Configuring pools provides the capacity to fine tune your cluster to ensure processing time.\n\nTo enable all streaming queries to execute jobs concurrently and to share the cluster efficiently, you can set the queries to execute in separate scheduler pools. This **local property configuration** will be in the same notebook cell where we start the streaming query. For example:\n\n**Run streaming query1 in scheduler pool1**\n```\nspark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"pool1\")\ndf.writeStream.queryName(\"query1\").format(\"parquet\").start(path1)\n```\n**Run streaming query2 in scheduler pool2**\n\n```\nspark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"pool2\")\ndf.writeStream.queryName(\"query2\").format(\"delta\").start(path2)\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0321cf71-8633-4b8a-808d-b56c8e95299b"}}},{"cell_type":"code","source":["%run ../Includes/ade-setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6879896d-6fa1-4984-99b7-e622496b8f2c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Auto Optimize and Auto Compaction\n\nWe'll want to ensure that our bronze table and 3 parsed silver tables don't contain too many small files. Turning on Auto Optimize and Auto Compaction help us to avoid this problem. For more information on these settings, <a href=\"https://docs.databricks.com/delta/optimizations/auto-optimize.html\" target=\"_blank\">consult our documentation</a>."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"47b6b8a1-5e95-466b-ba62-7b1a9f952b05"}}},{"cell_type":"code","source":["spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", True)\nspark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e1e14836-9fa2-4c8c-a580-ed453818cfbd"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Bronze"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d901dd9c-cd55-461e-ac61-81216a51a59d"}}},{"cell_type":"code","source":["dateLookup = spark.table(\"date_lookup\").select(\"date\", \"week_part\")\ndateLookup.cache().count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"281073ff-2f60-4944-9de5-84963847c58d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def process_bronze(source, table_name, checkpoint, once=False, processing_time=\"5 seconds\"):\n    schema = \"key BINARY, value BINARY, topic STRING, partition LONG, offset LONG, timestamp LONG\"\n    \n    data_stream_writer = (spark.readStream\n            .format(\"cloudFiles\")\n            .schema(schema)\n            .option(\"maxFilesPerTrigger\", 2)\n            .option(\"cloudFiles.format\", \"json\")\n            .load(source)\n            .join(F.broadcast(dateLookup), [F.to_date((F.col(\"timestamp\")/1000).cast(\"timestamp\")) == F.col(\"date\")], \"left\")\n            .writeStream\n            .option(\"checkpointLocation\", checkpoint)\n            .partitionBy(\"topic\", \"week_part\")\n            .queryName(\"bronze\")\n         )\n    \n    if once == True:\n        (data_stream_writer\n            .trigger(once=True)\n            .table(table_name)\n            .awaitTermination(60)\n        )\n    else:\n        (data_stream_writer\n            .trigger(processingTime=processing_time)\n            .table(table_name)\n        )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a172c061-3bdb-49ea-8815-633413cdf3aa"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"bronze\")\nprocess_bronze(Paths.source30m, \"bronze_dev\", Paths.bronzeCheckpoint, once=once)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"45b7182f-2b70-4920-80cb-50fb73b1fbc5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Parse Silver Tables\n\nIn the next cell, we define a Python class to handle the queries that result in our `heart_rate_silver` and `workouts_silver`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"621cb6d0-da67-49ed-b3a1-3c44f553b7df"}}},{"cell_type":"code","source":["class Upsert:\n    def __init__(self, query, update_temp=\"stream_updates\"):\n        self.query = query\n        self.update_temp = update_temp \n        \n    def upsertToDelta(self, microBatchDF, batch):\n        microBatchDF.createOrReplaceTempView(self.update_temp)\n        microBatchDF._jdf.sparkSession().sql(self.query)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc99678b-f0bf-43b3-9b10-04dfb97123b7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# heart_rate_silver\ndef heart_rate_silver(source_table=\"bronze\", once=False, processing_time=\"10 seconds\"):\n    \n    query = \"\"\"\n        MERGE INTO heart_rate_silver a\n        USING heart_rate_updates b\n        ON a.device_id=b.device_id AND a.time=b.time\n        WHEN NOT MATCHED THEN INSERT *\n        \"\"\"\n\n    streamingMerge=Upsert(query, \"heart_rate_updates\")\n    \n    data_stream_writer = (spark.readStream\n        .option(\"ignoreDeletes\", True)\n        .table(source_table)\n        .filter(\"topic = 'bpm'\")\n        .select(F.from_json(F.col(\"value\").cast(\"string\"), \"device_id LONG, time TIMESTAMP, heartrate DOUBLE\").alias(\"v\"))\n        .select(\"v.*\", F.when(F.col(\"v.heartrate\") <= 0, \"Negative BPM\").otherwise(\"OK\").alias(\"bpm_check\"))\n        .withWatermark(\"time\", \"30 seconds\")\n        .dropDuplicates([\"device_id\", \"time\"])\n        .writeStream\n        .foreachBatch(streamingMerge.upsertToDelta)\n        .outputMode(\"update\")\n        .option(\"checkpointLocation\", Paths.silverRecordingsCheckpoint)\n        .queryName(\"heart_rate_silver\")\n    )\n  \n    if once == True:\n        (data_stream_writer\n            .trigger(once=True)\n            .start()\n            .awaitTermination(60)\n        )\n    else:\n        (data_stream_writer\n            .trigger(processingTime=processing_time)\n            .start()\n        )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cd1405bb-65f1-41af-9e6f-013185799718"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# workouts_silver\ndef workouts_silver(source_table=\"bronze\", once=False, processing_time=\"15 seconds\"):\n    \n    query = \"\"\"\n        MERGE INTO workouts_silver a\n        USING workout_updates b\n        ON a.user_id=b.user_id AND a.time=b.time\n        WHEN NOT MATCHED THEN INSERT *\n        \"\"\"\n\n    streamingMerge=Upsert(query, \"workout_updates\")\n    \n    data_stream_writer = (spark.readStream\n        .option(\"ignoreDeletes\", True)\n        .table(source_table)\n        .filter(\"topic = 'workout'\")\n        .select(F.from_json(F.col(\"value\").cast(\"string\"), \"user_id INT, workout_id INT, timestamp FLOAT, action STRING, session_id INT\").alias(\"v\"))\n        .select(\"v.*\")\n        .select(\"user_id\", \"workout_id\", F.col(\"timestamp\").cast(\"timestamp\").alias(\"time\"), \"action\", \"session_id\")\n        .withWatermark(\"time\", \"30 seconds\")\n        .dropDuplicates([\"user_id\", \"time\"])\n        .writeStream\n        .foreachBatch(streamingMerge.upsertToDelta)\n        .outputMode(\"update\")\n        .option(\"checkpointLocation\", Paths.silverWorkoutsCheckpoint)\n        .queryName(\"workouts_silver\")\n\n    )\n\n    if once == True:\n        (data_stream_writer\n            .trigger(once=True)\n            .start()\n            .awaitTermination(60)\n        )\n    else:\n        (data_stream_writer\n            .trigger(processingTime=processing_time)\n            .start()\n        )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e4f77c72-460d-4927-a4c3-cdb0147f7104"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# users\nfrom pyspark.sql.window import Window\n\nwindow = Window.partitionBy(\"alt_id\").orderBy(F.col(\"updated\").desc())\n\ndef batch_rank_upsert(microBatchDF, batchId):\n    appId = \"batch_rank_upsert\"\n    \n    (microBatchDF\n        .filter(F.col(\"update_type\").isin([\"new\", \"update\"]))\n        .withColumn(\"rank\", F.rank().over(window)).filter(\"rank == 1\").drop(\"rank\")\n        .createOrReplaceTempView(\"ranked_updates\"))\n    \n    microBatchDF._jdf.sparkSession().sql(\"\"\"\n        MERGE INTO users u\n        USING ranked_updates r\n        ON u.alt_id=r.alt_id\n        WHEN MATCHED AND u.updated < r.updated\n          THEN UPDATE SET *\n        WHEN NOT MATCHED\n          THEN INSERT *\n    \"\"\")\n\n    (microBatchDF\n         .filter(\"update_type = 'delete'\")\n         .select(\n            \"alt_id\", \n            F.col(\"updated\").alias(\"requested\"), \n            F.date_add(\"updated\", 30).alias(\"deadline\"), \n            F.lit(\"requested\").alias(\"status\"))\n        .write\n        .format(\"delta\")\n        .mode(\"append\")\n        .option(\"txnVersion\", batchId)\n        .option(\"txnAppId\", appId)\n        .option(\"path\", Paths.deleteRequests)\n        .saveAsTable(\"delete_requests\"))\n\n\ndef users_silver(source_table=\"bronze\", once=False, processing_time=\"30 seconds\"):\n\n    schema = \"\"\"\n        user_id LONG, \n        update_type STRING, \n        timestamp FLOAT, \n        dob STRING, \n        sex STRING, \n        gender STRING, \n        first_name STRING, \n        last_name STRING, \n        address STRUCT<\n            street_address: STRING, \n            city: STRING, \n            state: STRING, \n            zip: INT\n        >\"\"\"\n\n    salt = \"BEANS\"\n\n    data_stream_writer = (spark.readStream\n        .option(\"ignoreDeletes\", True)\n        .table(source_table)\n        .filter(\"topic = 'user_info'\")\n        .dropDuplicates()\n        .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\")).select(\"v.*\")\n        .select(F.sha2(F.concat(F.col(\"user_id\"), F.lit(salt)), 256).alias(\"alt_id\"),\n            F.col('timestamp').cast(\"timestamp\").alias(\"updated\"),\n            F.to_date('dob','MM/dd/yyyy').alias('dob'),\n            'sex', 'gender','first_name','last_name',\n            'address.*', \"update_type\")\n        .writeStream\n        .foreachBatch(batch_rank_upsert)\n        .outputMode(\"update\")\n        .option(\"checkpointLocation\", Paths.usersCheckpointPath)\n        .queryName(\"users\")\n    )\n    \n    if once == True:\n        (data_stream_writer\n            .trigger(once=True)\n            .start()\n            .awaitTermination(60)\n        )\n    else:\n        (data_stream_writer\n            .trigger(processingTime=processing_time)\n            .start()\n        )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3b6529a9-2acf-4150-ac06-95979c3142d4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"silver_parsed\")\nheart_rate_silver(source_table=\"bronze_dev\", once=once)\nworkouts_silver(source_table=\"bronze_dev\", once=once)\nusers_silver(source_table=\"bronze_dev\", once=once)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f4c47aee-6605-4532-be67-d789f79d85a4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2021 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a6ea16e0-97c3-4acd-8393-21206b04fc92"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ADES 4.02 - Schedule - Streaming Jobs","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2598055170582921}},"nbformat":4,"nbformat_minor":0}
