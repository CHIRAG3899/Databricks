{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"881cb51b-3390-49e0-9304-1d77c995667b"}}},{"cell_type":"markdown","source":["# Deleting at Partition Boundaries\n\nWhile we've deleted our PII from our silver tables, we haven't dealt with the fact that this data still exists in our `bronze` table.\n\nNote that because of stream composability and the design choice to use a multiplex bronze pattern, enabling Delta Change Data Feed (CDF) to propagate delete information would require redesigning each of our pipelines to take advantage of this output. Without using CDF, modification of data in a table will break downstream composability.\n\n<img src=\"https://files.training.databricks.com/images/ade/ADE_arch_bronze.png\" width=\"60%\" />\n\nIn this notebook, you'll learn how to delete partitions of data from Delta Tables and how to configure incremental reads to allow for these deletes.\n\nThis functionality is not only useful for permanently deleting PII, but this same pattern can be applied in companies that just want to expunge data older than a certain age from a given table. Similarly, data could be backed up to a cheaper storage tier, and then safely deleted from \"active\" or \"hot\" Delta tables to drive savings on cloud storage.\n\n## Learning Objectives\nBy the end of this notebook, students will be able to:\n- Delete data using partition boundaries\n- Configure downstream incremental reads to safely ignore these deletions\n- Use `VACUUM` to review files to be deleted and commit deletes\n- Union archived data with production tables to recreate a full historic dataset"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"faa7a4e4-878d-4136-9405-9d4943176c0f"}}},{"cell_type":"markdown","source":["Begin by running our setup script."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2457af0c-8b78-41bd-8895-a375811301fa"}}},{"cell_type":"code","source":["%run ../Includes/ade-setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"05029e10-9518-4106-8f4e-91ef6250c167"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Our Delta table is partitioned by two fields. Our top level partition is the `topic` column. Run the cell to note the 3 partition directories (and the Delta Log directory) that collectively comprise our `bronze` table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d5f0a06a-2b06-4a9f-9fcb-f4993a70a67a"}}},{"cell_type":"code","source":["dbutils.fs.ls(Paths.bronzeTable)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"00151d0a-2060-4de4-b87d-9e570248ed49"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Our 2nd level partition was on our `week_part` column, which we derived as the year and week of year. There are around 20 directories currently present at this level."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3430ec9f-a2b3-4ea2-93e6-02e10a7c36a8"}}},{"cell_type":"code","source":["dbutils.fs.ls(Paths.bronzeTable + \"/topic=user_info\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1222b710-4c9c-4382-bc58-817e767e1d55"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note that in our current dataset, we're tracking only a small number of total users in these files."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7ce12a78-c6a0-4c1c-9648-e797ec3fea5e"}}},{"cell_type":"code","source":["spark.table(\"bronze\").filter(\"topic='user_info'\").filter(\"week_part<='2019-48'\").count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"16568e52-151b-477d-a139-85c1adabd1c1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Deleting at a Partition Boundary\nHere we'll model deleting all `user_info` that was received before week 49 of 2019.\n\nNote that we are deleting cleanly along partition boundaries. All the data contained in the specified `week_part` directories will be removed from our table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ae1e23e8-79d6-48ea-a741-f63ec7006cfd"}}},{"cell_type":"code","source":["%sql\nDELETE FROM bronze \nWHERE topic = 'user_info'\nAND week_part <= '2019-48'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5d6383bf-133a-44ea-b3c5-6f6be3c3c099"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We can confirm this delete processed successfully by looking at the history. The `operationMetrics` column will indicate the number of removed files."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1132f8d7-aeb8-4023-819c-8b63110d1a84"}}},{"cell_type":"code","source":["%sql\nDESCRIBE HISTORY bronze"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bba0cfff-3508-4397-98f1-c9b17742b178"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["When deleting along partition boundaries, we don't write out new data files; recording the files as removed in the Delta log is sufficient. However, file deletion will not actually occur until we VACUUM our table. Note that all of our week partitions still exist in our `user_info` directory and that data files still exist in each week directory."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bb595501-f800-4830-8efd-8f15c1864c4e"}}},{"cell_type":"code","source":["dbutils.fs.ls(Paths.bronzeTable + \"/topic=user_info/week_part=2019-48\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c8c07b6-ffaf-4bfb-a6b1-ca9b941f2e71"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Archiving Data\nIf a company wishes to maintain an archive of historic records (but only maintain recent records in production tables), cloud-native settings for auto-archiving data can be configured to move data files automatically to lower-cost storage locations.\n\nThe cell below simulates this process (here using copy instead of move). Note that because only the data files and partition directories are being relocated, the resultant table will be Parquet by default.\n\n**NOTE**: The code below is copying the same data files that we just deleted in the previous step. This is meant to demonstrate the fact that until commits are deleted with a `VACUUM` call, records should not be considered deleted."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c52ed4c3-9f4f-460e-bf77-f796b4daa8ca"}}},{"cell_type":"code","source":["spark.sql(\"DROP TABLE IF EXISTS user_info_archived\")\ndbutils.fs.rm(Paths.basePath + \"/pii_archive\", True)\n\n[dbutils.fs.cp(x[0], Paths.basePath + \"/pii_archive/\" + x[1], True) for x in dbutils.fs.ls(Paths.bronzeTable + \"/topic=user_info\") if x[1][-8:-1] <= '2019-48'];\n\nspark.sql(f\"\"\"\nCREATE TABLE IF NOT EXISTS user_info_archived\nUSING parquet\nLOCATION '{Paths.basePath + \"/pii_archive/\"}'\n\"\"\")\n\nspark.sql(\"MSCK REPAIR TABLE user_info_archived\")\n\ndisplay(spark.sql(\"SELECT COUNT(*) FROM user_info_archived\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ead652f0-2ea5-4bd6-8fae-5d1f9cbfa4ca"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note that the directory structure was maintained as files were copied."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ea020a6e-0f9f-440b-97f0-7e117fd4a4da"}}},{"cell_type":"code","source":["dbutils.fs.ls(Paths.basePath + \"/pii_archive/\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ea47e08c-944d-4fe3-ae7d-46492177f859"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Reviewing and Committing Deletes\n\nBy default, the Delta engine will prevent `VACUUM` operations with less than 7 days of retention. The cell below overrides this check."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9aba3a07-548c-4bd4-a8e5-22eaafe2caf5"}}},{"cell_type":"code","source":["spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"32f0968a-0270-4f3b-8aa0-f2aa1d6bef50"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Adding the `DRY RUN` keyword to the end of our `VACUUM` statement allows us to preview files to be deleted before they are permanently removed. \n\nNote that at this moment we could still recover our deleted data by running:\n\n```\nRESTORE bronze\nTO VERSION AS OF <version>\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"de60e4d7-1edc-49ee-b642-deaa6cb38cf3"}}},{"cell_type":"code","source":["%sql\nVACUUM bronze RETAIN 0 HOURS DRY RUN"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c7b21389-a6d7-463e-ae0f-fad5440c3b13"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Executing the `VACUUM` command below permanently deletes these files."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11957541-cef6-43ea-b463-489dc3db2961"}}},{"cell_type":"code","source":["%sql\nVACUUM bronze RETAIN 0 HOURS"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24a4d9e4-ab6d-4bef-9ab1-5e4785a290fb"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["For safety, it's best to always re-enable our `retentionDurationCheck`. In production, you should avoid overriding this check whenever possible (if other operations are acting against files not yet committed to a Delta table and written before the retention threshold, `VACUUM` can result in data corruption)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"410f1e74-1d6d-4641-828f-57561080d442"}}},{"cell_type":"code","source":["spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7b53fcab-9ed5-479d-b2b5-94f03e450a5d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note that empty directories will eventually be cleaned up with `VACUUM`, but may not always be deleted as they are emptied of data files. \n\nThe cell below attempts to list the directory for week 48 of 2019 for the `user_info` topic. If it fails, it will list all the week partition directories left in this topic. \n\nEither of these list operations will demonstrate that we have successfully committed the deletes against our tombstoned files."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be6eb308-abb9-470f-ae64-a70460d95a99"}}},{"cell_type":"code","source":["try:\n    print(dbutils.fs.ls(Paths.bronzeTable + \"/topic=user_info/week_part=2019-48\"))\nexcept:\n    display(dbutils.fs.ls(Paths.bronzeTable + \"/topic=user_info\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"29b1276d-4b6f-404d-ad1a-2d5b57f7ae44"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["As such, querying the `bronze` table with the same filters used in our delete statement should yield 0 records."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"70a90c27-abe7-4194-b2cd-70d5720a4c18"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM bronze WHERE topic='user_info' AND week_part <= '2019-48'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"00dd0d36-a19c-4fec-a1cf-ca90ca53f61e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Recreating Full Table History\n\nNote that because Parquet using directory partitions as columns in the resulting dataset, the data that was backed up no longer has a `topic` field in its schema.\n\nThe logic below addresses this while calling `UNION` on the archived and production datasets to recreate the full history of the `user_info` topic."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"592ea7e6-f559-46db-a166-9a55bc56d248"}}},{"cell_type":"code","source":["%sql\nWITH full_bronze_user_info AS (\n\n  SELECT key, value, partition, offset, timestamp, date, week_part \n  FROM bronze \n  WHERE topic='user_info'\n  \n  UNION \n  \n  SELECT * FROM user_info_archived) \n  \nSELECT COUNT(*) FROM full_bronze_user_info"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"784f5b1c-5a24-453e-8ff8-7503c4830e14"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Updating Streaming Reads to Ignore Changes\n\nThe cell below condenses all the code used to perform streaming updates to our `users` table.\n\nIf you try to execute this code right now, you'll raise an exception\n> Detected deleted data from streaming source\n\nLine 22 of the cell below adds the `.option(\"ignoreDeletes\", True)` to the DataStreamReader. This option is all that is necessary to enable streaming processing from Delta tables with partition deletes."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5aa7c479-ab2d-4d4a-a66e-c01854722f3f"}}},{"cell_type":"code","source":["from pyspark.sql.window import Window\n\nschema = \"\"\"\n    user_id LONG, \n    update_type STRING, \n    timestamp FLOAT, \n    dob STRING, \n    sex STRING, \n    gender STRING, \n    first_name STRING, \n    last_name STRING, \n    address STRUCT<\n        street_address: STRING, \n        city: STRING, \n        state: STRING, \n        zip: INT\n    >\"\"\"\n\nsalt = \"BEANS\"\n\nunpackedDF = (spark.readStream\n    .option(\"ignoreDeletes\", True)     # This is new!\n    .table(\"bronze\")\n    .filter(\"topic = 'user_info'\")\n    .dropDuplicates()\n    .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\")).select(\"v.*\")\n    .select(F.sha2(F.concat(F.col(\"user_id\"), F.lit(salt)), 256).alias(\"alt_id\"),\n        F.col('timestamp').cast(\"timestamp\").alias(\"updated\"),\n        F.to_date('dob','MM/dd/yyyy').alias('dob'),\n        'sex', 'gender','first_name','last_name',\n        'address.*', \"update_type\"))\n\n\nwindow = Window.partitionBy(\"alt_id\").orderBy(F.col(\"updated\").desc())\n\ndef batch_rank_upsert(microBatchDF, batchId):\n    appId = \"batch_rank_upsert\"\n    \n    (microBatchDF\n        .filter(F.col(\"update_type\").isin([\"new\", \"update\"]))\n        .withColumn(\"rank\", F.rank().over(window)).filter(\"rank == 1\").drop(\"rank\")\n        .createOrReplaceTempView(\"ranked_updates\"))\n    \n    microBatchDF._jdf.sparkSession().sql(\"\"\"\n        MERGE INTO users u\n        USING ranked_updates r\n        ON u.alt_id=r.alt_id\n        WHEN MATCHED AND u.updated < r.updated\n          THEN UPDATE SET *\n        WHEN NOT MATCHED\n          THEN INSERT *\n    \"\"\")\n\n    (microBatchDF\n         .filter(\"update_type = 'delete'\")\n         .select(\n            \"alt_id\", \n            F.col(\"updated\").alias(\"requested\"), \n            F.date_add(\"updated\", 30).alias(\"deadline\"), \n            F.lit(\"requested\").alias(\"status\"))\n        .write\n        .format(\"delta\")\n        .mode(\"append\")\n        .option(\"txnVersion\", batchId)\n        .option(\"txnAppId\", appId)\n        .option(\"path\", Paths.deleteRequests)\n        .saveAsTable(\"delete_requests\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b32d9c04-1518-40d3-ad69-00f83504b2bb"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["(unpackedDF.writeStream\n    .foreachBatch(batch_rank_upsert)\n    .outputMode(\"update\")\n    .option(\"checkpointLocation\", Paths.usersCheckpointPath)\n    .trigger(once=True)\n    .start()\n    .awaitTermination())    "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d55a9b3-721f-41b0-9388-5f1e54b31916"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note that we may seen the table version as this code completes."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"934b2e43-0ce6-48c5-bc1f-217cc097be90"}}},{"cell_type":"code","source":["%sql\nDESCRIBE HISTORY users"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c71bbd11-3e97-4069-9de3-9037e80c768e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["However, by examining the Delta log file this version, we'll note that the file written out is just indicating the data change, but that no new records were added or modified."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e25837d-ed2f-4d17-9665-bff14f1dbc8e"}}},{"cell_type":"code","source":["max_version = max([file.name for file in dbutils.fs.ls(Paths.users + \"/_delta_log\") if file.name.endswith(\".json\")])\ndisplay(spark.read.json(Paths.users + f\"/_delta_log/{max_version}\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4133300d-0dc8-47c9-9b83-ef7046f8832c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Next Steps\nWhile we did not modify data in our `workout` or `bpm` partitions, because these read from the same `bronze` table we'll need to also update their DataStreamReader logic to ignore changes."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a217265-4ad3-4a3c-a01e-3f50aecf7d59"}}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2021 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7522c530-4a7c-47cb-9174-91769d466a40"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ADES 3.05 - Delete Partition","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2598055170584606}},"nbformat":4,"nbformat_minor":0}
