{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1efead65-f954-4091-95c9-88a67c556989"}}},{"cell_type":"markdown","source":["# Storing PII Securely\n\nAdding a pseudonymized key to incremental workloads is as simple as adding a transformation.\n\nIn this notebook, we'll examine design patterns for ensuring PII is stored securely and updated accurately. We'll also demonstrate an approach for processing delete requests to make sure these are captured appropriately.\n\n<img src=\"https://files.training.databricks.com/images/ade/ADE_arch_users.png\" width=\"60%\" />\n\n## Learning Objectives\nBy the end of this notebook, students will be able to:\n- Apply incremental transformations to store data with pseudonymized keys\n- Use windowed ranking to identify the most-recent records in a CDC feed\n-"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"978b9557-7af9-4ce6-800c-7775d64f0cbe"}}},{"cell_type":"markdown","source":["Begin by running the following cell to set up relevant databases and paths."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"871b8e3a-5448-40b7-9de2-93c8b014351c"}}},{"cell_type":"code","source":["%run \"../Includes/ade-setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5f20b496-725e-4cd2-a9ec-b4f6d2ffd512"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Execute the following cell to reset your `users` table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a4e3d7a-ea84-4f80-b444-44391d717291"}}},{"cell_type":"code","source":["spark.sql(\"DROP TABLE IF EXISTS users\")\ndbutils.fs.rm(Paths.users, True)\ndbutils.fs.rm(Paths.usersCheckpointPath, True)\n\nspark.sql(\"DROP TABLE IF EXISTS delete_requests\")\ndbutils.fs.rm(Paths.deleteRequests, True)\n\nspark.sql(f\"\"\"\n  CREATE TABLE users\n  (alt_id STRING, dob DATE, sex STRING, gender STRING, first_name STRING, last_name STRING, street_address STRING, city STRING, state STRING, zip INT, updated TIMESTAMP)\n  USING DELTA\n  LOCATION '{Paths.users}'\n\"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aaf718f3-c92f-4832-80db-9265e37c9a21"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## ELT with Pseudonymization\nThe data in the `user_info` topic contains complete row outputs from a Change Data Capture feed.\n\nThere are three values for `update_type` present in the data: `new`, `update`, and `delete`.\n\nThe `users` table will be implemented as a Type 1 table, so only the most recent value matters\n\nRun the cell below to visually confirm that both `new` and `update` records contain all the fields we need for our `users` table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3cb23a43-3048-47d4-bcf4-20f609595d9d"}}},{"cell_type":"code","source":["schema = \"\"\"\n    user_id LONG, \n    update_type STRING, \n    timestamp FLOAT, \n    dob STRING, \n    sex STRING, \n    gender STRING, \n    first_name STRING, \n    last_name STRING, \n    address STRUCT<\n        street_address: STRING, \n        city: STRING, \n        state: STRING, \n        zip: INT\n    >\"\"\"\n\nusersDF = (spark.table(\"bronze\")\n    .filter(\"topic = 'user_info'\")\n    .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\")).select(\"v.*\")\n    .filter(F.col(\"update_type\").isin([\"new\", \"update\"]))\n          )\n\ndisplay(usersDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7e9701cd-a241-4ccb-8e5c-7f98e6693984"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Processing Right to Be Forgotten Requests\n\nWhile it is possible to process deletes at the same time as appends and updates, the fines around right to be forgotten requests may warrant a separate process.\n\nBelow, logic for setting up a simple table to process delete requests through the users data is displayed. A simple deadline of 30 days after the request is inserted, allowing internal automated audits to leverage this table to ensure compliance."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5a350401-b092-48e6-a5e1-93e3f3073b25"}}},{"cell_type":"code","source":["display(spark.table(\"bronze\")\n    .filter(\"topic = 'user_info'\")\n    .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\")).select(\"v.*\", F.col('v.timestamp').cast(\"timestamp\").alias(\"requested\"))\n    .filter(\"update_type = 'delete'\")\n    .select(\"user_id\",\n        \"requested\",\n        F.date_add(\"requested\", 30).alias(\"deadline\"), \n        F.lit(\"requested\").alias(\"status\")\n           )\n   )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb9d8540-9cc2-49c9-9a62-ae1d371bfbfe"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Deduplication with Windowed Ranking\n\nWe've previously explored some ways to remove duplicate records:\n- Using Delta Lake's `MERGE` syntax, we can update or insert records based on keys, matching new records with previously loaded data\n- `dropDuplicates` will remove exact duplicates within a table or incremental microbatch\n\nNow we have multiple records for a given primary key BUT these records are not identical. `dropDuplicates` will not work to remove these records, and we'll get an error from our merge statement if we have the same key present multiple times.\n\nBelow, a third approach for removing duplicates is shown below using the [pySpark Window class](http://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Window.html?highlight=window#pyspark.sql.Window)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ed321e44-518e-4b36-9f0a-e170ec04a15d"}}},{"cell_type":"code","source":["from pyspark.sql.window import Window\n\nwindow = Window.partitionBy(\"user_id\").orderBy(F.col(\"timestamp\").desc())\nrankedDF = usersDF.dropDuplicates().withColumn(\"rank\", F.rank().over(window)).filter(\"rank == 1\").drop(\"rank\")\n\ndisplay(rankedDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"088dad86-8cd3-4e36-a321-4815adb0b5d6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["As desired, we get only the newest (`rank == 1`) entry for each unique `user_id`.\n\nUnfortunately, if we try to apply this to a streaming read of our data, we'll learn that\n> Non-time-based windows are not supported on streaming DataFrames"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"97be11cd-d2fe-4bce-8401-53b832824a97"}}},{"cell_type":"code","source":["streamingRankedDF = (spark.readStream.table(\"bronze\")\n    .filter(\"topic = 'user_info'\")\n    .dropDuplicates()\n    .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\")).select(\"v.*\")\n    .filter(F.col(\"update_type\").isin([\"new\", \"update\"]))\n    .withColumn(\"rank\", F.rank().over(window)).filter(\"rank == 1\").drop(\"rank\")\n                    )\n  \ntry:\n    display(streamingRankedDF)\n    raise Exception(\"Expected failure.\")\n\nexcept pyspark.sql.utils.AnalysisException as e:\n    print(\"Failed as expected...\")\n    print(e)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2c7af03b-6aa1-478b-9eb8-71a6a7569c6e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Luckily we have a workaround to avoid this restriction."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4a33edef-67c8-48c2-86ee-d541db40d03c"}}},{"cell_type":"markdown","source":["## Implementing Streaming Ranked De-duplication\n\nAs we saw previously, when apply `MERGE` logic with a Structured Streaming job, we need to use `foreachBatch` logic.\n\nRecall that while we're inside a streaming microbatch, we interact with our data using batch syntax.\n\nThis means that if we can apply our ranked `Window` logic within our `foreachBatch` function, we can avoid the restriction throwing our error.\n\nThe code below sets up all the incremental logic needed to load in the data in the correct schema from the bronze table. This includes:\n- Filter for the `user_info` topic\n- Dropping identical records within the batch\n- Unpack all of the JSON fields from the `value` column into the correct schema\n- Update field names and types to match the `users` table schema\n- Use the salted hash function to cast the `user_id` to `alt_id`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"68b0255c-c403-4b78-9d4c-34151eeb4596"}}},{"cell_type":"code","source":["salt = \"BEANS\"\n\nunpackedDF = (spark.readStream\n    .table(\"bronze\")\n    .filter(\"topic = 'user_info'\")\n    .dropDuplicates()\n    .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\")).select(\"v.*\")\n    .select(F.sha2(F.concat(F.col(\"user_id\"), F.lit(salt)), 256).alias(\"alt_id\"),\n        F.col('timestamp').cast(\"timestamp\").alias(\"updated\"),\n        F.to_date('dob','MM/dd/yyyy').alias('dob'),\n        'sex', 'gender','first_name','last_name',\n        'address.*', \"update_type\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4b490941-bfb3-47f5-b724-cea8110f047a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The updated Window logic is provided below. Note that this is being applied to each `microBatchDF` to result in a local `rankedDF` that will be used for merging.\n \nFor our `MERGE` statement, we need to:\n- Match entries on our `alt_id`\n- Update all when matched **if** the new record has is newer than the previous entry\n- When not matched, insert all\n\nBecause `foreachBatch` allows for arbitrary writes to multiple tables from the same stream, processing the delete requests to a second table can happen in this same logic."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2b50ca54-4f1d-44ce-b03f-5b647b601ab9"}}},{"cell_type":"code","source":["from pyspark.sql.window import Window\n\nwindow = Window.partitionBy(\"alt_id\").orderBy(F.col(\"updated\").desc())\n\ndef batch_rank_upsert(microBatchDF, batchId):\n    appId = \"batch_rank_upsert\"\n    \n    (microBatchDF\n        .filter(F.col(\"update_type\").isin([\"new\", \"update\"]))\n        .withColumn(\"rank\", F.rank().over(window)).filter(\"rank == 1\").drop(\"rank\")\n        .createOrReplaceTempView(\"ranked_updates\"))\n    \n    microBatchDF._jdf.sparkSession().sql(\"\"\"\n        MERGE INTO users u\n        USING ranked_updates r\n        ON u.alt_id=r.alt_id\n        WHEN MATCHED AND u.updated < r.updated\n          THEN UPDATE SET *\n        WHEN NOT MATCHED\n          THEN INSERT *\n    \"\"\")\n\n    (microBatchDF\n         .filter(\"update_type = 'delete'\")\n         .select(\n            \"alt_id\", \n            F.col(\"updated\").alias(\"requested\"), \n            F.date_add(\"updated\", 30).alias(\"deadline\"), \n            F.lit(\"requested\").alias(\"status\"))\n        .write\n        .format(\"delta\")\n        .mode(\"append\")\n        .option(\"txnVersion\", batchId)\n        .option(\"txnAppId\", appId)\n        .option(\"path\", Paths.deleteRequests)\n        .saveAsTable(\"delete_requests\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"601b83ff-f10d-4381-9bf0-003b3b237801"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now we can apply this function to our data. Here, we'll run a trigger once batch to process all records."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d4e2f71b-2061-4b6d-9411-6e86acb518d5"}}},{"cell_type":"code","source":["(unpackedDF.writeStream\n    .foreachBatch(batch_rank_upsert)\n    .outputMode(\"update\")\n    .option(\"checkpointLocation\", Paths.usersCheckpointPath)\n    .trigger(once=True)\n    .start()\n    .awaitTermination())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"556c1295-9a4c-45b2-9c67-399833a1205f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The `users` table should only have 1 record for each unique ID."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e47830ba-10ed-43dd-b15c-e050ba963f11"}}},{"cell_type":"code","source":["assert spark.table(\"users\").count() == spark.table(\"users\").select(\"alt_id\").distinct().count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8f8db7bf-5ca7-4333-b0ee-a38d66f9b2b6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Assuming some requests to be forgotten have been made, there should be records in the `delete_requests` table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7c8e8522-0a24-435c-9800-d86542f9f76f"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM delete_requests"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"28ac9bcc-e440-4d47-b22d-36666f813bbf"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2021 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"60c32e73-b936-4471-bc08-5e363259e359"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ADES 3.02 - Storing PII Securely","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2598055170584650}},"nbformat":4,"nbformat_minor":0}
