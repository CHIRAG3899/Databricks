{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"50eceb8a-faaf-4a59-b0e5-6e51e10f6408"}}},{"cell_type":"markdown","source":["# Propagating Deletes with Change Data Feed\n\nWhile the PII for users has been pseudonymized, generalized, and redacted through several approaches, we have not yet addressed how deletes can be effectively and efficiently handled in the Lakehouse.\n\nIn this notebook, we'll combine Structured Streaming, Delta Lake, and Change Data Feed to demonstrate processing delete requests incrementally and propagating deletes through the Lakehouse.\n\n<img src=\"https://files.training.databricks.com/images/ade/ADE_arch_users.png\" width=\"60%\" />\n\n## Learning Objectives\nBy the end of this lesson, students will be able to:\n- Commit arbitrary messages to the Delta log to record important events\n- Apply deletes using Delta Lake DDL\n- Propagate deletes using Change Data Feed\n- Leverage incremental syntax to ensure deletes are committed fully\n- Describe default data retention settings for Change Data Feed"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eddc1583-7c71-42c6-b7f8-937f25c362bf"}}},{"cell_type":"markdown","source":["Begin by running the following cell to set up relevant databases and paths."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aa4c59d7-51fe-4db6-99e0-8770feedb105"}}},{"cell_type":"code","source":["%run \"../Includes/ade-setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7e1702ad-be3f-461c-917b-4603dc4f24d5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Requirements for Fulfilling Requests to Be Forgotten\n\nThe `user_lookup` table contains the link between the `alt_id` used as the primary key for the `users` table and natural keys found elsewhere in the lakehouse.\n\nDifferent industries will have different requirements for data deletion and data retention. Here, we'll assume the following:\n1. All PII in the `users` table must be deleted\n1. Links between pseudonymized keys and natural keys should be forgotten\n1. A policy to remove historic data containing PII from raw data sources and logs should be enacted\n\nThis notebook will focus on the first two of these requirements; the third will be handled in the following lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c24bb0f4-1843-474b-b4db-50c2ae9ca6e4"}}},{"cell_type":"markdown","source":["## Processing Delete Requests\n\nThe architect for the current pipeline has implemented the `delete_requests` table to track users' requests to be forgotten. Note that it is possible to process delete requests alongside inserts and updates to existing data.\n\nBecause PII exists in several places through the current lakehouse, tracking requests and processing them asyncronously may provide better performance for production jobs with low latency SLAs. The approach modeled here also indicates the time at which the delete was requested and the deadline, and provides a field to indicate the current processing status of the request.\n\nReview the `delete_requests` table below."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"78cea0b1-3b2e-4ead-89e4-d72f072fbb21"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM delete_requests"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2c5a85b2-cdc2-451f-ab0c-3e87ceb11c50"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We'll define a view against this table to capture those delete requests that have yet to be processed.\n\n**NOTE**: Depending on your internal data retention policy, you may choose to add an additional filter and only delete those records older than a certain amount of time."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e00c507-3cfe-479f-8dc3-4df5df7f3763"}}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE VIEW new_requests\nAS (SELECT alt_id FROM delete_requests WHERE status = 'requested')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d6a2ab92-5b1d-4cb6-a78e-afcbe3dba93c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Adding Commit Messages\n\nDelta Lake supports arbitrary commit messages that will be recorded to the Delta transaction log and viewable in the table history. This can help with later auditing.\n\nWe'll set a message that will be used for all subsequent operations in our notebook."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"71825d60-72fe-4a04-b6b8-ab0afe482285"}}},{"cell_type":"code","source":["%sql\nSET spark.databricks.delta.commitInfo.userMetadata=Deletes committed"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"851e7439-7775-4b74-8bf1-75fc2b562344"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Committing Deletes\nWhen working with static data, commiting deletes is simple. \n\nThe following logic modifies the `user_lookup` table by rewriting all data files containing records affected by the `DELETE` statement. Recall that with Delta Lake, deleting data will create new data files rather than deleting existing data files."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b700aae5-d29e-4979-ae4c-44ee6e690f66"}}},{"cell_type":"code","source":["%sql\nDELETE FROM user_lookup\nWHERE alt_id IN (SELECT alt_id FROM delete_requests WHERE status = 'requested')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dea8c0a7-2f81-4712-b64d-d04822a1a94d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Incrementally Processing Deletes\n\nThe `user_lookup` table was created with Change Data Feed enabled (this can be confirmed by looking at the table properties)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"21bf1c98-3084-40d7-81bc-d1705018d451"}}},{"cell_type":"code","source":["%sql\nDESCRIBE EXTENDED user_lookup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8806c28c-e765-470f-94f0-e671f59ef287"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["While the lakehouse architecture implemented here typically uses the `user_lookup` as a static table in joins with incremental data, the Change Data Feed can be separately leveraged as an incremental record of data changes.\n\nThe code below configures as incremental read of all changes committed to the `user_lookup` table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2f6167d7-bee3-4ab0-b41e-cfdacb5d0570"}}},{"cell_type":"code","source":["deleteDF = (spark.readStream\n    .format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingVersion\", 0)\n    .table(\"user_lookup\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b66e6568-b6fd-404a-a62f-2f993e083551"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The relationships between our natural keys (`user_id`, `device_id`, and `mac_address`) are stored in our `user_lookup`. These allow us to link a user's data between various pipelines/sources. The Change Data Feed from this table will maintain all these fields, allowing successful identification of records to be deleted or modified in downstream tables.\n\nThe function below demonstrates committing deletes to two tables using different keys and syntax. Note that in this case, the `MERGE` syntax demonstrated is not necessary to process the deletes to the `users` table; this code block does demonstrate the basic syntax that could be expanded if inserts and updates were to be processed in the same code block as deletes.\n\nAssuming successful completion of these two table modifications, an update will be process back to the `delete_requests` table. Note that we're leveraging data that has been successfully deleted from the `user_lookup` table to update a value in the `delete_requests` table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"69c173fe-4c71-4159-bf11-c55163eb9287"}}},{"cell_type":"code","source":["def process_deletes(microBatchDF, batchId):\n    \n    (microBatchDF\n        .filter(\"_change_type = 'delete'\")\n        .createOrReplaceTempView(\"deletes\"))\n    \n    microBatchDF._jdf.sparkSession().sql(\"\"\"\n        MERGE INTO users u\n        USING deletes d\n        ON u.alt_id = d.alt_id\n        WHEN MATCHED\n            THEN DELETE\n    \"\"\")\n\n    microBatchDF._jdf.sparkSession().sql(\"\"\"\n        DELETE FROM user_bins\n        WHERE user_id IN (SELECT user_id FROM deletes)\n    \"\"\")\n    \n    microBatchDF._jdf.sparkSession().sql(\"\"\"\n        MERGE INTO delete_requests dr\n        USING deletes d\n        ON d.alt_id = dr.alt_id\n        WHEN MATCHED\n          THEN UPDATE SET status = \"deleted\"\n    \"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4437de7a-0243-4681-a274-f05746b257be"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Recall that this workload is being driven by incremental changes to the `user_lookup` table. Executing the following cell will propagate deletes to a single table to multiple tables throughout the lakehouse."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"581f8161-68a0-4042-83d4-afed4d260cbd"}}},{"cell_type":"code","source":["(deleteDF.writeStream\n    .foreachBatch(process_deletes)\n    .outputMode(\"update\")\n    .option(\"checkpointLocation\", Paths.checkpointPath + \"/deletes\")\n    .trigger(once=True)\n    .start()\n    .awaitTermination())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e2710d79-fdd7-4df7-8250-07eae109f042"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Review Delete Commits\nNote that with our current implementation, if a user registration never made it into the `user_lookup` table, data for this user will not be deleted from other tables. However, the status for these records in the `delete_requests` table will also remain `requested`, so a redundant approach could be applied if necessary."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"685d4df1-530e-43f7-a5a1-e2e93f15a753"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM delete_requests"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ba18508c-dcc3-499e-b767-e12fc3398a5c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Note that our commit message will be in the far right column of our history, under the column `userMetadata`.\n\nFor the `users` table, the operation field in the history will indicate a merge because of the chosen syntax, even though only deletes were committed. The number of deleted rows can be reviewed in the `operationMetrics`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6cedd4f8-9dff-4225-9d8c-9ee7a244f6fa"}}},{"cell_type":"code","source":["%sql\nDESCRIBE HISTORY users"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"279d29f6-fb5c-4adf-9f80-7dbeb454758f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["As expected, `user_bins` will show a delete."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c444b2dc-e119-4b15-8f7b-94f3a37284a4"}}},{"cell_type":"code","source":["%sql\nDESCRIBE HISTORY user_bins"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d44d4fee-1140-4b68-9ddd-2ec9a64266e4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The changes to `delete_requests` also show a merge operation, and appropriately show that records have been updated rather than deleted in this table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5d6953a1-d709-4bd8-8c66-a4a61445c714"}}},{"cell_type":"code","source":["%sql\nDESCRIBE HISTORY delete_requests"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d114ca26-306b-41b5-91d9-b8b1ef1290be"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Are Deletes Fully Committed?\n\nNot exactly.\n\nBecause of how Delta Lake's history and CDF features are implemented, deleted values are still present in older versions of the data.\n\nThe query below shows the records deleted in v1 of the `user_bins` table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e6cbd1d7-c568-46cb-b78d-f00d8eb6baf6"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM user_bins@v0 u1\nEXCEPT \nSELECT * FROM user_bins u2"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ed81fbfe-c493-41ba-90f7-cc3552bbe60b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Similarly, while we've already applied our logic on the incremental data produced by deletes committed to the `user_lookup` table, this information is still available within the change feed."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"91fdf3e3-bd88-4ebf-80eb-c9057d8d4a14"}}},{"cell_type":"code","source":["display(spark.read\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingVersion\", 0)\n    .table(\"user_lookup\")\n    .filter(\"_change_type = 'delete'\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"61860b8b-2d33-4085-b9f1-c60029e8816c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The next notebook will explore fully committing these deletes, as well as providing guidance for removing access to historic raw data containing PII."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"57a31af3-31c5-4043-8c06-34f765793b63"}}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2021 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e3b1cd72-89aa-4759-ae83-750864ee7b6f"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ADES 3.04 - Propagating Deletes wCDF","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2598055170584518}},"nbformat":4,"nbformat_minor":0}
