{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5fc4a5ae-3237-4e5c-bbe4-cffa87fceda9"}}},{"cell_type":"markdown","source":["# End-to-End ETL in the Lakehouse\n## Final Steps\n\nWe are picking up from the first notebook in this lab, [DE 12.2.1L - Instructions and Configuration]($./DE 12.2.1L - Instructions and Configuration)\n\nIf everything is setup correctly, you should have:\n* A DLT Pipeline running in **Continuous** mode\n* A job that is feeding that pipline new data every 2 minutes\n* A series of Databricks SQL Queries analysing the outputs of that pipeline"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ffd7beb-377a-4e8e-b063-ad5e0735f067"}}},{"cell_type":"code","source":["%run ../../Includes/Classroom-Setup-12.2.4L"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"167a2e85-e24a-463c-bb33-4e7ef419e76b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Execute a Query to Repair Broken Data\n\nReview the code that defined the **`recordings_enriched`** table to identify the filter applied for the quality check.\n\nIn the cell below, write a query that returns all the records from the **`recordings_bronze`** table that were refused by this quality check."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a44fc4db-3c44-4daa-81b8-9ebd7c083915"}}},{"cell_type":"code","source":["%sql\nSELECT * FROM ${da.db_name}.recordings_bronze WHERE heartrate <= 0"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"887c7aab-16b7-406a-be14-c0a351b0fbff"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["For the purposes of our demo, let's assume that thorough manual review of our data and systems has demonstrated that occasionally otherwise valid heartrate recordings are returned as negative values.\n\nRun the following query to examine these same rows with the negative sign removed."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e935288d-b1cf-4837-8306-e996c876fd2a"}}},{"cell_type":"code","source":["%sql\nSELECT abs(heartrate), * FROM ${da.db_name}.recordings_bronze WHERE heartrate <= 0"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"caf0ebed-0776-497d-b0e7-8d8957a8793a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["To complete our dataset, we wish to insert these fixed records into the silver **`recordings_enriched`** table.\n\nUse the cell below to update the query used in the DLT pipeline to execute this repair.\n\n**NOTE**: Make sure you update the code to only process those records that were previously rejected due to the quality check."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"757add8c-8006-4dfb-97dd-848afeec06f5"}}},{"cell_type":"code","source":["%sql\nMERGE INTO ${da.db_name}.recordings_enriched t\nUSING (SELECT\n  CAST(a.device_id AS INTEGER) device_id, \n  CAST(a.mrn AS LONG) mrn, \n  abs(CAST(a.heartrate AS DOUBLE)) heartrate, \n  CAST(from_unixtime(a.time, 'yyyy-MM-dd HH:mm:ss') AS TIMESTAMP) time,\n  b.name\n  FROM ${da.db_name}.recordings_bronze a\n  INNER JOIN ${da.db_name}.pii b\n  ON a.mrn = b.mrn\n  WHERE heartrate <= 0) v\nON t.mrn=v.mrn AND t.time=v.time\nWHEN NOT MATCHED THEN INSERT *"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"309b46bc-eedb-4b34-b401-d9430560a042"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Use the cell below to manually or programmatically confirm that this update has been successful.\n\n(The total number of records in the **`recordings_bronze`** should now be equal to the total records in **`recordings_enriched`**)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ec43225c-9cdf-43ca-a4cc-d6d315cc4834"}}},{"cell_type":"code","source":["assert spark.table(f\"{DA.db_name}.recordings_bronze\").count() == spark.table(f\"{DA.db_name}.recordings_enriched\").count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a60dda4-fba1-4dd8-96b2-1caa5bb5f676"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Consider Production Data Permissions\n\nNote that while our manual repair of the data was successful, as the owner of these datasets, by default we have permissions to modify or delete these data from any location we're executing code.\n\nTo put this another way: our current permissions would allow us to change or drop our production tables permanently if an errant SQL query is accidentally executed with the current user's permissions (or if other users are granted similar permissions).\n\nWhile for the purposes of this lab, we desired to have full permissions on our data, as we move code from development to production, it is safer to leverage <a href=\"https://docs.databricks.com/administration-guide/users-groups/service-principals.html\" target=\"_blank\">service principals</a> when scheduling Jobs and DLT Pipelines to avoid accidental data modifications."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11546b1b-59cc-40ea-9c39-8f6ae92d9554"}}},{"cell_type":"markdown","source":["## Shut Down Production Infrastructure\n\nNote that Databricks Jobs, DLT Pipelines, and scheduled DBSQL queries and dashboards are all designed to provide sustained execution of production code. In this end-to-end demo, you were instructed to configure a Job and Pipeline for continuous data processing. To prevent these workloads from continuing to execute, you should **Pause** your Databricks Job and **Stop** your DLT pipeline. Deleting these assets will also ensure that production infrastructure is terminated.\n\n**NOTE**: All instructions for DBSQL asset scheduling in previous lessons instructed users to set the update schedule to end tomorrow. You may choose to go back and also cancel these updates to prevent DBSQL endpoints from staying on until that time."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc95dd4d-d24a-42bc-9a61-333eecfb92ac"}}},{"cell_type":"code","source":["DA.cleanup()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"936eac56-d4d5-4f37-8bb3-b61d6e8bf701"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"32b8f279-dd0d-4677-97c5-f8c74a50f185"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"DE 12.2.4L - Final Steps","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2841292000076237}},"nbformat":4,"nbformat_minor":0}
