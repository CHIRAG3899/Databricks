{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b43f43d-3e31-4570-986f-ebdfe9265d72"}}},{"cell_type":"markdown","source":["# Using the Delta Live Tables UI\n\nThis demo will explore the DLT UI. \n\n\n## Learning Objectives\nBy the end of this lesson, you should be able to:\n* Deploy a DLT pipeline\n* Explore the resultant DAG\n* Execute an update of the pipeline\n* Look at metrics"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f66e5599-10a0-49f6-90a3-c29532eca0d2"}}},{"cell_type":"markdown","source":["## Run Setup\n\nThe following cell is configured to reset this demo."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a40c71a-003a-4fa3-b768-c7b90cdd9c23"}}},{"cell_type":"code","source":["%run ../../Includes/Classroom-Setup-8.1.1"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd72480c-1bb9-45e5-a91c-ef6c4bcd757a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Execute the following cell to print out values that will be used during the following configuration steps."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6fd705fb-a6f6-45d2-9544-e89968cbec2a"}}},{"cell_type":"code","source":["DA.print_pipeline_config()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a1cb6312-c54d-46ea-9091-146158731ca2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Create and Configure a Pipeline\n\nIn this section you will create a pipeline using a notebook provided with the courseware. We'll explore the contents of the notebook in the following lesson.\n\n1. Click the **Jobs** button on the sidebar.\n1. Select the **Delta Live Tables** tab.\n1. Click **Create Pipeline**.\n1. Leave **Product Edition** as **Advanced**.\n1. Fill in a **Pipeline Name** - because these names must be unique, we suggest using the **`Pipeline Name`** provided by the cell above.\n1. For **Notebook Libraries**, use the navigator to locate and select the companion notebook called **DE 8.1.2 - SQL for Delta Live Tables**.   \n   * Alternatively, you can copy the **`Notebook Path`** provided by the cell above and paste it into the provided field.\n   * Even though this document is a standard Databricks Notebook, the SQL syntax is specialized to DLT table declarations.\n   * We will be exploring the syntax in the exercise that follows.\n1. In the **Target** field, specify the database name printed out next to **`Target`** in the cell above.<br/>\nThis should follow the pattern **`dbacademy_<username>_dewd_dlt_demo_81`**\n   * This field is optional; if not specified, then tables will not be registered to a metastore, but will still be available in the DBFS. Refer to the <a href=\"https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-user-guide.html#publish-tables\" target=\"_blank\">documentation</a> for more information on this option.\n1. In the **Storage location** field, copy the **`Storage Location`** path printed by the cell above.\n   * This optional field allows the user to specify a location to store logs, tables, and other information related to pipeline execution. \n   * If not specified, DLT will automatically generate a directory.\n1. For **Pipeline Mode**, select **Triggered**\n   * This field specifies how the pipeline will be run.\n   * **Triggered** pipelines run once and then shut down until the next manual or scheduled update.\n   * **Continuous** pipelines run continuously, ingesting new data as it arrives. Choose the mode based on latency and cost requirements.\n1. Uncheck the **Enable autoscaling** box, and set the number of workers to **`1`** (one).\n   * **Enable autoscaling**, **Min Workers** and **Max Workers** control the worker configuration for the underlying cluster processing the pipeline. Notice the DBU estimate provided, similar to that provided when configuring interactive clusters.\n1. Click **Create**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6703b83a-0813-41c7-bcf1-eb8f9de80156"}}},{"cell_type":"markdown","source":["## Run a Pipeline\n\nWith a pipeline created, you will now run the pipeline.\n\n1. Select **Development** to run the pipeline in development mode. \n  * Development mode provides for more expeditious iterative development by reusing the cluster (as opposed to creating a new cluster for each run) and disabling retries so that you can readily identify and fix errors.\n  * Refer to the <a href=\"https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-user-guide.html#optimize-execution\" target=\"_blank\">documentation</a> for more information on this feature.\n2. Click **Start**.\n\nThe initial run will take several minutes while a cluster is provisioned. \n\nSubsequent runs will be appreciably quicker."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fe992b7a-72b1-45e9-a932-3da8c44f6f6b"}}},{"cell_type":"markdown","source":["## Exploring the DAG\n\nAs the pipeline completes, the execution flow is graphed. \n\nSelecting the tables reviews the details.\n\nSelect **sales_orders_cleaned**. Notice the results reported in the **Data Quality** section. Because this flow has data expectations declared, those metrics are tracked here. No records are dropped because the constraint is declared in a way that allows violating records to be included in the output. This will be covered in more details in the next exercise."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cdd32753-700a-40fb-adf7-e517f20f794b"}}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2022 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9771b06c-12f4-47e5-98ac-91a88713c1d7"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"DE 8.1.1 - DLT UI Walkthrough","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2841292000077029}},"nbformat":4,"nbformat_minor":0}
