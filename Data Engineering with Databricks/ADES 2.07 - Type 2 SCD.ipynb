{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"38893860-f7a6-41aa-8215-c7e64e13cce9"}}},{"cell_type":"markdown","source":["# Type 2 Slowly Changing Data\n\nIn this notebook, we'll create a silver table that links workouts to heart rate through our resultant table.\n\nWe'll use a Type 2 table to record this data, which will provide us the ability to match our heart rate recordings to users through this information.\n\n<img src=\"https://files.training.databricks.com/images/ade/ADE_arch_completed_workouts.png\" width=\"60%\" />\n\n## Learning Objectives\nBy the end of this lesson, students will be able to:\n- Describe how Slowly Changing Dimension tables can be implemented in the Lakehouse\n- Use custom logic to implement a SCD Type 2 table with batch overwrite logic"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1d6894a2-0dab-4d29-8350-3c8e732c6168"}}},{"cell_type":"markdown","source":["Set up path and checkpoint variables (these will be used later)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"05406092-6c2e-43a3-a4dd-58ddd68a90a1"}}},{"cell_type":"code","source":["%run ../Includes/workouts-setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e969404-0bb0-4e83-a2c9-40707f1ecbac"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["A helper function was defined to land and propagate a batch of data to the `workouts_silver` table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ba203200-441f-4580-9a50-107236c6ba66"}}},{"cell_type":"code","source":["process_silver_workouts()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0ccc9134-0f97-46b7-8c4e-27042db42bd2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Load and preview the `workouts_silver` data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"63a7bdf5-8ce6-4d3e-82f5-bbc51838d561"}}},{"cell_type":"code","source":["workoutDF = spark.table(\"workouts_silver\")\ndisplay(workoutDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0d3623b7-13f6-4b53-91e2-c2e4db408b4b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["For this data, the `user_id` and `session_id` form a composite key. Each pair should eventually have 2 records present, marking the \"start\" and \"stop\" action for each workout."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"88da5756-e064-49fe-a7af-129e6fc748b3"}}},{"cell_type":"code","source":["display(workoutDF.groupby(\"user_id\", \"session_id\").count())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc638c73-adb5-48ec-96ae-14d1d9b38f9e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Because we'll be triggering a shuffle in this notebook, we'll be explicit about how many partitions we want at the end of our shuffle."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2a801c81-9e02-4fe0-88ce-fc79d660de02"}}},{"cell_type":"code","source":["sqlContext.setConf(\"spark.sql.shuffle.partitions\", \"4\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"27eae0f8-4bd0-4a7a-b0d2-776d1a4574a5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Create Completed Workouts Table\n\nThe query below matches our start and stop actions, capturing the time for each action. The `in_progress` field indicates whether or not a given workout session is ongoing."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"efc1f75b-3c4c-4c34-acd3-370529390112"}}},{"cell_type":"code","source":["%sql\n\nCREATE OR REPLACE TEMP VIEW TEMP_completed_workouts AS (\n  SELECT a.user_id, a.workout_id, a.session_id, a.start_time start_time, b.end_time end_time, a.in_progress AND (b.in_progress IS NULL) in_progress\n  FROM (\n    SELECT user_id, workout_id, session_id, time start_time, null end_time, true in_progress\n    FROM workouts_silver\n    WHERE action = \"start\") a\n  LEFT JOIN (\n    SELECT user_id, workout_id, session_id, null start_time, time end_time, false in_progress\n    FROM workouts_silver\n    WHERE action = \"stop\") b\n  ON a.user_id = b.user_id AND a.session_id = b.session_id\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dab4fdee-a0df-4174-901f-6ad899227595"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Register Target Table\n\nThe following cell is provided to allow for easy re-setting of this demo. In production, you will _not_ want to drop your target table each time. As such, once you have this notebook working, you should comment out the following cell.\n\n**HINT**: To comment out an entire block of code, select all text and then hit \"**CMD** + **/**\" (Mac)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37bc6d93-0f57-4351-b901-b6dcf497d40a"}}},{"cell_type":"code","source":["spark.sql(\"DROP TABLE IF EXISTS completed_workouts\")\n\ndbutils.fs.rm(Paths.completedWorkouts, True)\n\nspark.sql(f\"\"\"\n  CREATE TABLE IF NOT EXISTS completed_workouts\n  (user_id INT, workout_id INT, session_id INT, start_time TIMESTAMP, end_time TIMESTAMP, in_progress BOOLEAN)\n  USING DELTA\n  LOCATION '{Paths.completedWorkouts}'\n\"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"486309d0-6e87-4d31-b455-5b10b6b5e8b4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Write Results as a Batch Overwrite\n\nOur present implementation will replace the `charts_valid` table entirely with each triggered batch. What are some potential benefits and drawbacks of this approach?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a552bd84-70ae-43b2-bc75-5c1d07a28d61"}}},{"cell_type":"code","source":["def process_completed_workouts():\n    (spark.table(\"TEMP_completed_workouts\").write\n        .mode(\"overwrite\")\n        .saveAsTable(\"completed_workouts\"))\nprocess_completed_workouts()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"79476740-68c3-4716-a4d4-33eb712be6e4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["You can now perform a query directly on your `completed_workouts` table to check your results. Uncomment the `WHERE` clauses below to confirm various functionality of the logic above."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8662001a-8cd0-49f1-ad8d-d90417174516"}}},{"cell_type":"code","source":["%sql\n\nSELECT COUNT(*)\nFROM completed_workouts\n-- WHERE in_progress=true                        -- where record is still awaiting end time\n-- WHERE end_time IS NOT NULL                    -- where end time has been recorded\n-- WHERE start_time IS NULL                      -- where end time arrived before start time\n-- WHERE in_progress=true AND end_time IS NULL   -- confirm that no entries are valid with end_time"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1ffae8f1-7308-4d6f-8edb-e53749aab074"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Use the functions below to propagate another batch of records through the pipeline to this point."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e26f23e1-c447-438e-a33e-5267ca228b5b"}}},{"cell_type":"code","source":["process_silver_workouts()\nprocess_completed_workouts()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0bd96be0-a1be-4008-8bb5-241ed0c260ba"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n\nSELECT COUNT(*)\nFROM completed_workouts"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"055e192d-de23-45ba-8127-9b218ddc4d86"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2021 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bd72497e-cf71-4ea6-8247-2dadf920bf99"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ADES 2.07 - Type 2 SCD","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2598055170583505}},"nbformat":4,"nbformat_minor":0}
